{"docstore/metadata": {"c9ef9b7c-6dd0-4935-a552-9b3674a6f388": {"doc_hash": "0aabd50dc4c5ab13f3a917f150b8976adb00326a2a0654b2e879c8710f3cc572"}, "9e1bb884-42d3-4a53-b6fd-0c763c91b8ff": {"doc_hash": "0dddf5bf69a835a468364a576ba8cf9cfe899cf6170088b0f0a5fd5cae2a8af7", "ref_doc_id": "c9ef9b7c-6dd0-4935-a552-9b3674a6f388"}, "7344652d-6789-4c15-96cc-ea8f7128b966": {"doc_hash": "253ce19eb04bcc376e45f7bedf37452dfa4fe826a9be2ed58e6f3f656efa976a"}, "e504fc55-ba53-482d-8ee8-08eb0be8e6f2": {"doc_hash": "db8c5cc228bad7229d7a3112e3fd3f3d79bf0dedcd650b40ef74102a1a46b269", "ref_doc_id": "7344652d-6789-4c15-96cc-ea8f7128b966"}, "951cb1fd-c1c1-48e4-9e60-c1db7e07330d": {"doc_hash": "31d4fe5e986c94b6827098c11f56fad1bfb4ba2a672704a0f9407e14a4c143d9", "ref_doc_id": "7344652d-6789-4c15-96cc-ea8f7128b966"}}, "docstore/data": {"9e1bb884-42d3-4a53-b6fd-0c763c91b8ff": {"__data__": {"id_": "9e1bb884-42d3-4a53-b6fd-0c763c91b8ff", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-0.txt", "file_name": "output-0.txt", "file_type": "text/plain", "file_size": 61, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9ef9b7c-6dd0-4935-a552-9b3674a6f388", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-0.txt", "file_name": "output-0.txt", "file_type": "text/plain", "file_size": 61, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "0aabd50dc4c5ab13f3a917f150b8976adb00326a2a0654b2e879c8710f3cc572", "class_name": "RelatedNodeInfo"}}, "text": "# 403 Forbidden\n\nRequest forbidden by administrative rules.", "start_char_idx": 0, "end_char_idx": 59, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e504fc55-ba53-482d-8ee8-08eb0be8e6f2": {"__data__": {"id_": "e504fc55-ba53-482d-8ee8-08eb0be8e6f2", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7344652d-6789-4c15-96cc-ea8f7128b966", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "253ce19eb04bcc376e45f7bedf37452dfa4fe826a9be2ed58e6f3f656efa976a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "951cb1fd-c1c1-48e4-9e60-c1db7e07330d", "node_type": "1", "metadata": {}, "hash": "a97f315051673ece8b4a009e54ba0dcfc460b4fd8bc0946a48a068b12ec2890e", "class_name": "RelatedNodeInfo"}}, "text": "[ \ud30c\uc774\ud1a0\uce58 \ud55c\uad6d \uc0ac\uc6a9\uc790 \ubaa8\uc784 ](/)\n\n#  [Salesforce, InstructBLIP \ubaa8\ub378\uc758 \ub17c\ubb38 / \ucf54\ub4dc / \uac00\uc911\uce58 \uacf5\uac1c](/t/salesforce-\ninstructblip/1571)\n\n[ \uc77d\uc744\uac70\ub9ac&\uc815\ubcf4\uacf5\uc720 ](/c/news/14)\n\n[lavis](https://discuss.pytorch.kr/tag/lavis),\n[blip2](https://discuss.pytorch.kr/tag/blip2),\n[minigpt-4](https://discuss.pytorch.kr/tag/minigpt-4),\n[instructblip](https://discuss.pytorch.kr/tag/instructblip),\n[salesforce](https://discuss.pytorch.kr/tag/salesforce),\n[gpt-4](https://discuss.pytorch.kr/tag/gpt-4),\n[multimodal](https://discuss.pytorch.kr/tag/multimodal), [vision-\nlanguage](https://discuss.pytorch.kr/tag/vision-language)\n\n[9bow](https://discuss.pytorch.kr/u/9bow) (\ubc15\uc815\ud658)  5\uc6d4 17, 2023, 9:59\uc624\uc804  1\n\nSalesforce\uc5d0\uc11c BLIP-2 \ubaa8\ub378\uc5d0 \uc774\uc5b4 InstructBLIP \ubaa8\ub378\uc758 \ub17c\ubb38\uacfc \uad6c\ud604, \uadf8\ub9ac\uace0 \ud559\uc2b5\ub41c \uac00\uc911\uce58\ub97c \uacf5\uac1c\ud588\uc2b5\ub2c8\ub2e4.\n\n##  InstructBLIP: Towards General-purpose Vision-Language Models with\nInstruction Tuning\n\n[![image](https://discuss.pytorch.kr/uploads/default/original/2X/7/720b1a5fdeccd3fb8e52ace10a474304e32beee9.jpeg)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\n\uc544\ub798\uc640 \uac19\uc774 Vicuna, T5\ub97c \uc0ac\uc6a9\ud55c 2 \uc885\ub958\uc758 \ubaa8\ub378\uc774 \uc788\uc73c\uba70,\n\n    \n    \n    # ==================================================\n    # Architectures                  Types\n    # ==================================================\n    # blip2_vicuna_instruct          vicuna7b, vicuna13b\n    # blip2_t5_instruct              flant5xl, flant5xxl\n    \n\n~~[Salesforce\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 LAVIS \ud328\ud0a4\uc9c0](https://github.com/salesforce/LAVIS)(`pip\ninstall salesforce-lavis`)\ub97c \uc124\uce58\ud558\uc5ec \ubc14\ub85c \uc0ac\uc6a9\ud574 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.~~  \n\uc544\uc9c1 PyPI\uc758 \ud328\ud0a4\uc9c0\uc5d0\ub294 InstructBLIP \ubaa8\ub378\ub4e4\uc774 \ubc18\uc601\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc544\uc11c, GitHub\uc5d0\uc11c \uc9c1\uc811 \uc124\uce58\ud558\uc154\uc57c \ud569\ub2c8\ub2e4.\n\n    \n    \n    git clone https://github.com/salesforce/LAVIS.git\n    cd LAVIS\n    pip install -e .\n    \n    \n    \n    from lavis.models import load_model_and_preprocess\n    # loads InstructBLIP model\n    model, vis_processors, _ = load_model_and_preprocess(name=\"blip2_vicuna_instruct\", model_type=\"vicuna7b\", is_eval=True, device=device)\n    # prepare the image\n    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n    \n\n  \n\ucf54\ub4dc\uc640 \uc0ac\uc6a9\ubc95\uc774 \uad81\uae08\ud558\uc2e0 \ubd84\ub4e4\uaed8\uc11c\ub294 GitHub \uc800\uc7a5\uc18c\uc5d0\uc11c,\n\n![](https://github.githubassets.com/favicons/favicon.svg)\n[github.com](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\n### [LAVIS/projects/instructblip at main \u00b7\nsalesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\n[main/projects/instructblip](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\nLAVIS - A One-stop Library for Language-Vision Intelligence\n\n  \n\ub17c\ubb38\uc774 \uad81\uae08\ud558\uc2e0 \ubd84\ub4e4\uaed8\uc11c\ub294 arXiv\uc5d0\uc11c \ubc14\ub85c \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \n~~(\uc800\ub294 \uc624\ub298\ub3c4 \uc77d\uae30 \ud050\uc5d0 \ub123\uae30\ub9cc \ud558\uace0\n\uc788\uc2b5\ub2c8\ub2e4;;;![:sweat_smile:](https://discuss.pytorch.kr/images/emoji/apple/sweat_smile.png?v=12)\n)~~\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/c/c683569a48ce1952ba841c851ae3b1f282d4b00f.png)\n[arXiv.org](https://arxiv.org/abs/2305.06500)\n\n!", "start_char_idx": 0, "end_char_idx": 2748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "951cb1fd-c1c1-48e4-9e60-c1db7e07330d": {"__data__": {"id_": "951cb1fd-c1c1-48e4-9e60-c1db7e07330d", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7344652d-6789-4c15-96cc-ea8f7128b966", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "253ce19eb04bcc376e45f7bedf37452dfa4fe826a9be2ed58e6f3f656efa976a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e504fc55-ba53-482d-8ee8-08eb0be8e6f2", "node_type": "1", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "db8c5cc228bad7229d7a3112e3fd3f3d79bf0dedcd650b40ef74102a1a46b269", "class_name": "RelatedNodeInfo"}}, "text": "~~(\uc800\ub294 \uc624\ub298\ub3c4 \uc77d\uae30 \ud050\uc5d0 \ub123\uae30\ub9cc \ud558\uace0\n\uc788\uc2b5\ub2c8\ub2e4;;;![:sweat_smile:](https://discuss.pytorch.kr/images/emoji/apple/sweat_smile.png?v=12)\n)~~\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/c/c683569a48ce1952ba841c851ae3b1f282d4b00f.png)\n[arXiv.org](https://arxiv.org/abs/2305.06500)\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/8/86a2c8ac52804c90ede238d99cb944037c6f82f6.png)\n\n### [InstructBLIP: Towards General-purpose Vision-Language Models with\nInstruction...](https://arxiv.org/abs/2305.06500)\n\nGeneral-purpose language models that can solve various language-domain tasks\nhave emerged driven by the pre-training and instruction-tuning pipeline.\nHowever, building general-purpose vision-language models is challenging due to\nthe increased task...\n\n  \n\ubb38\uc11c\uc758 \ub9c8\uc9c0\ub9c9\uc5d0 \uc81c\uc2dc\ub41c \uc774\ubbf8\uc9c0\uc758 \uc774\uc0c1\ud55c \ubd80\ubd84\uc744 \uc124\uba85\ud558\ub77c\ub294 \uc9c0\ubb38\uc5d0 \ub300\ud55c `InstructBLIP`\uacfc `GPT-4`, `miniGPT-4` \ub4f1\uc758\n\ub2f5\ubcc0\uc744 \ube44\uad50\ud574 \ub450\uc5c8\ub294\ub370 \uc778\uc0c1\uc801\uc774\ub124\uc694.\n![:monkey:](https://discuss.pytorch.kr/images/emoji/apple/monkey.png?v=12)\n\n![image](https://discuss.pytorch.kr/uploads/default/original/2X/e/ea9a956b0e6f6adb323ec01f0234cdac379b4d68.png)\n\n[[\ubb34\ub8cc/\uc628\ub77c\uc778/\uc601\uc5b4] ChatGPT\uc640 CLIP\uc744 \uc0ac\uc6a9\ud55c Semantic Visual Search\n\ub9cc\ub4e4\uae30](https://discuss.pytorch.kr/t/chatgpt-clip-semantic-visual-search/1731)\n\n[9bow](https://discuss.pytorch.kr/u/9bow) (\ubc15\uc815\ud658)  5\uc6d4 17, 2023, 10:04\uc624\uc804  2\n\n\ub17c\ubb38\uc5d0 \uc694\ub7f0 \uc0ac\ub840\ub3c4 \ucca8\ubd80\ub418\uc5b4 \uc788\ub124\uc694\n![:smiley:](https://discuss.pytorch.kr/images/emoji/apple/smiley.png?v=12)\n\n![image](https://discuss.pytorch.kr/uploads/default/original/2X/5/5f4b2578db249d9572ae390976096877be804d16.png)\n\n  * [\ud648 ](/)\n  * [\uce74\ud14c\uace0\ub9ac ](/categories)\n  * [FAQ/\uac00\uc774\ub4dc\ub77c\uc778 ](/guidelines)\n  * [\uc774\uc6a9\uc57d\uad00 ](/tos)\n  * [\uac1c\uc778\uc815\ubcf4 \ucde8\uae09\ubc29\uce68 ](/privacy)\n\n[Discourse](https://www.discourse.org)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. JavaScript\uac00 \ud65c\uc131\ud654\ub41c \uc0c1\ud0dc\uc5d0\uc11c \uac00\uc7a5 \uc798\n\ubcf4\uc785\ub2c8\ub2e4.", "start_char_idx": 2473, "end_char_idx": 4128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"c9ef9b7c-6dd0-4935-a552-9b3674a6f388": {"node_ids": ["9e1bb884-42d3-4a53-b6fd-0c763c91b8ff"], "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-0.txt", "file_name": "output-0.txt", "file_type": "text/plain", "file_size": 61, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}}, "7344652d-6789-4c15-96cc-ea8f7128b966": {"node_ids": ["e504fc55-ba53-482d-8ee8-08eb0be8e6f2", "951cb1fd-c1c1-48e4-9e60-c1db7e07330d"], "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}}}}