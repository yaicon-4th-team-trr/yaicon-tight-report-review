{"docstore/metadata": {"acf592e6-9295-43cf-bbe5-9c8a014b88e5": {"doc_hash": "75715c50b932d47d1959a3b5b52a7fd380751cbc8e7c18b27ed3d05664085189"}, "6554357d-4926-41bb-ba09-2a30ad8ab2b7": {"doc_hash": "5e34f5b14204cdb11f412636ca335860f4be0221a3528c6d439b488fb18dd997"}, "7350dff5-ca52-492b-9708-b7b75bfd56db": {"doc_hash": "e5a92a1dd37a5bcd469285e4832c07dfef4ea180c7c6e82a0f0a587e26bfa4f4"}, "ac547bcb-2d29-43e1-ab9b-b2dfc38e2757": {"doc_hash": "ca09af74149531c3cbf8092dadf93c614c0cd52ca6f091fbd74c81f8a544e4f4"}, "ef2eb8b6-8e82-4ab5-9926-3541b5aa7d8e": {"doc_hash": "48ca68d1d4319fbf473cca18126a799b74f721922930358eeba288b0003dd320"}, "fb4ea85d-82fb-46f1-bb96-06835299f948": {"doc_hash": "9b15c36c5f2d8a01fb73d4959c1579ae892e3f4037c2d31ac2f7805937e26f06"}, "a0df6c12-be5a-438a-bea1-e3142635b74e": {"doc_hash": "08d98ff2eb8dbd094c6afb4c232eb5520e0db79066795e417bed5bdb2ecf3c47"}, "3eed4cb8-08a2-4d0b-8393-36f2c3eceb8b": {"doc_hash": "40111c118eba4bbf141016a973d83a814269db64efc9abf331bd0df7a715ed18"}, "bdb898f8-0891-45bc-ae70-c691039e0a17": {"doc_hash": "2cd8e58106c0eacb051aba09179fb825aa06d4063c7da70e145ab79d1e368a66"}, "f61369ac-29e5-4502-a61c-592a8c2fcf67": {"doc_hash": "d1e1f44b6ad74b1df5608c47601dbc5d4939d308b0be3fb163203da47ebef1f7"}, "36e4c4a0-8bab-4984-95e5-6730b539c32c": {"doc_hash": "ce552ec6ab0fa42041d6b9984952bfcf376b47a0f7993965ff45f6f0691d59ef"}, "9d28bed6-b420-4177-a957-99164f1f3b8e": {"doc_hash": "2687d7da21a9c5a72dfdb4fbdd54993893c7267ced64a91a141b0c870a5b8f1b"}, "b83f35e1-2afe-456a-b61b-a3ae742985bb": {"doc_hash": "f55914395cc83f60fb8c6e3268e252041b31e18d919e8c9bdaba0d916143675d"}, "a81a760d-f444-4b72-a2a7-50810d061857": {"doc_hash": "b0b497610c92f2b22d076623482c362a07fa2a2d45b2029c8fa35086aecd5597"}, "ac046439-d904-452e-9cbe-da9420a97f05": {"doc_hash": "dcd430a2934fdbcc38ac667c620044fc1db802be57aa7bd1f310d1cd0b7bec6a"}, "f920d13a-e0d1-450b-8b67-1a59681e69a3": {"doc_hash": "ec67d0c60d819dc3b42d8b1caa7865f5956e6a76322a3fa4164023d73a17abe2"}, "7a860a07-9471-4ede-aeb8-20d98007a3e1": {"doc_hash": "a8a635ece2f7d1b6ab472481a4365545a44970aa8e0326c72946e7c534c9a8ed"}, "e97950f3-7089-482d-9562-744934d0b313": {"doc_hash": "e9362ff2e0adec55fe60a3bbfef33e45e8fe81c179de342ae3d8508c71f5d9b3"}, "51476963-e70d-43e5-a6a4-6848457832a9": {"doc_hash": "e5e9ef70879c68e9c60090c30fb7c56f10b62e93f1890e17b2d6fcfb3829f40f"}, "ad60ec12-2434-488c-89a7-6f9a21c74503": {"doc_hash": "af4c36cd6ff945590eb4125b096b5c748471e8c135cd50808e62e2e8c330d480"}, "809a37e7-8f8e-4a6d-8b66-6276a0665c73": {"doc_hash": "c9993f1cb22eb09a1ff8502cfebc15c3391ee0009d143e7e4a91e875d8b04d74"}, "10919fbc-8e30-4272-9692-40b17cbd4248": {"doc_hash": "7e671a438eee50543282f931afe8fff032f75b04f1932fdce21c3b1dc4bf4b39"}, "4d9374f3-8713-4746-88b7-6de3b8ccb2a0": {"doc_hash": "1aab5d05cea7bf8dca235f6cdedd2cb939e18ada1680ac5c68aaf1369c5f7a3d"}, "7eb0cc9d-5f11-47ec-97bb-688ba2146a76": {"doc_hash": "17e4e0b6f70eb20a897376fc9b2bbcd1e9e5fc7534dce19ccd5d3db1e7fcc876", "ref_doc_id": "acf592e6-9295-43cf-bbe5-9c8a014b88e5"}, "161d833c-af25-4eb3-910e-a34688815a0e": {"doc_hash": "da874c17ca300800bd00d168c2849a837fbfc05f78db963acb436798793276e4", "ref_doc_id": "6554357d-4926-41bb-ba09-2a30ad8ab2b7"}, "530b4333-1c1d-459c-9fd6-f31605152fc7": {"doc_hash": "f78998d09d50d8692a971c6845db1cafed67efb76d54fa8e8d36114cb3a496cc", "ref_doc_id": "6554357d-4926-41bb-ba09-2a30ad8ab2b7"}, "7f509bea-bf88-4dc4-bf7f-cdecc69b51aa": {"doc_hash": "e9a6dfb706ec9f92d878801b48ff0bc7239b76a08951adb52e3c33eaadbc3d44", "ref_doc_id": "7350dff5-ca52-492b-9708-b7b75bfd56db"}, "2b7b9716-5929-49cf-8265-478b14c4d4ad": {"doc_hash": "13ee51aaa17328062f1fe16e83088ede09a618b8a4bf7ff750e84284369e3d0a", "ref_doc_id": "7350dff5-ca52-492b-9708-b7b75bfd56db"}, "39964f9b-a72a-4ec6-bbba-a745f799bdeb": {"doc_hash": "994fb44696ce951f1e708830fb8f4ec1315be92934f4ca16514cbbfd732d6afd", "ref_doc_id": "7350dff5-ca52-492b-9708-b7b75bfd56db"}, "0b281146-76ed-4052-b392-f4866df7a9f9": {"doc_hash": "1801d6c6d2e350a5447f1e2f0abdc7c723ff92df0d91abac37f1d2c8d8ebc981", "ref_doc_id": "7350dff5-ca52-492b-9708-b7b75bfd56db"}, "78d4d6dd-f2e9-4781-95dc-04b3a03f848f": {"doc_hash": "64ac8ac41112006219377059fc635e241d1ef58c123cdada47edc5df4c561233", "ref_doc_id": "7350dff5-ca52-492b-9708-b7b75bfd56db"}, "86d0853d-c2a3-46e1-ac1c-e829a085c814": {"doc_hash": "f0861fda7069af7f849336da854a13c82bee7918486c76f4efbbbbe5eef5dd23", "ref_doc_id": "7350dff5-ca52-492b-9708-b7b75bfd56db"}, "ee303cf7-04f5-42ae-9181-8b4f9d225214": {"doc_hash": "94785f92b8990e73b070a47a9b8e3a0c6655d4c961380de83d08d2f8a633cf26", "ref_doc_id": "7350dff5-ca52-492b-9708-b7b75bfd56db"}, "7d628fe1-636e-414e-8473-cdbb5222e352": {"doc_hash": "2ef57b8924a7e9990c0f7f492e070284e4ebe7aebb7e72d3087d69f3ef398d29", "ref_doc_id": "ac547bcb-2d29-43e1-ab9b-b2dfc38e2757"}, "773858b9-6987-4c5f-85ff-09360d93b19f": {"doc_hash": "77fd019502f4577a1ca4a478894a9b2f7a23d149425649e6a334acc7d106aadc", "ref_doc_id": "ac547bcb-2d29-43e1-ab9b-b2dfc38e2757"}, "11c66f9d-b201-410b-9bc5-403b02e1fd39": {"doc_hash": "d3f65920026b74e00e6688a8c458d3b2b9ddf527fd1bdefc8437af79e68fae5d", "ref_doc_id": "ac547bcb-2d29-43e1-ab9b-b2dfc38e2757"}, "3438890e-9827-4b9a-b240-daaacafe4f4e": {"doc_hash": "31a5564dd1cf8b0ce82050c61f11a3004ed9bcbe128749fd91db0aa41ac38410", "ref_doc_id": "ac547bcb-2d29-43e1-ab9b-b2dfc38e2757"}, "f44313e1-0f4d-4ddf-a22a-00ee400bceec": {"doc_hash": "0749232f1841ea2f0a449cb550ae529adeb2ea9a0d8d66084f88b0e064c288c1", "ref_doc_id": "ef2eb8b6-8e82-4ab5-9926-3541b5aa7d8e"}, "8dfaba2a-f2da-41f1-a929-7abfcef5a112": {"doc_hash": "8ce388970bd3b49a8d94b1726605cf0e7950ad0941fcf555fe233c7008a85dce", "ref_doc_id": "ef2eb8b6-8e82-4ab5-9926-3541b5aa7d8e"}, "c39324ab-fda1-4623-8062-418b3f6249b1": {"doc_hash": "50064676bb960f567e924be1bfc296f8bf9a8314339f9c13f73bab6da7e2274e", "ref_doc_id": "ef2eb8b6-8e82-4ab5-9926-3541b5aa7d8e"}, "ff5492a3-4e5d-41a8-bc14-8bd7828667fb": {"doc_hash": "d98a4a338492176fcef657cdec8e7479e892d3e7b687c76ec189fdee1ab448ae", "ref_doc_id": "ef2eb8b6-8e82-4ab5-9926-3541b5aa7d8e"}, "bbe0f3d3-22e0-42c8-bc46-0c8da58a9254": {"doc_hash": "9b15c36c5f2d8a01fb73d4959c1579ae892e3f4037c2d31ac2f7805937e26f06", "ref_doc_id": "fb4ea85d-82fb-46f1-bb96-06835299f948"}, "40e718a6-fcc6-4777-9461-16f7e65759a3": {"doc_hash": "08d98ff2eb8dbd094c6afb4c232eb5520e0db79066795e417bed5bdb2ecf3c47", "ref_doc_id": "a0df6c12-be5a-438a-bea1-e3142635b74e"}, "873c6f4d-f39b-4882-9e3b-59204bc8ee70": {"doc_hash": "40111c118eba4bbf141016a973d83a814269db64efc9abf331bd0df7a715ed18", "ref_doc_id": "3eed4cb8-08a2-4d0b-8393-36f2c3eceb8b"}, "60608057-4f7e-4585-917e-0e6005242122": {"doc_hash": "2cd8e58106c0eacb051aba09179fb825aa06d4063c7da70e145ab79d1e368a66", "ref_doc_id": "bdb898f8-0891-45bc-ae70-c691039e0a17"}, "fac5522a-de1d-49c1-9a80-fafc453d8291": {"doc_hash": "d1e1f44b6ad74b1df5608c47601dbc5d4939d308b0be3fb163203da47ebef1f7", "ref_doc_id": "f61369ac-29e5-4502-a61c-592a8c2fcf67"}, "ba0e7b24-3a37-456a-a2ce-a71d59d3605f": {"doc_hash": "2f44dba3fee6113e8cd46fbcbea601d4cd681248183c08b98f1d105583dab919", "ref_doc_id": "36e4c4a0-8bab-4984-95e5-6730b539c32c"}, "a36f4b89-16f5-4e52-b40d-27435ba5e371": {"doc_hash": "f434e1d28c35cc4d7b570b2f5583be3924d8c549fa13dd721e14bef82c5a683d", "ref_doc_id": "36e4c4a0-8bab-4984-95e5-6730b539c32c"}, "382d0fa1-99c2-49b3-8138-b29689495db6": {"doc_hash": "5797920a9699d48883a7fb90cdfb721da17e2129b6be135fbb784e9c605356c2", "ref_doc_id": "9d28bed6-b420-4177-a957-99164f1f3b8e"}, "d7d1f3ef-f306-45fd-9f5d-ec66756ccb78": {"doc_hash": "80e95066cbebf161566539b1656cd4569f008473f64ea26180196c57bf56c47f", "ref_doc_id": "9d28bed6-b420-4177-a957-99164f1f3b8e"}, "b22e5180-934e-4c10-8007-fb78a7fed541": {"doc_hash": "f55914395cc83f60fb8c6e3268e252041b31e18d919e8c9bdaba0d916143675d", "ref_doc_id": "b83f35e1-2afe-456a-b61b-a3ae742985bb"}, "e6751b41-c8fd-435b-96c8-eca5b6e95391": {"doc_hash": "cd2eb004d230ecdb72b1343f2eef404c773e409defd74cf35a55fb7f55c2f6fa", "ref_doc_id": "a81a760d-f444-4b72-a2a7-50810d061857"}, "9f204945-3cca-4ccb-8176-aa27b08aebcc": {"doc_hash": "9975369eeb050baa2f7f969aec37e5bc01459f2e8a39a6ea66768598b0ac78b0", "ref_doc_id": "a81a760d-f444-4b72-a2a7-50810d061857"}, "ec6d42bb-c7d0-4020-8512-c7aae69e3bfb": {"doc_hash": "ed90c7205b7b33b7e12f9566b5d0f87405b35f8a07420b18c4c0d405f367ca1b", "ref_doc_id": "ac046439-d904-452e-9cbe-da9420a97f05"}, "f3ef1694-aa9f-47e8-8a66-e6ce07d19e3f": {"doc_hash": "9e613f28d61a81870dd300bd890798cd07e5654544ee013cb523b16f5a9b376b", "ref_doc_id": "ac046439-d904-452e-9cbe-da9420a97f05"}, "af63cc9f-3ed5-49d0-bb5c-01881023f736": {"doc_hash": "ff9e93eb7d7e42447e03f05be1f19427c9937028cd5131e4292587b5d9b612bf", "ref_doc_id": "f920d13a-e0d1-450b-8b67-1a59681e69a3"}, "bef711b3-dbce-45fb-a965-c317a14f6402": {"doc_hash": "d64b625c8c5cb9503877abe9623ab9c29777a2a4667fa5a5a544036e92e0f1ca", "ref_doc_id": "f920d13a-e0d1-450b-8b67-1a59681e69a3"}, "f2be6baf-755e-4ef2-88ef-92fbee2bfa55": {"doc_hash": "f0c204ebcdec82251cfa7a5ade33bf2c90d449138537742d274cf160610f913a", "ref_doc_id": "7a860a07-9471-4ede-aeb8-20d98007a3e1"}, "2992647c-a0da-491b-a2e1-8b3aa6064f60": {"doc_hash": "ac2cd5b08c04bfbbe108f1e2561590e4534cb73aa0401b7d23e8a115613195b8", "ref_doc_id": "7a860a07-9471-4ede-aeb8-20d98007a3e1"}, "c4dc81e3-77aa-4752-b9b5-48561c1a5231": {"doc_hash": "e9362ff2e0adec55fe60a3bbfef33e45e8fe81c179de342ae3d8508c71f5d9b3", "ref_doc_id": "e97950f3-7089-482d-9562-744934d0b313"}, "f4c12ecc-b531-4069-8937-f43d4a15cd58": {"doc_hash": "e5e9ef70879c68e9c60090c30fb7c56f10b62e93f1890e17b2d6fcfb3829f40f", "ref_doc_id": "51476963-e70d-43e5-a6a4-6848457832a9"}, "ebe2528f-d00a-43ec-b384-ffe25dfcbe20": {"doc_hash": "af4c36cd6ff945590eb4125b096b5c748471e8c135cd50808e62e2e8c330d480", "ref_doc_id": "ad60ec12-2434-488c-89a7-6f9a21c74503"}, "75e847c2-5502-48b1-8e0d-5d41e41c7716": {"doc_hash": "c9993f1cb22eb09a1ff8502cfebc15c3391ee0009d143e7e4a91e875d8b04d74", "ref_doc_id": "809a37e7-8f8e-4a6d-8b66-6276a0665c73"}, "8e2cfce8-713f-4e23-be73-13a2e249139f": {"doc_hash": "7e671a438eee50543282f931afe8fff032f75b04f1932fdce21c3b1dc4bf4b39", "ref_doc_id": "10919fbc-8e30-4272-9692-40b17cbd4248"}, "7f208b55-e402-4c7f-98ff-453712472e7a": {"doc_hash": "1aab5d05cea7bf8dca235f6cdedd2cb939e18ada1680ac5c68aaf1369c5f7a3d", "ref_doc_id": "4d9374f3-8713-4746-88b7-6de3b8ccb2a0"}}, "docstore/data": {"7eb0cc9d-5f11-47ec-97bb-688ba2146a76": {"__data__": {"id_": "7eb0cc9d-5f11-47ec-97bb-688ba2146a76", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "acf592e6-9295-43cf-bbe5-9c8a014b88e5", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "75715c50b932d47d1959a3b5b52a7fd380751cbc8e7c18b27ed3d05664085189", "class_name": "RelatedNodeInfo"}}, "text": "# 403 Forbidden\n\nRequest forbidden by administrative rules.", "start_char_idx": 0, "end_char_idx": 59, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "161d833c-af25-4eb3-910e-a34688815a0e": {"__data__": {"id_": "161d833c-af25-4eb3-910e-a34688815a0e", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6554357d-4926-41bb-ba09-2a30ad8ab2b7", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "5e34f5b14204cdb11f412636ca335860f4be0221a3528c6d439b488fb18dd997", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "530b4333-1c1d-459c-9fd6-f31605152fc7", "node_type": "1", "metadata": {}, "hash": "a97f315051673ece8b4a009e54ba0dcfc460b4fd8bc0946a48a068b12ec2890e", "class_name": "RelatedNodeInfo"}}, "text": "[ \ud30c\uc774\ud1a0\uce58 \ud55c\uad6d \uc0ac\uc6a9\uc790 \ubaa8\uc784 ](/)\n\n#  [Salesforce, InstructBLIP \ubaa8\ub378\uc758 \ub17c\ubb38 / \ucf54\ub4dc / \uac00\uc911\uce58 \uacf5\uac1c](/t/salesforce-\ninstructblip/1571)\n\n[ \uc77d\uc744\uac70\ub9ac&\uc815\ubcf4\uacf5\uc720 ](/c/news/14)\n\n[lavis](https://discuss.pytorch.kr/tag/lavis),\n[blip2](https://discuss.pytorch.kr/tag/blip2),\n[minigpt-4](https://discuss.pytorch.kr/tag/minigpt-4),\n[instructblip](https://discuss.pytorch.kr/tag/instructblip),\n[salesforce](https://discuss.pytorch.kr/tag/salesforce),\n[gpt-4](https://discuss.pytorch.kr/tag/gpt-4),\n[multimodal](https://discuss.pytorch.kr/tag/multimodal), [vision-\nlanguage](https://discuss.pytorch.kr/tag/vision-language)\n\n[9bow](https://discuss.pytorch.kr/u/9bow) (\ubc15\uc815\ud658)  5\uc6d4 17, 2023, 9:59\uc624\uc804  1\n\nSalesforce\uc5d0\uc11c BLIP-2 \ubaa8\ub378\uc5d0 \uc774\uc5b4 InstructBLIP \ubaa8\ub378\uc758 \ub17c\ubb38\uacfc \uad6c\ud604, \uadf8\ub9ac\uace0 \ud559\uc2b5\ub41c \uac00\uc911\uce58\ub97c \uacf5\uac1c\ud588\uc2b5\ub2c8\ub2e4.\n\n##  InstructBLIP: Towards General-purpose Vision-Language Models with\nInstruction Tuning\n\n[![image](https://discuss.pytorch.kr/uploads/default/original/2X/7/720b1a5fdeccd3fb8e52ace10a474304e32beee9.jpeg)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\n\uc544\ub798\uc640 \uac19\uc774 Vicuna, T5\ub97c \uc0ac\uc6a9\ud55c 2 \uc885\ub958\uc758 \ubaa8\ub378\uc774 \uc788\uc73c\uba70,\n\n    \n    \n    # ==================================================\n    # Architectures                  Types\n    # ==================================================\n    # blip2_vicuna_instruct          vicuna7b, vicuna13b\n    # blip2_t5_instruct              flant5xl, flant5xxl\n    \n\n~~[Salesforce\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 LAVIS \ud328\ud0a4\uc9c0](https://github.com/salesforce/LAVIS)(`pip\ninstall salesforce-lavis`)\ub97c \uc124\uce58\ud558\uc5ec \ubc14\ub85c \uc0ac\uc6a9\ud574 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.~~  \n\uc544\uc9c1 PyPI\uc758 \ud328\ud0a4\uc9c0\uc5d0\ub294 InstructBLIP \ubaa8\ub378\ub4e4\uc774 \ubc18\uc601\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc544\uc11c, GitHub\uc5d0\uc11c \uc9c1\uc811 \uc124\uce58\ud558\uc154\uc57c \ud569\ub2c8\ub2e4.\n\n    \n    \n    git clone https://github.com/salesforce/LAVIS.git\n    cd LAVIS\n    pip install -e .\n    \n    \n    \n    from lavis.models import load_model_and_preprocess\n    # loads InstructBLIP model\n    model, vis_processors, _ = load_model_and_preprocess(name=\"blip2_vicuna_instruct\", model_type=\"vicuna7b\", is_eval=True, device=device)\n    # prepare the image\n    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n    \n\n  \n\ucf54\ub4dc\uc640 \uc0ac\uc6a9\ubc95\uc774 \uad81\uae08\ud558\uc2e0 \ubd84\ub4e4\uaed8\uc11c\ub294 GitHub \uc800\uc7a5\uc18c\uc5d0\uc11c,\n\n![](https://github.githubassets.com/favicons/favicon.svg)\n[github.com](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\n### [LAVIS/projects/instructblip at main \u00b7\nsalesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\n[main/projects/instructblip](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\nLAVIS - A One-stop Library for Language-Vision Intelligence\n\n  \n\ub17c\ubb38\uc774 \uad81\uae08\ud558\uc2e0 \ubd84\ub4e4\uaed8\uc11c\ub294 arXiv\uc5d0\uc11c \ubc14\ub85c \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \n~~(\uc800\ub294 \uc624\ub298\ub3c4 \uc77d\uae30 \ud050\uc5d0 \ub123\uae30\ub9cc \ud558\uace0\n\uc788\uc2b5\ub2c8\ub2e4;;;![:sweat_smile:](https://discuss.pytorch.kr/images/emoji/apple/sweat_smile.png?v=12)\n)~~\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/c/c683569a48ce1952ba841c851ae3b1f282d4b00f.png)\n[arXiv.org](https://arxiv.org/abs/2305.06500)\n\n!", "start_char_idx": 0, "end_char_idx": 2748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "530b4333-1c1d-459c-9fd6-f31605152fc7": {"__data__": {"id_": "530b4333-1c1d-459c-9fd6-f31605152fc7", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6554357d-4926-41bb-ba09-2a30ad8ab2b7", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "5e34f5b14204cdb11f412636ca335860f4be0221a3528c6d439b488fb18dd997", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "161d833c-af25-4eb3-910e-a34688815a0e", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "da874c17ca300800bd00d168c2849a837fbfc05f78db963acb436798793276e4", "class_name": "RelatedNodeInfo"}}, "text": "~~(\uc800\ub294 \uc624\ub298\ub3c4 \uc77d\uae30 \ud050\uc5d0 \ub123\uae30\ub9cc \ud558\uace0\n\uc788\uc2b5\ub2c8\ub2e4;;;![:sweat_smile:](https://discuss.pytorch.kr/images/emoji/apple/sweat_smile.png?v=12)\n)~~\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/c/c683569a48ce1952ba841c851ae3b1f282d4b00f.png)\n[arXiv.org](https://arxiv.org/abs/2305.06500)\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/8/86a2c8ac52804c90ede238d99cb944037c6f82f6.png)\n\n### [InstructBLIP: Towards General-purpose Vision-Language Models with\nInstruction...](https://arxiv.org/abs/2305.06500)\n\nGeneral-purpose language models that can solve various language-domain tasks\nhave emerged driven by the pre-training and instruction-tuning pipeline.\nHowever, building general-purpose vision-language models is challenging due to\nthe increased task...\n\n  \n\ubb38\uc11c\uc758 \ub9c8\uc9c0\ub9c9\uc5d0 \uc81c\uc2dc\ub41c \uc774\ubbf8\uc9c0\uc758 \uc774\uc0c1\ud55c \ubd80\ubd84\uc744 \uc124\uba85\ud558\ub77c\ub294 \uc9c0\ubb38\uc5d0 \ub300\ud55c `InstructBLIP`\uacfc `GPT-4`, `miniGPT-4` \ub4f1\uc758\n\ub2f5\ubcc0\uc744 \ube44\uad50\ud574 \ub450\uc5c8\ub294\ub370 \uc778\uc0c1\uc801\uc774\ub124\uc694.\n![:monkey:](https://discuss.pytorch.kr/images/emoji/apple/monkey.png?v=12)\n\n![image](https://discuss.pytorch.kr/uploads/default/original/2X/e/ea9a956b0e6f6adb323ec01f0234cdac379b4d68.png)\n\n[[\ubb34\ub8cc/\uc628\ub77c\uc778/\uc601\uc5b4] ChatGPT\uc640 CLIP\uc744 \uc0ac\uc6a9\ud55c Semantic Visual Search\n\ub9cc\ub4e4\uae30](https://discuss.pytorch.kr/t/chatgpt-clip-semantic-visual-search/1731)\n\n[9bow](https://discuss.pytorch.kr/u/9bow) (\ubc15\uc815\ud658)  5\uc6d4 17, 2023, 10:04\uc624\uc804  2\n\n\ub17c\ubb38\uc5d0 \uc694\ub7f0 \uc0ac\ub840\ub3c4 \ucca8\ubd80\ub418\uc5b4 \uc788\ub124\uc694\n![:smiley:](https://discuss.pytorch.kr/images/emoji/apple/smiley.png?v=12)\n\n![image](https://discuss.pytorch.kr/uploads/default/original/2X/5/5f4b2578db249d9572ae390976096877be804d16.png)\n\n  * [\ud648 ](/)\n  * [\uce74\ud14c\uace0\ub9ac ](/categories)\n  * [FAQ/\uac00\uc774\ub4dc\ub77c\uc778 ](/guidelines)\n  * [\uc774\uc6a9\uc57d\uad00 ](/tos)\n  * [\uac1c\uc778\uc815\ubcf4 \ucde8\uae09\ubc29\uce68 ](/privacy)\n\n[Discourse](https://www.discourse.org)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. JavaScript\uac00 \ud65c\uc131\ud654\ub41c \uc0c1\ud0dc\uc5d0\uc11c \uac00\uc7a5 \uc798\n\ubcf4\uc785\ub2c8\ub2e4.", "start_char_idx": 2473, "end_char_idx": 4128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f509bea-bf88-4dc4-bf7f-cdecc69b51aa": {"__data__": {"id_": "7f509bea-bf88-4dc4-bf7f-cdecc69b51aa", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7350dff5-ca52-492b-9708-b7b75bfd56db", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e5a92a1dd37a5bcd469285e4832c07dfef4ea180c7c6e82a0f0a587e26bfa4f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b7b9716-5929-49cf-8265-478b14c4d4ad", "node_type": "1", "metadata": {}, "hash": "cb74bb9c8b1ff581cc1a7f9f4ea29431a022cec4c0e48143aa32ea34b37d9073", "class_name": "RelatedNodeInfo"}}, "text": "#  [ IBOK ](https://bo-10000.tistory.com/ \"IBOK\")\n\n  * [\ud648](/)\n\n\ud83c\udf0c Deep Learning/\ub17c\ubb38 \ub9ac\ubdf0 [KOR]\n\n## \uce74\uce74\uc624\ube0c\ub808\uc778 Multimodal LLM Honeybee \ub17c\ubb38 \ub9ac\ubdf0\n\n\ubcf5\ub9cc 2024\\. 3. 2. 16:09\n\n\uce74\uce74\uc624\ube0c\ub808\uc778\uc5d0\uc11c \uc791\ub144 \ub9d0 Multimodal LLM\uc778 Honeybee\ub97c \ubc1c\ud45c\ud588\ub2e4. \uc544\uc27d\uac8c\ub3c4 \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc740 \uc544\ub2c8\uace0 \uc601\uc5b4 \ubaa8\ub378\uc774\uace0, 5\uac1c\uc758\n\ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c SoTA\ub97c \ub2ec\uc131\ud588\ub2e4\uace0 \ud574\uc11c \ub274\uc2a4\uac00 \uc5c4\uccad \ub9ce\uc774 \ub098\uc654\ub2e4.\n\n\n\n\ub17c\ubb38: <https://arxiv.org/pdf/2312.06742.pdf>\n\n\uae43\ud5d9: <https://github.com/kakaobrain/honeybee>\n\n[  \n\nGitHub - kakaobrain/honeybee: The official implementation of project\n\"Honeybee\"\n\nThe official implementation of project \"Honeybee\". Contribute to\nkakaobrain/honeybee development by creating an account on GitHub.\n\ngithub.com\n\n](https://github.com/kakaobrain/honeybee)\n\n\n\n\n\n\n\n\n\n## **1\\. \ubc30\uacbd**\n\n\n\n**MLLM (Multimodal LLM)** \uc740 _**vision encoder, LLM, projector**_ \uc138\uac00\uc9c0\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/2LHDY/btsFoEXC9jw/KGnS1YJYP9zSFjO7MqM7D1/img.png)\n\n\n\n**vision encoder** \uacfc **LLM** \uc740 \uac01\uac01 \ub530\ub85c\ub530\ub85c \uc0ac\uc804\ud559\uc2b5\ub41c \uac83\uc744 \uc0ac\uc6a9\ud55c\ub2e4. \ub530\ub77c\uc11c \ub450 \ubaa8\ub378\uc744 \uc5f0\uacb0\ud574\uc8fc\uae30 \uc704\ud574\n**projector** \uac00 \ud544\uc694\ud558\ub2e4.  **projector\uc740 vision encoder\uc5d0\uc11c \ub098\uc628 visual feature\uc744 LLM\uc758\nfeature space\ub85c \ub9e4\ud551** \ud574\uc8fc\ub294 \uc5ed\ud560\uc744 \ud55c\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c **vision encoder\uacfc LLM\uc740 \uace0\uc815\ud574\ub450\uace0 projector\uc744\n\ud559\uc2b5** \ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\uc774 \uc9c4\ud589\ub41c\ub2e4.\n\n\n\n\n\n\ub530\ub77c\uc11c \uc774 projector\uc758 \uc5ed\ud560\uc774 \ub9e4\uc6b0 \uc911\uc694\ud55c\ub370, **\ud06c\uac8c \ub450 \uac00\uc9c0 \ud0c0\uc785\uc73c\ub85c \ub098\ub20c \uc218 \uc788\ub2e4.**\n\n\n\n![](https://blog.kakaocdn.net/dn/bsC2Wt/btsFnX4bBfH/qwb0kaNXJkxbezxzCkzTx0/img.png)\n\n\n\n\uccab\ubc88\uc9f8\ub294 LLaVA \ub4f1\uc5d0\uc11c \uc0ac\uc6a9\ud55c _**linear projector**_ \uc774\ub2e4. \ub9d0\uadf8\ub300\ub85c linear layer\uc744 \uc774\uc6a9\ud574 image\nfeature\uc744 \ubcc0\ud658\ud558\ub294 \ubc29\uc2dd\uc778\ub370, \uc774 \ubc29\ubc95\uc740 **feature\uc744 \uc77c\ub300\uc77c \ub9e4\ud551\ud574\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 \uacc4\uc0b0\ub7c9\uc774 \ub9ce\ub2e4** \ub294 \ub2e8\uc810\uc774 \uc788\ub2e4.\n\n\n\n\ub2e4\ub978 \ud558\ub098\ub294 _**Abstractor**_ \ub77c\uace0 \ubd88\ub9ac\ub294 \uae30\ubc95\uc73c\ub85c, InstructBLIP, BLIP-2, miniGPT-4 \ub4f1\uc5d0\uc11c \uc0ac\uc6a9\ud55c\n\ubc29\ubc95\uc774\ub2e4. \uc774\ub4e4\uc740 \uc815\ud574\uc9c4 \uc218\uc758 visual token\uc744 \ucd94\ucd9c\ud574 \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c, **visual token\uc758 \uc218\ub97c \uc801\uc808\ud558\uac8c \uc870\uc808\ud560 \uc218 \uc788\uc5b4\nflexibility\uc640 efficiency\uac00 \ub192\uc73c\ub098 information loss\uac00 \uc788\uc744 \uc218 \uc788\ub2e4.** Abstractor \ubc29\uc2dd\uc740\nresampler, Q-former \ub4f1\uc774 \uc788\ub2e4.\n\n\n\n\n\n\uc774\ub7ec\ud55c efficiency\uc640 flexibility \ub54c\ubb38\uc5d0 **\ucd5c\uadfc abstractor \ubc29\uc2dd\uc774 \ub9ce\uc774 \uc0ac\uc6a9\ub418\uace0 \uc788\ub2e4.** \uadf8\ub7ec\ub098\nabstractor\uc740 **locality preservation\uc774 \uc57d\ud558\ub2e4** \ub294 \ub2e8\uc810\uc774 \uc788\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/cPex1B/btsFoaWz72R/NVLm1YtB7iQAmc8vBnLNAk/img.png)\n\n\n\n\uc704 \uadf8\ub9bc\uc744 \ubcf4\uba74 \ud070 feature\uc778 man\ub9cc \uc7a1\uc544\ub0b4\uace0 pizza, glass \uac19\uc740 \uc560\ub4e4\uc740 \ubabb \uc7a1\uc544 \ub0b4\uace0 \uc788\ub294\uac78 \ubcfc \uc218 \uc788\ub2e4. \ub530\ub77c\uc11c\nspatial understanding \ub2a5\ub825\uc774 \ub5a8\uc5b4\uc9c4\ub2e4.", "start_char_idx": 0, "end_char_idx": 1960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b7b9716-5929-49cf-8265-478b14c4d4ad": {"__data__": {"id_": "2b7b9716-5929-49cf-8265-478b14c4d4ad", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7350dff5-ca52-492b-9708-b7b75bfd56db", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e5a92a1dd37a5bcd469285e4832c07dfef4ea180c7c6e82a0f0a587e26bfa4f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f509bea-bf88-4dc4-bf7f-cdecc69b51aa", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e9a6dfb706ec9f92d878801b48ff0bc7239b76a08951adb52e3c33eaadbc3d44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39964f9b-a72a-4ec6-bbba-a745f799bdeb", "node_type": "1", "metadata": {}, "hash": "c093dbd8492d212560a0ab72e6f1692902b6d95fbe836f33eacc9616503c00a3", "class_name": "RelatedNodeInfo"}}, "text": "\uc774\ub7ec\ud55c efficiency\uc640 flexibility \ub54c\ubb38\uc5d0 **\ucd5c\uadfc abstractor \ubc29\uc2dd\uc774 \ub9ce\uc774 \uc0ac\uc6a9\ub418\uace0 \uc788\ub2e4.** \uadf8\ub7ec\ub098\nabstractor\uc740 **locality preservation\uc774 \uc57d\ud558\ub2e4** \ub294 \ub2e8\uc810\uc774 \uc788\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/cPex1B/btsFoaWz72R/NVLm1YtB7iQAmc8vBnLNAk/img.png)\n\n\n\n\uc704 \uadf8\ub9bc\uc744 \ubcf4\uba74 \ud070 feature\uc778 man\ub9cc \uc7a1\uc544\ub0b4\uace0 pizza, glass \uac19\uc740 \uc560\ub4e4\uc740 \ubabb \uc7a1\uc544 \ub0b4\uace0 \uc788\ub294\uac78 \ubcfc \uc218 \uc788\ub2e4. \ub530\ub77c\uc11c\nspatial understanding \ub2a5\ub825\uc774 \ub5a8\uc5b4\uc9c4\ub2e4.\n\n\n\n\ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \ub2e8\uc810\uc744 \uadf9\ubcf5\ud558\uae30 \uc704\ud574 _**local context\ub97c \ubcf4\uc874\ud560 \uc218 \uc788\ub294 abstractor \ubc29\uc2dd**_ \uc744 \uc0c8\ub86d\uac8c\n\uc81c\uc548\ud558\uace0, \uc774\ub97c \uc801\uc6a9\ud55c MLLM\uc778 _**Honeybee**_ \ub97c \ubc1c\ud45c\ud588\ub2e4.\n\n\n\n\n\n\n\n## **\b2. Honeybee**\n\n\n\n![](https://blog.kakaocdn.net/dn/bHHahh/btsFsaapgzl/M2JR8DM52SpjwzxaQqKKVK/img.png)\n\n\n\nHoneybee\uc758 \uc804\uccb4 \uad6c\uc870\ub294 \uc704\uc640 \uac19\ub2e4. **vision encoder** \uc5d0\uc11c visual feature\uc744 \ucd94\ucd9c \ud6c4\n**projector** \uc744 \uac70\uccd0 visual token\uc73c\ub85c \ubcc0\ud658\ud558\uace0, text token\uacfc \ud568\uaed8 **LLM** \uc758 input\uc73c\ub85c \ub123\ub294\ub2e4.\n\n\n\n\uc5ec\uae30\uae4c\uc9c0\ub294 \uc5ec\ud0c0 MLLM\ub4e4\uacfc \ub3d9\uc77c\ud55c \uad6c\uc870\uc774\uace0, \ud575\uc2ec \uad6c\uc870\ub294 \uc0c8\ub86d\uac8c \uc81c\uc548\ud55c projector\uc778 _**C-abstractor**_ \uacfc\n_**D-abstractor**_ \uc774\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bMUtIq/btsFodZ9BI0/FCgCK1ItgJOt7BQYu50GK1/img.png)\n\n\n\n_**C-Abstractor**_ \uc740 local context\ub97c \uc798 \ud3ec\ucc29\ud558\ub294 convolution\uc744 \uc774\uc6a9\ub2e4. ResNet\uc744 \uc5ec\ub7ec\uac1c \uc313\uc544\nvisual token\uc744 \ucd94\ucd9c\ud55c\ub2e4.\n\n**_D-Abstractor_** \uc740 [DETR](https://arxiv.org/pdf/2005.12872.pdf)\uc5d0\uc11c \uc81c\uc548\ud55c\ndeformable attention\uc744 \uc774\uc6a9\ud558\uc5ec visual token\uc744 \ucd94\ucd9c\ud55c\ub2e4.\n\n\n\n\n\n\n\n## **3\\. \ud559\uc2b5\ubc29\ubc95**\n\n\n\n\ud559\uc2b5\uc740 \ub450\ub2e8\uacc4\ub85c \uc9c4\ud589\ub41c\ub2e4. \uccab\ubc88\uc9f8\ub85c vision encoder\uacfc LLM\uc740 freeze\ud558\uace0 abstractor\ub9cc \ud559\uc2b5\ud55c\ub2e4. \uadf8 \ub2e4\uc74c\uc73c\ub85c\nfreeze\ub97c \ud480\uace0 \ubaa8\ub4e0 parameter\uc744 \uc138\ubd80 \uc870\uc815\ud558\ub294 \ub2e8\uacc4\ub97c \uac70\uce5c\ub2e4.\n\n\n\nLLM\uc73c\ub85c\ub294 Vicuna-v1.5 (7B, 13B) \ub450\uac00\uc9c0 \ud06c\uae30\uc758 \ubaa8\ub378\uc744 \uc774\uc6a9\ud588\uace0, vision encoder\uc740 CLIP ViT-L/14\n\ubaa8\ub378\uc744 \uc774\uc6a9\ud588\ub2e4.\n\n\n\n\n\n\n\n## **4\\. \uc2e4\ud5d8\uacb0\uacfc**\n\n\n\n\uacb0\uacfc \uc694\uc57d - **5\uac1c bench\uc5d0\uc11c SoTA\ub97c \ub2ec\uc131\ud588\ub2e4.**\n\n\n\n![](https://blog.kakaocdn.net/dn/cwKCS4/btsFmmiWfD3/KbhYskCikJTU8e9SekjgvK/img.png)\n\n\n\n\ucc38\uace0\ub85c \uac01 bench\uc758 \uc608\uc2dc\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bCNB3J/btsFm7y7kIA/Z43wTCdauhfptj3yR94I31/img.png)\n\n\n\n\uc194\uc9c1\ud788 \uc0ac\ub78c\uc774 \ubd10\ub3c4 \uc880 \uc5b4\ub835\ub2e4.\n\n\n\n\ubcf4\ub2e4 \uc790\uc138\ud55c \uacb0\uacfc \uc9c0\ud45c\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/wrSvo/btsFuHS6tv0/AzYweMkg2mjAKsPnjyOczK/img.png)", "start_char_idx": 1642, "end_char_idx": 3374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39964f9b-a72a-4ec6-bbba-a745f799bdeb": {"__data__": {"id_": "39964f9b-a72a-4ec6-bbba-a745f799bdeb", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7350dff5-ca52-492b-9708-b7b75bfd56db", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e5a92a1dd37a5bcd469285e4832c07dfef4ea180c7c6e82a0f0a587e26bfa4f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b7b9716-5929-49cf-8265-478b14c4d4ad", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "13ee51aaa17328062f1fe16e83088ede09a618b8a4bf7ff750e84284369e3d0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b281146-76ed-4052-b392-f4866df7a9f9", "node_type": "1", "metadata": {}, "hash": "c6186340f522325ca540a135eeab2069f385018c17ec1ee761adb73bba2cb6a2", "class_name": "RelatedNodeInfo"}}, "text": "![](https://blog.kakaocdn.net/dn/cwKCS4/btsFmmiWfD3/KbhYskCikJTU8e9SekjgvK/img.png)\n\n\n\n\ucc38\uace0\ub85c \uac01 bench\uc758 \uc608\uc2dc\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bCNB3J/btsFm7y7kIA/Z43wTCdauhfptj3yR94I31/img.png)\n\n\n\n\uc194\uc9c1\ud788 \uc0ac\ub78c\uc774 \ubd10\ub3c4 \uc880 \uc5b4\ub835\ub2e4.\n\n\n\n\ubcf4\ub2e4 \uc790\uc138\ud55c \uacb0\uacfc \uc9c0\ud45c\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/wrSvo/btsFuHS6tv0/AzYweMkg2mjAKsPnjyOczK/img.png)\n\n\n\nQwen\uc774\ub098 LLaVA \ub4f1\uc740 \ub354 \ud070 vision encoder / image resolution / \ub354 \ub9ce\uc740 visual token\uc744\n\uc774\uc6a9\ud588\uc9c0\ub9cc Honeybee\uc758 \uc131\ub2a5\uc774 \ub354 \ub192\uc558\ub2e4\uace0 \ud55c\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/beW08S/btsFuNMzzrQ/binNxMunkybjmob5oJGtl1/img.png)\n\n\n\nHoneybee\ub3c4 \uc774\ub807\uac8c image resolution\uacfc visual token \uc218\ub97c \ub192\uc774\uba74 \uc131\ub2a5\uc774 \ub354 \uc0c1\uc2b9\ud55c\ub2e4\uace0 \ud55c\ub2e4.\n\n\n\n\n\n\ub2e4\uc74c\uc740 \uc2e4\ud5d8\ub2e8\uacc4\uc5d0\uc11c \uc138\uc6b4 \uac01 \uac00\uc124\uc5d0 \ub300\ud55c \uac80\uc99d\uc774\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/btmjs6/btsFn9i79Ck/4sL3JZmXCk62THWgXrCxS1/img.png)\n\n\n\nC/D-abstractor\uc774 local context preservation\uc5d0 \uc88b\ub2e4\ub294 \uac83\uc744 \ubcf4\uc774\uae30 \uc704\ud574 spatial\nunderstanding capability\ub97c \ubcfc \uc218 \uc788\ub294 task\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \uce21\uc815\ud588\ub2e4\uace0 \ud55c\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bVFBIL/btsFsaha1TU/65IB1parTaujQC7s2P1bT0/img.png)\n\n\n\n\uc704\ub294 performance\uc640 efficiency\uc5d0 \ub300\ud55c \ube44\uad50\uc774\ub2e4. linear\uc740 \uc55e\uc11c \ub9d0\ud588\ub4ef\uc774 \uc77c\ub300\uc77c \ub300\uc751\uc774\ub77c flexibility\uac00 \uc544\uc608\n\uc5c6\ub2e4. resampler\uacfc C-abstractor\uc740 flexible\ud558\uac8c \ub514\uc790\uc778\ud560 \uc218 \uc788\uc73c\uba70, visual token \uc218\uac00 \ub298\uc5b4\ub0a0\uc218\ub85d \uc131\ub2a5\uc774\n\uc99d\uac00\ud558\ub294 \uc591\uc0c1\uc744 \ubcf4\uc774\ub098 C-abstractor\uc758 \uc131\ub2a5\uc774 \ud6e8\uc52c \uc88b\ub2e4.\n\n\n\n\n\n\ub9c8\uc9c0\ub9c9\uc73c\ub85c Honeybee\uac00 \uc0dd\uc131\ud55c \ub2f5\ubcc0\uc758 \uc608\uc2dc\ub4e4\uc774\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bzQkIe/btsFoaWAfZe/FhzUQNAnB8V5QjIHVdpokk/img.png)\n![](https://blog.kakaocdn.net/dn/vpM1G/btsFqNzNeoe/RjiwgyCKodTVzGjsI8FwTK/img.png)", "start_char_idx": 3044, "end_char_idx": 4377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b281146-76ed-4052-b392-f4866df7a9f9": {"__data__": {"id_": "0b281146-76ed-4052-b392-f4866df7a9f9", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7350dff5-ca52-492b-9708-b7b75bfd56db", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e5a92a1dd37a5bcd469285e4832c07dfef4ea180c7c6e82a0f0a587e26bfa4f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39964f9b-a72a-4ec6-bbba-a745f799bdeb", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "994fb44696ce951f1e708830fb8f4ec1315be92934f4ca16514cbbfd732d6afd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78d4d6dd-f2e9-4781-95dc-04b3a03f848f", "node_type": "1", "metadata": {}, "hash": "d25457c36a5ef85fc0c7733b33d36ec9381399282eba582e42aaca7df69eb450", "class_name": "RelatedNodeInfo"}}, "text": "\ub9c8\uc9c0\ub9c9\uc73c\ub85c Honeybee\uac00 \uc0dd\uc131\ud55c \ub2f5\ubcc0\uc758 \uc608\uc2dc\ub4e4\uc774\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bzQkIe/btsFoaWAfZe/FhzUQNAnB8V5QjIHVdpokk/img.png)\n![](https://blog.kakaocdn.net/dn/vpM1G/btsFqNzNeoe/RjiwgyCKodTVzGjsI8FwTK/img.png)\n\n\n\n\ucc38 \uc798\ud558\ub124..\n\n\ubc18\uc751\ud615\n\n\uacf5\uc720\ud558\uae30\n\n\uac8c\uc2dc\uae00 \uad00\ub9ac\n\n_\uad6c\ub3c5\ud558\uae30_ **IBOK**\n\n#### '[\ud83c\udf0c Deep Learning](/category/%F0%9F%8C%8C%20Deep%20Learning) > [\ub17c\ubb38 \ub9ac\ubdf0\n[KOR]](/category/%F0%9F%8C%8C%20Deep%20Learning/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0%20%5BKOR%5D)'\n\uce74\ud14c\uace0\ub9ac\uc758 \ub2e4\ub978 \uae00\n\n[[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] MeZO: Fine-Tuning Language Models with Just Forward Passes\n(NeurIPS 2023)](/206)  (2) | 2024.01.28  \n---|---  \n[[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] AIM: Scalable Pre-training of Large Autoregressive Image Models\n(Apple, 2024)](/205)  (0) | 2024.01.21  \n[Apple\uc758 Multimodal LLM Ferret \ub17c\ubb38 \ub9ac\ubdf0](/203)  (2) | 2024.01.07  \n[[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-\ninvariant Weights (Naver AI Lab, ICLR 2021)](/195)  (0) | 2023.07.23  \n[[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] Audio-Visual Speech Enhancement Using Multimodal Deep\nConvolutional Neural Networks](/168)  (1) | 2022.09.21  \n  \n### Tag\n\n[mllm](/tag/mllm), [Multimodal](/tag/Multimodal)\n\n### '\ud83c\udf0c Deep Learning/\ub17c\ubb38 \ub9ac\ubdf0 [KOR]'\uc758 \ub2e4\ub978\uae00\n\n  * [\uc774\uc804\uae00 **[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] MeZO: Fine-Tuning Language Models with Just Forward Passes (NeurIPS 2023)**](/206)\n  * \ud604\uc7ac\uae00 **\uce74\uce74\uc624\ube0c\ub808\uc778 Multimodal LLM Honeybee \ub17c\ubb38 \ub9ac\ubdf0**\n  * \n\n### \uad00\ub828\uae00\n\n  * [ **[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] MeZO: Fine-Tuning Language Models with Just Forward Passes (NeurIPS 2023)** 2024.01.28 ](/206?category=948904)\n  * [ **[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] AIM: Scalable Pre-training of Large Autoregressive Image Models (Apple, 2024)** 2024.01.21 ](/205?category=948904)\n  * [ **Apple\uc758 Multimodal LLM Ferret \ub17c\ubb38 \ub9ac\ubdf0** 2024.01.07 ](/203?category=948904)\n  * [ **[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights (Naver AI Lab, ICLR 2021)** 2023.07.23 ](/195?category=948904)\n\n\ub313\uae00 1\n\n  * \n\n\ube44\ubc00\uae00 \ub4f1\ub85d\n\n!", "start_char_idx": 4177, "end_char_idx": 5994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78d4d6dd-f2e9-4781-95dc-04b3a03f848f": {"__data__": {"id_": "78d4d6dd-f2e9-4781-95dc-04b3a03f848f", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7350dff5-ca52-492b-9708-b7b75bfd56db", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e5a92a1dd37a5bcd469285e4832c07dfef4ea180c7c6e82a0f0a587e26bfa4f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b281146-76ed-4052-b392-f4866df7a9f9", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "1801d6c6d2e350a5447f1e2f0abdc7c723ff92df0d91abac37f1d2c8d8ebc981", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86d0853d-c2a3-46e1-ac1c-e829a085c814", "node_type": "1", "metadata": {}, "hash": "697a21797801cda05c0908a47976b6ea6fddc204c310322cc10495416648bc51", "class_name": "RelatedNodeInfo"}}, "text": "[\ud504\ub85c\ud544\uc0ac\uc9c4](https://tistory1.daumcdn.net/tistory/3487102/attach/fb04976601014f93b22d0aff6d652500)\n\n\ud83d\udc2c\n\n  * [ \ubd84\ub958 \uc804\uccb4\ubcf4\uae30 (174) ](/category)\n    * [ \ud83c\udf0c Deep Learning (50) ](/category/%F0%9F%8C%8C%20Deep%20Learning)\n      * [ \ub17c\ubb38 \ub9ac\ubdf0 [KOR] (24) ](/category/%F0%9F%8C%8C%20Deep%20Learning/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0%20%5BKOR%5D)\n      * [ Paper Review [ENG] (0) ](/category/%F0%9F%8C%8C%20Deep%20Learning/Paper%20Review%20%5BENG%5D)\n      * [ DL & ML \uc870\uac01 \uc9c0\uc2dd (4) ](/category/%F0%9F%8C%8C%20Deep%20Learning/DL%20%26%20ML%20%EC%A1%B0%EA%B0%81%20%EC%A7%80%EC%8B%9D)\n      * [ Overview (6) ](/category/%F0%9F%8C%8C%20Deep%20Learning/Overview)\n      * [ Dataset (3) ](/category/%F0%9F%8C%8C%20Deep%20Learning/Dataset)\n      * [ \ud3c9\uac00 (3) ](/category/%F0%9F%8C%8C%20Deep%20Learning/%ED%8F%89%EA%B0%80)\n      * [ Implementation (6) ](/category/%F0%9F%8C%8C%20Deep%20Learning/Implementation)\n      * [ Etc. (4) ](/category/%F0%9F%8C%8C%20Deep%20Learning/Etc.)\n    * [ \ud83d\udc0d Python & library (49) ](/category/%F0%9F%90%8D%20Python%20%26%20library)\n      * [ Python (5) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Python)\n      * [ PyTorch (18) ](/category/%F0%9F%90%8D%20Python%20%26%20library/PyTorch)\n      * [ PyTorch Lightning (2) ](/category/%F0%9F%90%8D%20Python%20%26%20library/PyTorch%20Lightning)\n      * [ Tensorflow (1) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Tensorflow)\n      * [ Flax (0) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Flax)\n      * [ HuggingFace (5) ](/category/%F0%9F%90%8D%20Python%20%26%20library/HuggingFace)\n      * [ Scikit-Learn (4) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Scikit-Learn)\n      * [ numpy (2) ](/category/%F0%9F%90%8D%20Python%20%26%20library/numpy)\n      * [ librosa (4) ](/category/%F0%9F%90%8D%20Python%20%26%20library/librosa)\n      * [ SimpleITK (4) ](/category/%F0%9F%90%8D%20Python%20%26%20library/SimpleITK)\n      * [ Etc. (4) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Etc.)", "start_char_idx": 5994, "end_char_idx": 7945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86d0853d-c2a3-46e1-ac1c-e829a085c814": {"__data__": {"id_": "86d0853d-c2a3-46e1-ac1c-e829a085c814", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7350dff5-ca52-492b-9708-b7b75bfd56db", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e5a92a1dd37a5bcd469285e4832c07dfef4ea180c7c6e82a0f0a587e26bfa4f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78d4d6dd-f2e9-4781-95dc-04b3a03f848f", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "64ac8ac41112006219377059fc635e241d1ef58c123cdada47edc5df4c561233", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee303cf7-04f5-42ae-9181-8b4f9d225214", "node_type": "1", "metadata": {}, "hash": "9b6f58cd4998afa03b9216eefd4a5d3c6595359c5b7703353c9c864e7fc4f95a", "class_name": "RelatedNodeInfo"}}, "text": "(4) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Etc.)\n    * [ \ud83d\udc7b OS & Tools (33) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools)\n      * [ Ubuntu (14) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/Ubuntu)\n      * [ Mac (3) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/Mac)\n      * [ Windows (1) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/Windows)\n      * [ VSCode (3) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/VSCode)\n      * [ Git (3) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/Git)\n      * [ LaTeX (8) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/LaTeX)\n      * [ Tools (1) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/Tools)\n    * [ \ud83d\udc7d Language & Frameworks (6) ](/category/%F0%9F%91%BD%20Language%20%26%20Frameworks)\n      * [ Matlab (1) ](/category/%F0%9F%91%BD%20Language%20%26%20Frameworks/Matlab)\n      * [ Spark (5) ](/category/%F0%9F%91%BD%20Language%20%26%20Frameworks/Spark)\n    * [ \ud83d\udd2c Medical Image (13) ](/category/%F0%9F%94%AC%20Medical%20Image)\n      * [ MRI (3) ](/category/%F0%9F%94%AC%20Medical%20Image/MRI)\n      * [ Processing (5) ](/category/%F0%9F%94%AC%20Medical%20Image/Processing)\n      * [ \ub17c\ubb38 \ub9ac\ubdf0 (5) ](/category/%F0%9F%94%AC%20Medical%20Image/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)\n    * [ \ud83d\udca9 \uc5d0\ub7ec \ud574\uacb0 (7) ](/category/%F0%9F%92%A9%20%EC%97%90%EB%9F%AC%20%ED%95%B4%EA%B2%B0)\n    * [ \ud83d\udd11 CS (11) ](/category/%F0%9F%94%91%20CS)\n      * [ \ucf54\ub529\ud14c\uc2a4\ud2b8 (10) ](/category/%F0%9F%94%91%20CS/%EC%BD%94%EB%94%A9%ED%85%8C%EC%8A%A4%ED%8A%B8)\n      * [ \uc54c\uace0\ub9ac\uc998 (1) ](/category/%F0%9F%94%91%20CS/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)\n    * [ \ud83e\udd68 \uc774\uac83\uc800\uac83 (5) ](/category/%F0%9F%A5%A8%20%EC%9D%B4%EA%B2%83%EC%A0%80%EA%B2%83)\n    * [ \ud83d\udcd6 \ub3c5\ud6c4\uac10 (0) ](/category/%F0%9F%93%96%20%EB%8F%85%ED%9B%84%EA%B0%90)\n\n\ubc18\uc751\ud615\n\n### \ucd5c\uadfc\uae00\uacfc \uc778\uae30\uae00\n\n  * \ucd5c\uadfc\uae00\n  * \uc778\uae30\uae00\n\n  * [\n\n**\uce74\uce74\uc624\ube0c\ub808\uc778 Multimodal LLM Honeybee \ub17c\ubb38 \ub9ac\ubdf0** 2024.03.02 16:09 ](/207)\n\n  * [\n\n**[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] MeZO: Fine-Tuning Language Models with Just Forward Passes \u22ef**\n2024.01.", "start_char_idx": 7886, "end_char_idx": 9778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee303cf7-04f5-42ae-9181-8b4f9d225214": {"__data__": {"id_": "ee303cf7-04f5-42ae-9181-8b4f9d225214", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7350dff5-ca52-492b-9708-b7b75bfd56db", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e5a92a1dd37a5bcd469285e4832c07dfef4ea180c7c6e82a0f0a587e26bfa4f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86d0853d-c2a3-46e1-ac1c-e829a085c814", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "f0861fda7069af7f849336da854a13c82bee7918486c76f4efbbbbe5eef5dd23", "class_name": "RelatedNodeInfo"}}, "text": "03.02 16:09 ](/207)\n\n  * [\n\n**[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] MeZO: Fine-Tuning Language Models with Just Forward Passes \u22ef**\n2024.01.28 22:42 ](/206)\n\n  * [\n\n**[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] AIM: Scalable Pre-training of Large Autoregressive Image Mo\u22ef**\n2024.01.21 23:03 ](/205)\n\n  * [\n\n**\ub0b4\uac00 \ubcf4\ub824\uace0 \uc815\ub9ac\ud558\ub294 LaTex \uc790\uc8fc \uc4f0\ub294 \uc218\uc2dd \uc815\ub9ac** 2021.11.16 15:29 ](/97)\n\n  * [\n\n**LaTex \ud45c \uad00\ub828 \ud301 (\ud45c \uc790\ub3d9 \uc0dd\uc131\uae30, \ud3f0\ud2b8 \ud06c\uae30 \uc870\uc815, \uc140 \ub108\ube44, \ud45c \ub0b4\ubd80 \uc5ec\ubc31, footnote \ub2ec\uae30)** 2022.04.26\n22:14 ](/128)\n\n  * [ **[HuggingFace] Trainer \uc0ac\uc6a9\ubc95** 2022.07.23 15:27 ](/154)\n\n### \ucd5c\uadfc\ub313\uae00\n\n  * [ **\ub17c\ubb38\uc744 \ubcf4\uba74 ResNet-D\ub294 ResNet-B\uc5d0 average pooling\uc744 \ucd94\uac00\ud55c \ud615\ud0dc\u22ef**\n\n\ud589\uc778\n\n](/133#comment21315301)\n\n  * [ **https://m.blog.naver.com/edennnie/223155243141**\n\nhmm\n\n](/73#comment21295201)\n\n  * [ **nvidia-smi\ub294 GPU\ub4dc\ub77c\uc774\ubc84\uac00 \uc9c0\uc6d0\ud558\ub294 \ucd5c\uc2e0 CUDA\ubc84\uc804\uc774\uace0, nvcc --vers\u22ef**\n\nhmm\n\n](/73#comment21295174)\n\n### \ubc29\ubb38\uc790\uc218Total\n\n413,313\n\n  * Today : 729\n  * Yesterday : 867\n\n### Calendar\n\n[\u00ab](/archive/202404 \"1\uac1c\uc6d4 \uc55e\uc758 \ub2ec\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\")   [2024/05](/archive/202405 \"\ud604\uc7ac \ub2ec\uc758\n\ub2ec\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\")   [\u00bb](/archive/202406 \"1\uac1c\uc6d4 \ub4a4\uc758 \ub2ec\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\") \uc77c | \uc6d4 | \ud654 | \uc218 | \ubaa9 | \uae08\n| \ud1a0  \n---|---|---|---|---|---|---  \n|  |  | 1 | 2 | 3 | 4  \n5 | 6 | 7 | 8 | 9 | 10 | 11  \n12 | 13 | 14 | 15 | 16 | 17 | 18  \n19 | 20 | 21 | 22 | 23 | 24 | 25  \n26 | 27 | 28 | 29 | 30 | 31 |  \n  \nCopyright \u00a9 Kakao Corp. All rights reserved.\n\n\uad00\ub828\uc0ac\uc774\ud2b8\n\n  * [Github](https://github.com/bo-10000)\n\n## \ud2f0\uc2a4\ud1a0\ub9ac\ud234\ubc14\n\n**IBOK** _\uad6c\ub3c5\ud558\uae30_", "start_char_idx": 9666, "end_char_idx": 10959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d628fe1-636e-414e-8473-cdbb5222e352": {"__data__": {"id_": "7d628fe1-636e-414e-8473-cdbb5222e352", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac547bcb-2d29-43e1-ab9b-b2dfc38e2757", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "ca09af74149531c3cbf8092dadf93c614c0cd52ca6f091fbd74c81f8a544e4f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "773858b9-6987-4c5f-85ff-09360d93b19f", "node_type": "1", "metadata": {}, "hash": "ff3dc708ce816629efd01f2f0d3bfadcc674537da03491320e9c49f21e8d2864", "class_name": "RelatedNodeInfo"}}, "text": "![](https://www.facebook.com/tr?id=579484675976917&ev=PageView&noscript=1)\n\n[](/home)\n\n[\ucee4\ub9ac\uc5b4\ub9ac \ud2b8\ub80c\ub4dc\n\n](/trends)[\ud604\uc9c1\uc790 Q&A](/qnas)\n\n[\uc0ac\uc774\ub4dc \ud504\ub85c\uc81d\ud2b8\n\n](/lounge)\n\n\ub85c\uadf8\uc778\n\n\ud68c\uc6d0\uac00\uc785\n\n[![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![\uc18c\uc131\uc740\ub2d8\uc758\n\ud504\ub85c\ud544 \uc0ac\uc9c4](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\uc18c\uc131\uc740\n\nSolutions Architect @ AWS\n\n](/profiles/16843)\n\n\ud314\ub85c\uc6b0\n\n# Salesforce\uc758 InstructBLIP\n\n\uc5bc\ub9c8 \uc804\uc5d0 Salesforce \uc5d0\uc11c BLIP2\ub97c \uacf5\uac1c\ud588\uc5c8\uc2b5\ub2c8\ub2e4. \ub192\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 Vision-Language LLM \uc774\uc5c8\uace0, \ud14c\uc2a4\ud2b8\ub97c \ud574\n\ubd24\uc744 \ub54c \uaf64 \ud765\ubbf8\ub85c\uc6b4 \uacb0\uacfc (\uc608\ub97c \ub4e4\uc5b4 \uc774\ubbf8\uc9c0 \ub0b4\uc758 \uc0ac\ub78c\uc774\ub098 \uac1d\uccb4 \uac2f\uc218\ub97c \uc798 \uce74\uc6b4\ud305\ud558\ub294 \ub4f1) \ub97c \ubcf4\uc5ec\uc8fc\uae30\ub3c4 \ud574\uc11c \ubc1c\uc804 \uc18d\ub3c4\uac00 \ub180\ub78d\ub2e4\uace0 \uc0dd\uac01\uc744\n\ud588\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ubc88\uc5d0 Salesforce\uc5d0\uc11c \uadf8 \ud6c4\uc18d\uc73c\ub85c InstrcutBLIP\uc744 \uacf5\uac1c\ud558\uc600\uc2b5\ub2c8\ub2e4. \\- \ucf54\ub4dc:\nhttps://github.com/salesforce/LAVIS/tree/main/projects/instructblip \uae30\uc874\uc758 BLIP2\n\uae30\ubc18\uc73c\ub85c \uc5ec\ub7ec \uac00\uc9c0 public dataset \uc744 \uac00\uc838\ub2e4\uac00 instruct tuning\uc774 \uac00\ub2a5\ud55c \ud615\ud0dc\ub85c \ub9cc\ub4e4\uc5b4\uc11c fine-tuning\uc744\n\uc9c4\ud589\ud55c \uac83\uc774\uace0, \ub9e4\uc6b0 \ud070 Vision-Language \ub370\uc774\ud130\uc14b\uc778 Flamingo\ub97c \ud3ec\ud568\ud55c \uc5ec\ub7ec \ub370\uc774\ud130\uc5d0 \ub300\ud574\uc11c zero-shot \uc5d0\uc11c\nSOTA\ub97c \ub2ec\uc131\ud558\uc600\ub2e4\uace0 \ud569\ub2c8\ub2e4. \ub17c\ubb38\uc744 \uc0b4\ud3b4\ubcf4\uba74 \uad6c\uc870 \uc790\uccb4\ub294 \uae30\uc874 BLIP2 \uc640 \ub2e4\ub97c \uac83\uc774 \ubcc4\ub85c \uc5c6\uc2b5\ub2c8\ub2e4. BLIP\uc758 \ud575\uc2ec\uc778 Q-former\n\ubd80\ubd84\uc5d0 \uae30\uc874\uc5d0\ub294 Query + Text \ud615\ud0dc\ub85c \ub123\ub358 \uac83\uc5d0\uc11c text\ub97c instruction \uc73c\ub85c \ud55c \uc815\ub3c4\ub9cc \ucc28\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n(Instruct-tuning \uc774\ubbc0\ub85c \ub2f9\uc5f0\ud55c \uac83\uc785\ub2c8\ub2e4\ub9cc...\u314e\u314e) \uc774\ubc88 \uc5f0\uad6c\ub294 \ubaa8\ub378 \uc544\ud0a4\ud14d\uccd0 \ub4f1\uc774 \uac1c\uc120\ub418\uc5c8\ub2e4\uae30 \ubcf4\ub2e4\ub294, \uc5ec\ub7ec \ub370\uc774\ud130\uc14b\uc5d0\n\ub300\ud574\uc11c fine-tuning\uc744 \uc9c4\ud589\ud558\uace0 \uc774 \ubaa8\ub378\uc744 \uacf5\uac1c\ud55c \uac83\uc5d0 \uc758\uc758\uac00 \uc788\ub2e4\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 Vicuna\uc640 FlanT5\uae30\ubc18\uc73c\ub85c \ub41c\n\ubaa8\ub378\uc744 \uacf5\uac1c\ud558\uc600\ub294\ub370 MiniGPT4 (https://github.com/Vision-CAIR/MiniGPT-4) \uc640 \ube44\uc2b7\ud558\ub124\uc694. \uc5b4\ucc0c\ub418\uc5c8\uac74\n\ucd5c\uadfc\uc5d0 Vision/Language\ub97c \uac19\uc774 \ub2e4\ub8e8\ub294 LLM\ub4e4\uc774 \ub9ce\uc544\uc9c0\uace0 \uc788\uace0 \uc810\uc810 \ub354 \uc5ec\ub7ec modality \ub85c \ud655\uc7a5\ub418\uc5b4 \uac00\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n\uadf8\uc640\ub294 \ubcc4\uac1c\ub85c Meta\uc758 OPT, LLaMA\ub098 Google\uc758 FlanT5 \ub4f1\uc740 \uc624\ud508\uc18c\uc2a4 \ucabd\uc5d0 \uc815\ub9d0 \ud070 \uae30\uc5ec\ub97c \ud558\uace0 \uc788\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n\n[LAVIS/projects/instructblip at main \u00b7 salesforce/LAVIS\n\nGitHub\n\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27160%27%20height=%27120%27/%3e)![LAVIS/projects/instructblip\nat main \u00b7\nsalesforce/LAVIS](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)!", "start_char_idx": 0, "end_char_idx": 1830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "773858b9-6987-4c5f-85ff-09360d93b19f": {"__data__": {"id_": "773858b9-6987-4c5f-85ff-09360d93b19f", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac547bcb-2d29-43e1-ab9b-b2dfc38e2757", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "ca09af74149531c3cbf8092dadf93c614c0cd52ca6f091fbd74c81f8a544e4f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d628fe1-636e-414e-8473-cdbb5222e352", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "2ef57b8924a7e9990c0f7f492e070284e4ebe7aebb7e72d3087d69f3ef398d29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11c66f9d-b201-410b-9bc5-403b02e1fd39", "node_type": "1", "metadata": {}, "hash": "d585fb613ce1388719ca9f76720d4356078fc896fac5e0bf1cb6abac40009f33", "class_name": "RelatedNodeInfo"}}, "text": "\uadf8\uc640\ub294 \ubcc4\uac1c\ub85c Meta\uc758 OPT, LLaMA\ub098 Google\uc758 FlanT5 \ub4f1\uc740 \uc624\ud508\uc18c\uc2a4 \ucabd\uc5d0 \uc815\ub9d0 \ud070 \uae30\uc5ec\ub97c \ud558\uace0 \uc788\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n\n[LAVIS/projects/instructblip at main \u00b7 salesforce/LAVIS\n\nGitHub\n\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27160%27%20height=%27120%27/%3e)![LAVIS/projects/instructblip\nat main \u00b7\nsalesforce/LAVIS](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![LAVIS/projects/instructblip\nat main \u00b7\nsalesforce/LAVIS](/_next/image?url=https%3A%2F%2Fpubly.imgix.net%2Farticle-\nthumbnails%2F2023.05%2F61115babdeb468d80642377c4d3e698ddf46eed0f9fc19c9e9d40415fa084290.webp%3Fw%3D200%26h%3D200&w=384&q=75)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\n\ub2e4\uc74c \ub0b4\uc6a9\uc774 \uad81\uae08\ud558\ub2e4\uba74?\n\n![kakao_icon](/_next/static/images/img_symbol-\nkakao-b1fe6197135e5beead71b1a90f8e2b7d.png)\uce74\uce74\uc624\ub85c 3\ucd08\ub9cc\uc5d0 \uac00\uc785\ud558\uae30\n\n\ub610\ub294\n\n\uc774\uba54\uc77c\ub85c \uac00\uc785\ud558\uae30\n\n\uc774\ubbf8 \ud68c\uc6d0\uc774\uc2e0\uac00\uc694?\n\n\ub85c\uadf8\uc778\ud558\uae30\n\n2023\ub144 5\uc6d4 14\uc77c \uc624\ud6c4 3:31\n\n\uc88b\uc544\uc694 **2**\n\n\u2022\n\n\ub9ac\ud3ec\uc2a4\ud2b8 **1**\n\n[ \uc800\uc7a5 **2** \u2022 \uc870\ud68c **2,804**\n\n](/comments/83593)\n\n\uc88b\uc544\uc694\n\n\ub9ac\ud3ec\uc2a4\ud2b8\n\n### \ub313\uae00 0\n\n![](/_next/static/images/img_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png)\n\n* * *\n\n* * *\n\n* * *\n\n### \ube44\uc2b7\ud55c \uac8c\uc2dc\ubb3c\n\n### \uc8fc\uac04 \uc778\uae30 TOP 10\n\n[1![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\uc774\uc7a5\uadfc NAVER \ub370\uc774\ud130\ubd84\uc11d\n\n\ud68c\uc0ac \ud14c\ud06c \ube14\ub85c\uadf8 \ubaa8\uc74c \ud83d\udc68\ud83c\udffb\u200d\ud83d\udcbb\n\n](/comments/103945)[2![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\ud55c\uc815\uc218 Software Engineer\n\n498\uac1c\uc758 \ud14c\uc2a4\ud2b8 \ucf54\ub4dc (\ubc88\uc5ed)\n\n](/comments/103928)[3![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)!", "start_char_idx": 1419, "end_char_idx": 3298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11c66f9d-b201-410b-9bc5-403b02e1fd39": {"__data__": {"id_": "11c66f9d-b201-410b-9bc5-403b02e1fd39", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac547bcb-2d29-43e1-ab9b-b2dfc38e2757", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "ca09af74149531c3cbf8092dadf93c614c0cd52ca6f091fbd74c81f8a544e4f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "773858b9-6987-4c5f-85ff-09360d93b19f", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "77fd019502f4577a1ca4a478894a9b2f7a23d149425649e6a334acc7d106aadc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3438890e-9827-4b9a-b240-daaacafe4f4e", "node_type": "1", "metadata": {}, "hash": "b1d540f414fe5aa9d3ac0beb44f8a4efe47e86485f5b5723186eda49034f7fce", "class_name": "RelatedNodeInfo"}}, "text": "[](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\ud55c\uc815\uc218 Software Engineer\n\n498\uac1c\uc758 \ud14c\uc2a4\ud2b8 \ucf54\ub4dc (\ubc88\uc5ed)\n\n](/comments/103928)[3![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\uc774\uc591\uc77c NaverCloud \ubc31\uc5d4\ub4dc \uac1c\ubc1c\uc790\n\n\ud83c\udf10 \uac1c\ubc1c\uc790\uac00 \uc54c\uace0 \uc788\uc73c\uba74 \uc88b\uc740 \uc0ac\uc774\ud2b8\n\n](/comments/103947)[4![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\ud558\uc870\uc740 Software Engineer\n\n\uc131\uc7a5\uc758 \uc774\uc720\n\n](/comments/103875)[5![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\uae40\uc758\uc911 \uc704\ub300\ud55c\uc0c1\uc0c1 \uac1c\ubc1c\uc790\n\nReact 19 \ubca0\ud0c0 \ucd9c\uc2dc: Actions\uc758 \ub3c4\uc785\uacfc \uc0c8\ub85c\uc6b4 \ube44\ub3d9\uae30\n\n](/comments/103907)\n\n[6![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\uc1a1\uc694\ucc3d \ubc30\ubbfc \ud504\ub860\ud2b8\uc5d4\ub4dc \ud504\ub85c\uadf8\ub798\uba38\n\n\uc2e0\uc785 \uac1c\ubc1c\uc790\uc758 \ud55c \ub2ec\n\n](/comments/103929)[7![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\ub178\uc218\uc9c4 \ub274\uc695\uc758 \uc2a4\ud0c0\ud2b8\uc5c5 \ud504\ub85c\uadf8\ub798\uba38\n\n\ucf54\ub529\ud558\ub2e4\uac00 \b\ub9c9\ud614\uc744\ub54c \uadf9\ubcf5\ud558\ub294 5\uac00\uc9c0 \ubc29\ubc95\n\n](/comments/103985)[8![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)!", "start_char_idx": 2986, "end_char_idx": 4858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3438890e-9827-4b9a-b240-daaacafe4f4e": {"__data__": {"id_": "3438890e-9827-4b9a-b240-daaacafe4f4e", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac547bcb-2d29-43e1-ab9b-b2dfc38e2757", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "ca09af74149531c3cbf8092dadf93c614c0cd52ca6f091fbd74c81f8a544e4f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11c66f9d-b201-410b-9bc5-403b02e1fd39", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "d3f65920026b74e00e6688a8c458d3b2b9ddf527fd1bdefc8437af79e68fae5d", "class_name": "RelatedNodeInfo"}}, "text": "[](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\ub178\uc218\uc9c4 \ub274\uc695\uc758 \uc2a4\ud0c0\ud2b8\uc5c5 \ud504\ub85c\uadf8\ub798\uba38\n\n\ucf54\ub529\ud558\ub2e4\uac00 \b\ub9c9\ud614\uc744\ub54c \uadf9\ubcf5\ud558\ub294 5\uac00\uc9c0 \ubc29\ubc95\n\n](/comments/103985)[8![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\uc774\uba85\uc9c4 \ud55c\uad6d\ub808\ub4dc\ud587 \uc194\ub8e8\uc158 \uc544\ud0a4\ud14d\ud2b8\n\n\ube44\uc96c\uc5bc \uc2a4\ud29c\ub514\uc624 \ucf54\ub4dc\uc5d0\uc11c Llama 3 \uc2e4\ud589\ud558\uae30\n\n](/comments/103969)[9![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\n\ub808\ub4dc\ubc84\uc2a4\ubc31\ub9e8 \ub9ac\uc11c\ucc98 \uc564 \ub77c\uc774\ud130\n\n\u27ea\uc131\uc219\ud55c \uc0ac\ub78c\uc774 \ub418\uae30 \uc704\ud55c 10\uac00\uc9c0 \ub178\ub825\u27eb\n\n](/comments/103955)[10![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2780%27/%3e)![](/_next/image?url=%2F_next%2Fstatic%2Fimages%2Fimg_profile-\ndummy-f39ccb87481ab4a70525a9d2d461307d.png&w=256&q=75)\n\nK\ub9ac\uadf8 \ud504\ub85c\uadf8\ub798\uba38 \ucee4\ud53c\ud55c\uc794 \uac1c\ubc1c\uc790\n\n\uc9d1\uc5d0\uc11c \uc11c\ubc84\ub97c \uc6b4\uc601\ud558\ub294 \uac8c \uac00\ub2a5\ud55c\uac00\uc694?\n\n](/comments/103919)\n\n### \ucd94\ucc9c \ud504\ub85c\ud544", "start_char_idx": 4543, "end_char_idx": 5682, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f44313e1-0f4d-4ddf-a22a-00ee400bceec": {"__data__": {"id_": "f44313e1-0f4d-4ddf-a22a-00ee400bceec", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef2eb8b6-8e82-4ab5-9926-3541b5aa7d8e", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "48ca68d1d4319fbf473cca18126a799b74f721922930358eeba288b0003dd320", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dfaba2a-f2da-41f1-a929-7abfcef5a112", "node_type": "1", "metadata": {}, "hash": "461d6188672094f1b330e2e6b66d543ac2e45bb8ed6c5c90bfb59f60585b0726", "class_name": "RelatedNodeInfo"}}, "text": "# [AttentionX](https://attentionx.github.io/)\n\n# ![image](https://i.ibb.co/jJJV9fs/1686903632416.jpg) AttentionX: AI Research\n& Startup Group\n\n## \ub3d9\uc544\ub9ac \ubaa9\ud45c/\ud65c\ub3d9\n\n  1. \ucee8\ud37c\ub7f0\uc2a4 \ub17c\ubb38 Accept\n  2. \uc624\ud508 \uc18c\uc2a4 \uae30\uc5ec\n  3. AI \uc2a4\ud0c0\ud2b8\uc5c5 \uc591\uc131\n\n## \ud83d\udca1 \ub3d9\uc544\ub9ac \uc18c\uac1c\n\n\uc548\ub155\ud558\uc138\uc694 \uc800\ud76c\ub294 \uc11c\uc6b8\ub300\ud559\uad50, \uce74\uc774\uc2a4\ud2b8, \uc678\uad6d \ub300\ud559\uad50 (Berkeley, Stanford, CMU, Oxford)\uc758\n\ud559\ubd80\uc0dd/\ub300\ud559\uc6d0\uc0dd\ub4e4(\uae40\uac74\ud76c, \uc11c\ubbfc\uc900, \uc774\uac15\uc6b1 \uad50\uc218\ub2d8 \uc5f0\uad6c\uc2e4)\uacfc ML \ud604\uc9c1\uc790\ub4e4(\uce74\uce74\uc624 \ube0c\ub808\uc778, \ud06c\ub798\ud504\ud1a4 \ub4f1)\uc774 \ud65c\ub3d9\ud574\uc628 \uac1c\ubc1c\uc790\uc640 \uc5f0\uad6c\uc790,\n\uc2a4\ud0c0\ud2b8\uc5c5 \ucc3d\uc5c5\uac00\ub4e4\uc758 \ucee4\ubba4\ub2c8\ud2f0\uc785\ub2c8\ub2e4.  \n\uc800\ud76c\ub294 LLM, Multimodal Language Model, Generative Model (3D, Audio, Video)\uc5d0 \uad00\ud55c\n\uc5f0\uad6c\ub97c \ud1b5\ud574 \ub17c\ubb38 \uc2e4\uc801\uc744 \ub0b4\uace0 \uc5f0\uad6c \uae30\ubc18 \ud504\ub85c\ub355\ud2b8 \uac1c\ubc1c\uacfc \uc2a4\ud0c0\ud2b8\uc5c5 \ube4c\ub529\uc744 \ud558\ub294 \uc870\uc9c1\uc785\ub2c8\ub2e4!\n\n### \u2728 2023\ub144 \ud65c\ub3d9 \uc694\uc57d\n\n  1. \ub17c\ubb38 \uc131\uacfc \n    * InstructBLIP PEFT\ud300, [NeurIPS Workshop 2023](https://neurips2023-enlsp.github.io/accepted_papers.html#:~:text=Parameter%2DEfficient%20Fine%2Dtuning%20of%20InstructBLIP%20for%20Visual%20Reasoning%20Tasks) accept (\uc11c\uc6b8\ub300\ud559\uad50 \uc774\uc7ac\uc724 \uad50\uc218\ub2d8 \uad50\uc2e0 \uc800\uc790)\n    * [Video-LLaMA Drive](https://github.com/sungyeonparkk/vision-assistant-for-driving)\ud300, WACV Workshop 2024 accept (\uc11c\uc6b8\ub300\ud559\uad50 \uae40\ub3d9\uaddc \uad50\uc218\ub2d8 \uad50\uc2e0 \uc800\uc790)\n  2. \uc2a4\ud0c0\ud2b8\uc5c5 \n    * 1\uae30 \uc2a4\ud0c0\ud2b8\uc5c5 2\ud300, [Krew Capital](https://krewcapital.com/) \ud22c\uc790 \uc720\uce58 \n      * [fastrepl](https://fastrepl.com)\n      * [Weavel](https://www.promptmodel.run)\n    * 2\ud300, YC W24 \uc778\ud130\ubdf0\n  3. \ub9e4\uc2a4\ud504\ub808\uc18c\uc640 \ud611\uc5c5, Math LLM \uc5f0\uad6c \ud504\ub85c\uc81d\ud2b8 \uc9c4\ud589\n  4. \ud574\ucee4\ud1a4 \uc131\uacfc \n    1. Weavel \ud300, OpenAI \uc8fc\uad00 [Prompter Day \ud574\ucee4\ud1a4](https://www.prompterday.com/main) 5\ub4f1 (500\ub9cc\uc6d0 \uc218\uc0c1)\n    2. [Agent Eval](https://www.youtube.com/watch?v=sjEMBY3Ngbk): [AGI House Agent \ud574\ucee4\ud1a4](https://partiful.com/e/I4oVKOY4DXEG5Bn9U61h) \uc6b0\uc2b9\n    3. LAWI \ud300, [\ud504\ub77c\uc774\uba38 Gen AI \ud574\ucee4\ud1a4](https://www.newswire.co.kr/newsRead.php?no=965386) \uacb0\uc120 \uc9c4\ucd9c\n  5. \ud504\ub85c\ub355\ud2b8 \uac1c\ubc1c & \ub7f0\uce6d \n    * [NALY](https://disquiet.io/@marc/makerlog/8017) (\uc2a4\ub9c8\ud2b8 \uc2a4\ud53c\ucee4): \ud504\ub9ac\uc624\ub354 800\ub9cc\uc6d0 \uae30\ub85d\n    * [\uac1c\uc778\ud654 \ucc57\ubd07, \ud504\ub85c\ub355\ud2b8 \ud5cc\ud2b8 Product of the Day 3\ub4f1](https://www.producthunt.com/products/is-it-you#is-it-you)\n  6.", "start_char_idx": 0, "end_char_idx": 1679, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dfaba2a-f2da-41f1-a929-7abfcef5a112": {"__data__": {"id_": "8dfaba2a-f2da-41f1-a929-7abfcef5a112", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef2eb8b6-8e82-4ab5-9926-3541b5aa7d8e", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "48ca68d1d4319fbf473cca18126a799b74f721922930358eeba288b0003dd320", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f44313e1-0f4d-4ddf-a22a-00ee400bceec", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0749232f1841ea2f0a449cb550ae529adeb2ea9a0d8d66084f88b0e064c288c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c39324ab-fda1-4623-8062-418b3f6249b1", "node_type": "1", "metadata": {}, "hash": "ad7ff7b40d67646ab0c48fa4a38873d98bef7db22270f6ec22dfe3ab2d36d5ef", "class_name": "RelatedNodeInfo"}}, "text": "LAWI \ud300, [\ud504\ub77c\uc774\uba38 Gen AI \ud574\ucee4\ud1a4](https://www.newswire.co.kr/newsRead.php?no=965386) \uacb0\uc120 \uc9c4\ucd9c\n  5. \ud504\ub85c\ub355\ud2b8 \uac1c\ubc1c & \ub7f0\uce6d \n    * [NALY](https://disquiet.io/@marc/makerlog/8017) (\uc2a4\ub9c8\ud2b8 \uc2a4\ud53c\ucee4): \ud504\ub9ac\uc624\ub354 800\ub9cc\uc6d0 \uae30\ub85d\n    * [\uac1c\uc778\ud654 \ucc57\ubd07, \ud504\ub85c\ub355\ud2b8 \ud5cc\ud2b8 Product of the Day 3\ub4f1](https://www.producthunt.com/products/is-it-you#is-it-you)\n  6. \uc624\ud508\uc18c\uc2a4 Project \n    * [TestGPT](https://github.com/AttentionX/testGPT): TDD\ub85c GPT \uad6c\ud604 (Andrej Karpathy\uc758 NanoGPT \uae30\ubc18)\n    * [InstructBLIP Fine-tuning](https://github.com/salesforce/LAVIS/issues/302#issuecomment-1931380335)\n  7. \uc624\ud508\uc18c\uc2a4 Contribution \n    * lit-llama: [PR#357](https://github.com/Lightning-AI/lit-llama/pull/357), [PR#238](https://github.com/Lightning-AI/lit-llama/pull/238), [PR#242](https://github.com/Lightning-AI/lit-llama/pull/242)\n  8. \uc11c\uc6b8\ub300\ud559\uad50 \uacf5\uacfc\ub300\ud559, GPU \ud074\ub7ec\uc2a4\ud130 \uc9c0\uc6d0 (a100 32\uac1c \uacf5\ub3d9 \uc0ac\uc6a9)\n\n### \ud83d\udd2c 3\uae30 \ud504\ub85c\uc81d\ud2b8\ub4e4\n\n  1. Autonomous Driving Assistant (\ubc15\uc131\uc5f0, \uace0\ub824\ub300 \uae40\uc9c4\uaddc \uad50\uc218\ub2d8 \uc9c0\ub3c4)\n  2. Math LLM (\uc870\uc8fc\ud658, \ub9e4\uc2a4\ud504\ub808\uc18c \ud611\uc5c5)\n  3. In-Context Knowledge Distillation (\uae40\ubbfc\ucc2c)\n  4. Controlled 3D Generation (3D MM) (\uae40\uc131\uacbd, \uce74\uc774\uc2a4\ud2b8 \uc131\ubbfc\ud601 \uad50\uc218\ub2d8 \uc9c0\ub3c4)\n  5. Cache Merging & Spend Less on V.A.T (\uc548\uc601\uc9c4)\n  6. Dynamic 3D Generation (\uc774\uc544\ub2f4, \uace0\ub824\ub300 \uae40\uc2b9\ub8e1 \uad50\uc218\ub2d8 \uc9c0\ub3c4)\n  7. LLM jailbreaking \uacf5\uaca9/\ubc29\uc5b4 (\uc720\uc0c1\uc724)\n  8. Video Highlight/Captioning (\ud55c\ub3d9\ud6c8)\n  9. Non-invasive biopotential decoding (\uae40\ub3c4\uc5fd)\n  10. Robot safety reward model (\ud64d\uc21c\ubc94)\n  11. Music Generation Research (\uace0\uacc4\ud6c8)\n  12. Plug-and-Play Knowledge Injection for LLMs (\ubc15\uc740\ud658)\n  13. Resolve LLM ambiguity by throwing questions (\uc11d\uc8fc\uc601)\n  14. Reflective Learning (\uc2e0\uc2b9\uc724)\n\n### \ud83d\udc68\u200d\ud83d\udc68\u200d\ud83d\udc67\u200d\ud83d\udc67 \uba64\ubc84\n\n  1. [1\uae30 \uba64\ubc84](https://abecid.notion.site/AttentionX-1-1fd7b9c8efb0422c969c877c8d1c09c4?pvs=4)\n  2. [2\uae30 \uba64\ubc84](https://abecid.notion.site/AttentionX-2-fd86468a8dbd436dab29ef10f5553da1?pvs=4)\n  3.", "start_char_idx": 1390, "end_char_idx": 3018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c39324ab-fda1-4623-8062-418b3f6249b1": {"__data__": {"id_": "c39324ab-fda1-4623-8062-418b3f6249b1", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef2eb8b6-8e82-4ab5-9926-3541b5aa7d8e", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "48ca68d1d4319fbf473cca18126a799b74f721922930358eeba288b0003dd320", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dfaba2a-f2da-41f1-a929-7abfcef5a112", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "8ce388970bd3b49a8d94b1726605cf0e7950ad0941fcf555fe233c7008a85dce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff5492a3-4e5d-41a8-bc14-8bd7828667fb", "node_type": "1", "metadata": {}, "hash": "0f62f76e82867024a4c95a0384033ae2ebe81b1a186fe3531e43d95a0c4c360c", "class_name": "RelatedNodeInfo"}}, "text": "Music Generation Research (\uace0\uacc4\ud6c8)\n  12. Plug-and-Play Knowledge Injection for LLMs (\ubc15\uc740\ud658)\n  13. Resolve LLM ambiguity by throwing questions (\uc11d\uc8fc\uc601)\n  14. Reflective Learning (\uc2e0\uc2b9\uc724)\n\n### \ud83d\udc68\u200d\ud83d\udc68\u200d\ud83d\udc67\u200d\ud83d\udc67 \uba64\ubc84\n\n  1. [1\uae30 \uba64\ubc84](https://abecid.notion.site/AttentionX-1-1fd7b9c8efb0422c969c877c8d1c09c4?pvs=4)\n  2. [2\uae30 \uba64\ubc84](https://abecid.notion.site/AttentionX-2-fd86468a8dbd436dab29ef10f5553da1?pvs=4)\n  3. [3\uae30 \uba64\ubc84](https://abecid.notion.site/AttentionX-3-936500ae929c487b82bb5d5cbbedd07f?pvs=4)\n\n\uba64\ubc84 \uc18c\uac1c \uc800\ud76c\ub294 \uc11c\uc6b8\ub300\ud559\uad50, \uce74\uc774\uc2a4\ud2b8, \uc678\uad6d \ub300\ud559\uad50 (Berkeley, Stanford, CMU, Oxford) \ucd9c\uc2e0\uc758 \uc5f0\uad6c\uc790, \uac1c\ubc1c\uc790\n\uc704\uc8fc\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uace0 \uae40\uac74\ud76c, \uc11c\ubbfc\uc900, \uc8fc\uc7ac\uac78, \uae40\uc8fc\ud638, \uae40\uc601\ubbfc, \ud669\uc2b9\uc6d0, Kwangwook Lee, Trevor Darrell,\nChelsea Finn \uad50\uc218\ub2d8 \ub7a9 \uc778\ud134\uacfc \uc11d\ubc15\uc0ac \ubd84\ub4e4\uc774 \ud65c\ub3d9\ud574\uc654\uc2b5\ub2c8\ub2e4! \uc5f0\uad6c \ud300\ub4e4\uc740 \uc131\ubbfc\ud601, \uc774\uae30\ubbfc, \uae40\uc2b9\ub8e1, \uc774\uc7ac\uc724 \uad50\uc218\ub2d8, \ucf74\ub2e4 \uc5f0\uad6c\ud300\uacfc\n\uc18c\ud1b5\uc744 \ud558\uac70\ub098 \uc9c0\ub3c4\ub97c \ubc1b\ub294 \ud300\ub4e4\uc774 \uc788\uace0 \ud504\ub85c\ub355\ud2b8 \ud300\ub4e4\uc740 \ub450 \ud300\uc774 \ud06c\ub8e8 \uce90\ud53c\ud138\uc5d0\uc11c \ud22c\uc790\ub97c \ubc1b\uace0 \ub450 \ud300\uc774 YC W24 \uc778\ud130\ubdf0\ub97c \ubcf4\uace0 \ud504\ub85c\ub355\ud2b8\n\uac1c\ubc1c\uc744 \ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. (\uc800\ud76c \uc870\uc9c1\uacfc fit\uc774 \ub9de\uc744\uac83 \uac19\uc740 \ubd84\ub4e4\uc740 \uc5b8\uc81c\ub4e0\uc9c0 \uc774\uba54\uc77c\ub85c \uc5f0\ub77d\uc8fc\uc138\uc694!)\n\n### \ud83e\udd35 \uac8c\uc2a4\ud2b8\n\n1\uae30 \uac8c\uc2a4\ud2b8 1\\. [\ub93c\ud2bc](https://wrtn.ai/) \uc774\uc138\uc601 \ub300\ud45c\ub2d8  \n2\\. [\ud300\ub7ec\ub108\uc2a4](https://www.learners.company/) \ub9f9\uc8fc\uc131 \ub300\ud45c\ub2d8  \n3\\. [\ub9c1\uae00](https://www.ringleplus.com/ko/student/landing/team) \uc774\uc131\ud30c \ub300\ud45c\ub2d8  2\uae30 \uac8c\uc2a4\ud2b8\n1\\. Krew Capital \uc1a1\ubbfc\uc7ac\ub2d8, \ubbfc\ubcd1\ud6c8\ub2d8  \n2\\. Neuralkind \uae40\uc900\ud76c\ub2d8  \n3\\. \ub775\uc2a4\ud50c\ub85c\uc6b0 \uc724\ud76c\uc0c1\ub2d8  \n4\\. \ucf54\ub974\uce74 \uc774\ud0dc\ud638\ub2d8  \n5\\. \ub93c\ud2bc \ud604\uc9c0\uc6c5\ub2d8  \n6\\. \ud574\uce58\ub7a9\uc2a4 \uae40\ubbfc\uc11d\ub2d8  3\uae30 \uac8c\uc2a4\ud2b8 1\\. VRCrew \ucd5c\uc131\uad11 \ub300\ud45c\ub2d8  \n2\\. Shift-up \uae40\ud0dc\ud6c8\ub2d8  \n3\\. Beeble.ai \uae40\ud6c8 \ub300\ud45c\ub2d8  \n\n### \ud83d\udd16 \uc804 \uae30\uc218 \ud65c\ub3d9\n\n  * [1\uae30 \ud65c\ub3d9](https://abecid.notion.site/1-e41e8583e724455ca1959d24332246e5?pvs=4)\n  * [2\uae30 \ud65c\ub3d9](https://abecid.notion.site/2-142fe495a46c4875bc1bef11ac2739f6?pvs=4)\n\n### \ud83d\udcda \uc790\ub8cc\uc2e4\n\n  1. [\ub17c\ubb38 \ub9ac\ubdf0 \uc138\uc158](https://skillful-houseboat-70c.notion.site/Study-Sessions-614371ce36a64c318b6c6bf3980f4467)\n  2.", "start_char_idx": 2636, "end_char_idx": 4140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff5492a3-4e5d-41a8-bc14-8bd7828667fb": {"__data__": {"id_": "ff5492a3-4e5d-41a8-bc14-8bd7828667fb", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef2eb8b6-8e82-4ab5-9926-3541b5aa7d8e", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "48ca68d1d4319fbf473cca18126a799b74f721922930358eeba288b0003dd320", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c39324ab-fda1-4623-8062-418b3f6249b1", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "50064676bb960f567e924be1bfc296f8bf9a8314339f9c13f73bab6da7e2274e", "class_name": "RelatedNodeInfo"}}, "text": "Shift-up \uae40\ud0dc\ud6c8\ub2d8  \n3\\. Beeble.ai \uae40\ud6c8 \ub300\ud45c\ub2d8  \n\n### \ud83d\udd16 \uc804 \uae30\uc218 \ud65c\ub3d9\n\n  * [1\uae30 \ud65c\ub3d9](https://abecid.notion.site/1-e41e8583e724455ca1959d24332246e5?pvs=4)\n  * [2\uae30 \ud65c\ub3d9](https://abecid.notion.site/2-142fe495a46c4875bc1bef11ac2739f6?pvs=4)\n\n### \ud83d\udcda \uc790\ub8cc\uc2e4\n\n  1. [\ub17c\ubb38 \ub9ac\ubdf0 \uc138\uc158](https://skillful-houseboat-70c.notion.site/Study-Sessions-614371ce36a64c318b6c6bf3980f4467)\n  2. [NLP \uae30\ucd08 \uc790\ub8cc](https://skillful-houseboat-70c.notion.site/NLP-0cf2ffe5cc2542a4a6edd9f8e86fb4ef)\n  3. [NLP \uc8fc\uc694 \ub17c\ubb38\ub4e4](https://skillful-houseboat-70c.notion.site/e805b63e1f304c53aed49b4b177d6019?v=22d6287722c341a3a0936638e73534b8)\n\n### \ud83d\udcac SNS\n\n  * [\uc778\uc2a4\ud0c0\uadf8\ub7a8](https://www.instagram.com/attentionx.ai/)\n  * [GitHub](https://github.com/AttentionX)\n  * [\uc720\ud22c\ube0c](https://www.youtube.com/@attentionx)\n\n### \u260e\ufe0f Contact\n\n  * attentionx.ai@gmail.com", "start_char_idx": 3799, "end_char_idx": 4565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbe0f3d3-22e0-42c8-bc46-0c8da58a9254": {"__data__": {"id_": "bbe0f3d3-22e0-42c8-bc46-0c8da58a9254", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb4ea85d-82fb-46f1-bb96-06835299f948", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "9b15c36c5f2d8a01fb73d4959c1579ae892e3f4037c2d31ac2f7805937e26f06", "class_name": "RelatedNodeInfo"}}, "text": "InstructBLIP: Towards General-purpose\nVision-Language Models with Instruction Tuning\nWenliang Dai\u20201,2\u2217Junnan Li\u2020,B,1Dongxu Li1Anthony Meng Huat Tiong1,3\nJunqi Zhao3Weisheng Wang3Boyang Li3Pascale Fung2Steven HoiB,1\n1Salesforce Research2Hong Kong University of Science and Technology\n3Nanyang Technological University, Singapore\nhttps://github.com/salesforce/LAVIS/tree/main/projects/instructblip\n\u2020Equal contributionBCorresponding authors: {junnan.li,shoi@salesforce.com}\nAbstract\nLarge-scale pre-training and instruction tuning have been successful at creating\ngeneral-purpose language models with broad competence. However, building\ngeneral-purpose vision-language models is challenging due to the rich input dis-\ntributions and task diversity resulting from the additional visual input. Although\nvision-language pretraining has been widely studied, vision-language instruction\ntuning remains under-explored. In this paper, we conduct a systematic and compre-\nhensive study on vision-language instruction tuning based on the pretrained BLIP-2\nmodels. We gather 26 publicly available datasets, covering a wide variety of tasks\nand capabilities, and transform them into instruction tuning format. Additionally,\nwe introduce an instruction-aware Query Transformer, which extracts informative\nfeatures tailored to the given instruction. Trained on 13 held-in datasets, Instruct-\nBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets,\nsubstantially outperforming BLIP-2 and larger Flamingo models. Our models\nalso lead to state-of-the-art performance when finetuned on individual downstream\ntasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Further-\nmore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent\nmultimodal models. All InstructBLIP models are open-sourced.\n1 Introduction\nA longstanding aspiration of Artificial Intelligence (AI) research is to build a single model that\ncan solve arbitrary tasks specified by the user. In natural language processing (NLP), instruction\ntuning [ 46,7] proves to be a promising approach toward that goal. By finetuning a large language\nmodel (LLM) on a wide range of tasks described by natural language instructions, instruction tuning\nenables the model to follow arbitrary instructions. Recently, instruction-tuned LLMs have also been\nleveraged for vision-language tasks. For example, BLIP-2 [ 20] effectively adapts frozen instruction-\ntuned LLMs to understand visual inputs and exhibits preliminary capabilities to follow instructions in\nimage-to-text generation.\nCompared to NLP tasks, vision-language tasks are more diverse in nature due to the additional visual\ninputs from various domains. This poses a greater challenge to a unified model that is supposed to\ngeneralize to diverse vision-language tasks, many unseen during training. Most previous work can\nbe grouped into two approaches. The first approach, multitask learning [ 6,27], formulates various\nvision-language tasks into the same input-output format. However, we empirically find multitask\nlearning without instructions (Table 4) does not generalize well to unseen datasets and tasks. The\n\u2217Work done during internship at Salesforce.\nPreprint. Under review.arXiv:2305.06500v2  [cs.CV]  15 Jun 2023", "start_char_idx": 0, "end_char_idx": 3282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40e718a6-fcc6-4777-9461-16f7e65759a3": {"__data__": {"id_": "40e718a6-fcc6-4777-9461-16f7e65759a3", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0df6c12-be5a-438a-bea1-e3142635b74e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "08d98ff2eb8dbd094c6afb4c232eb5520e0db79066795e417bed5bdb2ecf3c47", "class_name": "RelatedNodeInfo"}}, "text": "What could have happened based on the\ncurrent scene?\nBased on the current scene in the image, it is\npossible that a hurricane or severe weather event\ncaused significant damage to the buildings and\ninfrastructure in the area. The person walking\nthrough the debris-covered area suggests that they\nare inspecting the damage caused by the disaster .\nThe presence of palm trees indicates that the\nlocation may be in a tropical or sub-tropical region,\nwhich can be more susceptible to hurricanes and\nother natural disasters.\nIntroduce me this painting\nin detail.\nThe painting depicts a young girl with a pearl earring,\nalso known as \"The Girl with the Pearl Earring\" or\n\"Girl with a Pearl Earring\". She is wearing a blue\ndress and has a pearl earring on her left ear . Her hair\nis pulled back into a bun, and she is looking directly\nat the viewer . This painting is a famous work of art\nby Dutch painter Johannes Vermeer , created in 1665.\nIt is considered one of his masterpieces and is widely\nrecognized for its intricate details and realistic\nportrayal of the young girl\u2019 s facial expression.\nCan yon tell me about this image in detail?\nThe image depicts a collection of various\nvegetables including carrots, cucumbers,\ntomatoes, and nuts arranged on a stone surface.\nThere are several jars filled with dif ferent types of\ningredients, such as peanuts, cashews, sunflower\nseeds, and pumpkin seeds. These ingredients are\nlikely to be part of a healthy meal or snack.\nCan you give me an example of such meal?\nAn example of a healthy meal or snack that\nincorporates some of the ingredients in the image\ncould be a salad made with mixed greens, carrots,\ncucumbers, tomatoes, and nuts. The salad can be\ndressed with a homemade vinaigrette made with\nolive oil, lemon juice, salt, and pepper .\nAdditionally , the salad can be topped with\npumpkin seeds or sunflower seeds for added\ncrunch and nutritional value.\nSounds good! Show me steps of making this salad.\nSure! Here are the steps to make this healthy salad:\n1. Wash and chop the mixed greens, carrots,\ncucumbers, and tomatoes into bite-sized pieces.\n2. In a small bowl, whisk together olive oil, lemon\njuice, salt, and pepper to create a homemade\nvinaigrette dressing.\n3. Place the chopped vegetables in a lar ge bowl and\ntoss them with the vinaigrette dressing.\n4. Sprinkle pumpkin seeds or sunflower seeds on top\nof the salad for added crunch and nutritional value.\n5. Serve the salad immediately or store it in the\nrefrigerator until ready to enjoy .\nCan you describe this image\nin detail?\nThe image depicts a person standing in front of an\nopen doorway , looking out into space. They are\nwearing a jacket and appears to be contemplating the\nvastness of the universe beyond the doorway . In the\nbackground, there are multiple stars visible, creating\na stunning celestial scene. The person's gaze is\ndirected towards the center of the image, where a\nbright star can be seen shining brightly . There are\nalso smaller stars scattered throughout the scene,\nadding to the overall cosmic atmosphere.\nWhat if they jump over the doorway?\nIf the person were to jump over the doorway, they\nwould be leaving the confines of their current location\nand venturing into the vastness of space. This could be\na metaphorical representation of stepping out of one\u2019s\ncomfort zone or embarking on a new adventure.\nHowever, it is important to note that such an action\nwould be extremely dangerous and potentially life-\nthreatening due to the harsh conditions of space.\nWhat makes this dog special?\nThe dog in the image is wearing futuristic armor and\nhas a glowing green eye, giving it a unique and\nfuturistic appearance. This combination of elements\nmakes the dog stand out from other dogs and adds an\nelement of fantasy or sci-fi to the image.\nAdditionally , the dog's posture and facial expression\nconvey a sense of confidence and determination,\nfurther emphasizing its unique features.Figure 1: A few qualitative examples generated by our InstructBLIP Vicuna model. Here, a range of\nits diverse capabilities are demonstrated, including complex visual scene understanding and reasoning,\nknowledge-grounded image description, multi-turn visual conversation, etc.\n2", "start_char_idx": 0, "end_char_idx": 4192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "873c6f4d-f39b-4882-9e3b-59204bc8ee70": {"__data__": {"id_": "873c6f4d-f39b-4882-9e3b-59204bc8ee70", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3eed4cb8-08a2-4d0b-8393-36f2c3eceb8b", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "40111c118eba4bbf141016a973d83a814269db64efc9abf331bd0df7a715ed18", "class_name": "RelatedNodeInfo"}}, "text": "second approach [ 20,4] extends a pre-trained LLM with additional visual components, and trains the\nvisual components with image caption data. Nevertheless, such data are too limited to allow broad\ngeneralization to vision-language tasks that require more than visual descriptions.\nTo address the aforementioned challenges, this paper presents InstructBLIP, a vision-language\ninstruction tuning framework that enables general-purpose models to solve a wide range of visual-\nlanguage tasks through a unified natural language interface. InstructBLIP uses a diverse set of\ninstruction data to train a multimodal LLM. Specifically, we initialize training with a pre-trained\nBLIP-2 model consisting of an image encoder, an LLM, and a Query Transformer (Q-Former) to\nbridge the two. During instruction tuning, we finetune the Q-Former while keeping the image encoder\nand LLM frozen. Our paper makes the following key contributions:\n\u2022We perform a comprehensive and systematic study on vision-language instruction tuning. We\ntransform 26 datasets into the instruction tuning format and group them into 11 task categories.\nWe use 13 held-in datasets for instruction tuning and 13 held-out datasets for zero-shot evaluation.\nMoreover, we withhold four entire task categories for zero-shot evaluation at the task level.\nExhaustive quantitative and qualitative results demonstrate the effectiveness of InstructBLIP on\nvision-language zero-shot generalization.\n\u2022We propose instruction-aware visual feature extraction, a novel mechanism that enables flexible\nand informative feature extraction according to the given instructions. Specifically, the textual\ninstruction is given not only to the frozen LLM, but also to the Q-Former, so that it can extract\ninstruction-aware visual features from the frozen image encoder. Also, we propose a balanced\nsampling strategy to synchronize learning progress across datasets.\n\u2022We evaluate and open-source a suite of InstructBLIP models using two families of LLMs: 1)\nFlanT5 [ 7], an encoder-decoder LLM finetuned from T5 [ 34]; 2) Vicuna [ 2], a decoder-only\nLLM finetuned from LLaMA [ 41]. The InstructBLIP models achieve state-of-the-art zero-shot\nperformance on a wide range of vision-language tasks. Furthermore, InstructBLIP models lead\nto state-of-the-art finetuning performance when used as the model initialization on individual\ndownstream tasks.\n2 Vision-Language Instruction Tuning\nInstructBLIP aims to address the unique challenges in vision-language instruction tuning and provide\na systematic study on the models\u2019 improved generalization ability to unseen data and tasks. In this\nsection, we first introduce the construction of instruction-tuning data, followed by the training and\nevaluation protocols. Next, we delineate two techniques to improve instruction-tuning performance\nfrom the model and data perspectives, respectively. Lastly, we present the implementation details.\n2.1 Tasks and Datasets\nTo ensure the diversity of instruction tuning data while considering their accessibility, we gather\ncomprehensive set of publicly available vision-language datasets, and transform them into the\ninstruction tuning format. As shown in Figure 2, the final collection covers 11 task categories and 26\ndatasets, including image captioning [ 23,3,51], image captioning with reading comprehension [ 38],\nvisual reasoning [ 16,24,29], image question answering [ 11,12], knowledge-grounded image\nquestion answering [ 30,36,28], image question answering with reading comprehension [ 31,39],\nimage question generation (adapted from the QA datasets), video question answering [ 47,49], visual\nconversational question answering [ 8], image classification [ 18], and LLaV A-Instruct-150K [ 25]. We\ninclude detailed descriptions and statistics of each dataset in Appendix C.\nFor every task, we meticulously craft 10 to 15 distinct instruction templates in natural language.\nThese templates serve as the foundation for constructing instruction tuning data, which articulates\nthe task and the objective. For public datasets inherently favoring short responses, we use terms\nsuch as short andbriefly into some of their corresponding instruction templates to reduce the risk of\nthe model overfitting to always generating short outputs. For the LLaV A-Instruct-150K dataset, we\ndo not incorporate additional instruction templates since it is naturally structured in the instruction\nformat. The full list of instruction templates can be found in Appendix D.\n3", "start_char_idx": 0, "end_char_idx": 4479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60608057-4f7e-4585-917e-0e6005242122": {"__data__": {"id_": "60608057-4f7e-4585-917e-0e6005242122", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bdb898f8-0891-45bc-ae70-c691039e0a17", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "2cd8e58106c0eacb051aba09179fb825aa06d4063c7da70e145ab79d1e368a66", "class_name": "RelatedNodeInfo"}}, "text": "Image Captioning \nCOCO Caption\nWeb CapFilt\nNoCaps\nFlickr30K\nImage Question Answering\nVQA v2Visual Reasoning\nGQA\nVisual Spatial\nReasoning\nIconQA\nImage Question Generation\nVQA v2\nOKVQA\nA-OKVQAVisual Conversational QA\nVisual DialogVideo Question Answering\nMSVD QA\nMSR VTT  QA\niVQA\nLLaV A-Instruct-150K\nVisual Conversation\nComplex Reasoning\nImage ClassificationVizWiz Detailed Image\nDescription HatefulMemesImage Captioning \nReading Comprehension\nTextCapsImage Question Answering\nReading Comprehension\nOCR-VQA\nTextVQAKnowledge Grounded\nImage Question Answering\nOKVQA\nA-OKVQA\nScienceQAFigure 2: Tasks and their corresponding datasets used for vision-language instruction tuning. The\nheld-in datasets are indicated by yellow and the held-out datasets by white.\n2.2 Training and Evaluation Protocols\nTo ensure sufficient data and tasks for training and zero-shot evaluation, we divide the 26 datasets into\n13 held-in datasets and 13 held-out datasets, indicated by yellow and white respectively in Figure 2.\nWe employ the training sets of the held-in datasets for instruction tuning and their validation or test\nsets for held-in evaluation.\nFor held-out evaluation, our aim is to understand how instruction tuning improves the model\u2019s\nzero-shot performance on unseen data. We define two types of held-out data: 1) datasets not exposed\nto the model during training, but whose tasks are present in the held-in cluster; 2) datasets and their\nassociated tasks that remain entirely unseen during training. Addressing the first type of held-out\nevaluation is nontrivial due to the data distribution shift between held-in and held-out datasets. For\nthe second type, we hold out several tasks completely, including visual reasoning, video question\nanswering, visual conversational QA, and image classification.\nTo avoid data contamination, datasets are selected carefully so that no evaluation data appear in\nthe held-in training cluster across different datasets. During instruction tuning, we mix all the\nheld-in training sets and sample instruction templates uniformly for each dataset. The models\nare trained with the standard language modeling loss to directly generate the response given the\ninstruction. Furthermore, for datasets that involve scene texts, we add OCR tokens in the instruction\nas supplementary information.\n2.3 Instruction-aware Visual Feature Extraction\nExisting zero-shot image-to-text generation methods, including BLIP-2, take an instruction-agnostic\napproach when extracting visual features. That results in a set of static visual representations being\nfed into the LLM, regardless of the task. In contrast, an instruction-aware vision model can adapt to\nthe task instruction and produce visual representations most conducive to the task at hand. This is\nclearly advantageous if we expect the task instructions to vary considerably for the same input image.\nWe show the architecture of InstructBLIP in Figure 3. Similarly to BLIP-2 [ 20], InstructBLIP utilizes\na Query Transformer, or Q-Former, to extract visual features from a frozen image encoder. The\ninput to the Q-Former contains a set of Klearnable query embeddings, which interact with the\nimage encoder\u2019s output through cross attention. The output of the Q-Former consists of Kencoded\nvisual vectors, one per query embedding, which then go through a linear projection and are fed to\nthe frozen LLM. As in BLIP-2, the Q-Former is pretrained in two stages using image-caption data\n4", "start_char_idx": 0, "end_char_idx": 3452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fac5522a-de1d-49c1-9a80-fafc453d8291": {"__data__": {"id_": "fac5522a-de1d-49c1-9a80-fafc453d8291", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f61369ac-29e5-4502-a61c-592a8c2fcf67", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "d1e1f44b6ad74b1df5608c47601dbc5d4939d308b0be3fb163203da47ebef1f7", "class_name": "RelatedNodeInfo"}}, "text": "Input ImageQueries\u2026Q-Former\u2026\nResponseFullyConnected\u2026LLM\nleft oneInstruction\nImageEncoderInstructionQueriesFeed ForwardSelf AttentionFeed ForwardCross AttentionQ-Former\nChoose the correct option to the following question: which picture shows the pizza inside the over?Options: (a) left one (b) right one. Answer:ImageEmbeddings\u2026\nInstruction\u2026\u2026\u2026\n\u2026\nInstructionImage EmbeddingsFigure 3: Model architecture of InstructBLIP. The Q-Former extracts instruction-aware visual features\nfrom the output embeddings of the frozen image encoder, and feeds the visual features as soft prompt\ninput to the frozen LLM. We instruction-tune the model with the language modeling loss to generate\nthe response.\nbefore instruction tuning. The first stage pretrains the Q-Former with the frozen image encoder for\nvision-language representation learning. The second stage adapts the output of Q-Former as soft\nvisual prompts for text generation with a frozen LLM . After pretraining, we finetune the Q-Former\nwith instruction tuning, where the LLM receives as input the visual encodings from the Q-Former\nand the task instruction.\nExtending BLIP-2, InstructBLIP proposes an instruction-aware Q-former module, which takes in\nthe instruction text tokens as additional input. The instruction interacts with the query embeddings\nthrough self-attention layers of the Q-Former, and encourages the extraction of task-relevant image\nfeatures. As a result, the LLM receives visual information conducive to instruction following. We\ndemonstrate empirically (Table 2) that instruction-aware visual feature extraction provides substantial\nperformance improvements for both held-in and held-out evaluations.\n2.4 Balancing Training Datasets\nDue to the large number of training datasets and the significant differences in the size of each dataset,\nmixing them uniformly could cause the model to overfit smaller datasets and underfit larger datasets.\nTo mitigate the problem, we propose to sample datasets with probabilities proportional to the square\nroot of their sizes, or the numbers of training samples. Generally, given Ddatasets with sizes\n{S1, S2, . . . , S D}, the probability of a data sample being selected from a dataset dduring training\nispd=\u221aSdPD\ni=1\u221aSi. On top of this formula, we make manual adjustments to the weights of certain\ndatasets to improve optimization. This is warranted by inherent differences in the datasets and tasks\nthat require varying levels of training intensity despite similar sizes. To be specific, we lower the\nweight of A-OKVQA, which features multiple-choice questions, and increase the weight of OKVQA,\nwhich requires open-ended text generation. In Table 2, we show that the balanced dataset sampling\nstrategy improves overall performance for both held-in evaluation and held-out generalization.\n2.5 Inference Methods\nDuring inference time, we adopt two slightly different generation approaches for evaluation on\ndifferent datasets. For the majority of datasets, such as image captioning and open-ended VQA, the\ninstruction-tuned model is directly prompted to generate responses, which are subsequently compared\nto the ground truth to calculate metrics. On the other hand, for classification and multi-choice VQA\ntasks, we employ a vocabulary ranking method following previous works [ 46,22,21]. Specifically,\nwe still prompt the model to generate answers, but restrict its vocabulary to a list of candidates. Then,\nwe calculate log-likelihood for each candidate and select the one with the highest value as the final\nprediction. This ranking method is applied to ScienceQA, IconQA, A-OKVQA (multiple-choice),\nHatefulMemes, Visual Dialog, MSVD, and MSRVTT datasets. Furthermore, for binary classification,\n5", "start_char_idx": 0, "end_char_idx": 3707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba0e7b24-3a37-456a-a2ce-a71d59d3605f": {"__data__": {"id_": "ba0e7b24-3a37-456a-a2ce-a71d59d3605f", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36e4c4a0-8bab-4984-95e5-6730b539c32c", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ce552ec6ab0fa42041d6b9984952bfcf376b47a0f7993965ff45f6f0691d59ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a36f4b89-16f5-4e52-b40d-27435ba5e371", "node_type": "1", "metadata": {}, "hash": "bff235c20be22dae783e951f22afbeb7fa0719fb64f1162b1ecaee8cf019b373", "class_name": "RelatedNodeInfo"}}, "text": "NoCapsFlickr\n30KGQA VSR IconQA TextVQA Visdial HM VizWizSciQA\nimageMSVD\nQAMSRVTT\nQAiVQA\nFlamingo-3B [4] - 60.6 - - - 30.1 - 53.7 28.9 - 27.5 11.0 32.7\nFlamingo-9B [4] - 61.5 - - - 31.8 - 57.0 28.8 - 30.2 13.7 35.2\nFlamingo-80B [4] - 67.2 - - - 35.0 - 46.4 31.6 - 35.6 17.4 40.7\nBLIP-2 (FlanT5 XL) [20] 104.5 76.1 44.0 60.5 45.5 43.1 45.7 53.0 29.8 54.9 33.7 16.2 40.4\nBLIP-2 (FlanT5 XXL) [20] 98.4 73.7 44.6 68.2 45.4 44.1 46.9 52.0 29.4 64.5 34.4 17.4 45.8\nBLIP-2 (Vicuna-7B) 107.5 74.9 38.6 50.0 39.7 40.1 44.9 50.6 25.3 53.8 18.3 9.2 27.5\nBLIP-2 (Vicuna-13B) 103.9 71.6 41.0 50.9 40.6 42.5 45.1 53.7 19.6 61.0 20.3 10.3 23.5\nInstructBLIP (FlanT5 XL) 119.9 84.5 48.4 64.8 50.0 46.6 46.6 56.6 32.7 70.4 43.4 25.0 53.1\nInstructBLIP (FlanT5 XXL) 120.0 83.5 47.9 65.6 51.2 46.6 48.5 54.1 30.9 70.6 44.3 25.6 53.8\nInstructBLIP (Vicuna-7B) 123.1 82.4 49.2 54.3 43.1 50.1 45.2 59.6 34.5 60.5 41.8 22.1 52.2\nInstructBLIP (Vicuna-13B) 121.9 82.8 49.5 52.1 44.8 50.7 45.4 57.5 33.4 63.1 41.2 24.8 51.0\nTable 1: Zero-shot results on the held-out datasets. Here, Visdial, HM and SciQA denote the Visual\nDialog, HatefulMemes and ScienceQA datasets, respectively. For ScienceQA, we only evaluate on\nthe set with image context. Following previous works [ 4,49,32], we report the CIDEr score [ 42]\nfor NoCaps and Flickr30K, iVQA accuracy for iVQA, AUC score for HatefulMemes, and Mean\nReciprocal Rank (MRR) for Visual Dialog. For all other datasets, we report the top-1 accuracy (%).\nwe expand the positive and negative labels into a slightly broader set of verbalizers to exploit word\nfrequencies in natural text (e.g., yesandtruefor the positive class; noandfalse for the negative class).\nFor the video question-answering task, we utilize four uniformly-sampled frames per video. Each\nframe is processed by the image encoder and Q-Former individually, and the extracted visual features\nare concatenated before being fed into the LLM.\n2.6 Implementation Details\nArchitecture. Thanks to the flexibility enabled by the modular architectural design of BLIP-2, we\ncan quickly adapt the model to a wide range of LLMs.", "start_char_idx": 0, "end_char_idx": 2099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a36f4b89-16f5-4e52-b40d-27435ba5e371": {"__data__": {"id_": "a36f4b89-16f5-4e52-b40d-27435ba5e371", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36e4c4a0-8bab-4984-95e5-6730b539c32c", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ce552ec6ab0fa42041d6b9984952bfcf376b47a0f7993965ff45f6f0691d59ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba0e7b24-3a37-456a-a2ce-a71d59d3605f", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "2f44dba3fee6113e8cd46fbcbea601d4cd681248183c08b98f1d105583dab919", "class_name": "RelatedNodeInfo"}}, "text": "For all other datasets, we report the top-1 accuracy (%).\nwe expand the positive and negative labels into a slightly broader set of verbalizers to exploit word\nfrequencies in natural text (e.g., yesandtruefor the positive class; noandfalse for the negative class).\nFor the video question-answering task, we utilize four uniformly-sampled frames per video. Each\nframe is processed by the image encoder and Q-Former individually, and the extracted visual features\nare concatenated before being fed into the LLM.\n2.6 Implementation Details\nArchitecture. Thanks to the flexibility enabled by the modular architectural design of BLIP-2, we\ncan quickly adapt the model to a wide range of LLMs. In our experiments, we adopt four variations\nof BLIP-2 with the same image encoder (ViT-g/14 [ 10]) but different frozen LLMs, including FlanT5-\nXL (3B), FlanT5-XXL (11B), Vicuna-7B and Vicuna-13B. FlanT5 [ 7] is an instruction-tuned model\nbased on the encoder-decoder Transformer T5 [ 34]. Vicuna [ 2], on the other hand, is a recently\nreleased decoder-only Transformer instruction-tuned from LLaMA [ 41]. During vision-language\ninstruction tuning, we initialize the model from pre-trained BLIP-2 checkpoints, and only finetune\nthe parameters of Q-Former while keeping both the image encoder and the LLM frozen. Since the\noriginal BLIP-2 models do not include checkpoints for Vicuna, we perform pre-training with Vicuna\nusing the same procedure as BLIP-2.\nTraining and Hyper-parameters. We use the LA VIS library [ 19] for implementation, training, and\nevaluation. All models are instruction-tuned with a maximum of 60K steps and we validate model\u2019s\nperformance every 3K steps. For each model, a single optimal checkpoint is selected and used for\nevaluations on all datasets. We employ a batch size of 192, 128, and 64 for the 3B, 7B, and 11/13B\nmodels, respectively. The AdamW [26] optimizer is used, with \u03b21= 0.9,\u03b22= 0.999, and a weight\ndecay of 0.05. Additionally, we apply a linear warmup of the learning rate during the initial 1,000\nsteps, increasing from 10\u22128to10\u22125, followed by a cosine decay with a minimum learning rate of 0.\nAll models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.\n3 Experimental Results and Analysis\n3.1 Zero-shot Evaluation\nWe first evaluate InstructBLIP models on the set of 13 held-out datasets with instructions provided\nin Appendix E. We compare InstructBLIP with the previous SOTA models BLIP-2 and Flamingo.\nAs demonstrated in Table 1, we achieve new zero-shot SOTA results on all datasets. InstructBLIP\nconsistently surpasses its original backbone, BLIP-2, by a significant margin across all LLMs,\n6", "start_char_idx": 1412, "end_char_idx": 4074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "382d0fa1-99c2-49b3-8138-b29689495db6": {"__data__": {"id_": "382d0fa1-99c2-49b3-8138-b29689495db6", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d28bed6-b420-4177-a957-99164f1f3b8e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "2687d7da21a9c5a72dfdb4fbdd54993893c7267ced64a91a141b0c870a5b8f1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7d1f3ef-f306-45fd-9f5d-ec66756ccb78", "node_type": "1", "metadata": {}, "hash": "42fe004ffd1fff6d365779c244e0a1d048aaf906f5340b3bb016ca6b484c2fc1", "class_name": "RelatedNodeInfo"}}, "text": "Model Held-in Avg. GQAScienceQA\n(image-context)IconQA VizWiz iVQA\nInstructBLIP (FlanT5 XL) 94.1 48.4 70.4 50.0 32.7 53.1\nw/o Instruction-aware Visual Features 89.8 45.9 (\u21932.5) 63.4 (\u21937.0) 45.8 (\u21934.2) 25.1 (\u21937.6) 47.5 (\u21935.6)\nw/o Data Balancing 92.6 46.8 (\u21931.6) 66.0 (\u21934.4) 49.9 (\u21930.1) 31.8 (\u21930.9) 51.1 (\u21932.0)\nInstructBLIP (Vicuna-7B) 100.8 49.2 60.5 43.1 34.5 52.2\nw/o Instruction-aware Visual Features 98.9 48.2 (\u21931.0) 55.2 (\u21935.3) 41.2 (\u21931.9) 32.4 (\u21932.1) 36.8 (\u219315.4)\nw/o Data Balancing 98.8 47.8 (\u21931.4) 59.4 (\u21931.1) 43.5 (\u21910.4) 32.3 (\u21932.2) 50.3 (\u21931.9)\nTable 2: Results of ablation studies that remove the instruction-aware Visual Features (Section 2.3)\nand the balanced data sampling strategy (Section 2.4). For held-in evaluation, we compute the average\nscore of four datasets, including COCO Caption, OKVQA, A-OKVQA, and TextCaps. For held-out\nevaluation, we show five datasets from different tasks.\ndemonstrating the effectiveness of vision-language instruction tuning. For instance, InstructBLIP\nFlanT5 XLyields an average relative improvement of 15.0% when compared to BLIP-2 FlanT5 XL.\nFurthermore, instruction tuning boosts zero-shot generalization on unseen task categories such as\nvideo QA. InstructBLIP achieves up to 47.1% relative improvement on MSRVTT-QA over the\nprevious SOTA despite having never been trained with temporal video data. Finally, our smallest\nInstructBLIP FlanT5 XLwith 4B parameters outperforms Flamingo-80B on all six shared evaluation\ndatasets with an average relative improvement of 24.8%.\nFor the Visual Dialog dataset, we choose to report the Mean Reciprocal Rank (MRR) over the\nNormalized Discounted Cumulative Gain (NDCG) metric. This is because NDCG favors generic and\nuncertain answers while MRR prefers certain responses [ 32], making MRR better aligned with the\nzero-shot evaluation scenario.\n3.2 Ablation Study on Instruction Tuning Techniques\nTo investigate the impact of the instruction-aware visual feature extraction (Section 2.3) and the\nbalanced dataset sampling strategy (Section 2.4), we conduct ablation studies during the instruction\ntuning process. As illustrated in Table 2, the removal of instruction awareness in visual features\ndowngrades performance significantly across all datasets. The performance drop is more severe\nin datasets that involve spatial visual reasoning (e.g., ScienceQA) or temporal visual reasoning\n(e.g., iVQA), where the instruction input to the Q-Former can guide visual features to attend to\ninformative image regions. The removal of the data balancing strategy causes unstable and uneven\ntraining, as different datasets achieve peak performance at drastically different training steps. The\nlack of synchronized progress over multiple datasets harms the overall performance.\n3.3 Qualitative Evaluation\nBesides the systematic evaluation on public benchmarks, we further qualitatively examine Instruct-\nBLIP with more diverse images and instructions. As illustrated in Figure 1, InstructBLIP demonstrates\nits capacity for complex visual reasoning. For example, it can reasonably infer from the visual scene\nwhat could have happened and deduce the type of disaster from the location of the scene, which\nit extrapolates based on visual evidence like the palm trees. Moreover, InstructBLIP is capable\nof connecting visual input with embedded textual knowledge and generate informative responses,\nsuch as intruducing a famous painting. Furthermore, in descriptions of the overall atmosphere,\nInstructBLIP exhibits the ability to comprehend metaphorical implications of the visual imagery.", "start_char_idx": 0, "end_char_idx": 3564, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7d1f3ef-f306-45fd-9f5d-ec66756ccb78": {"__data__": {"id_": "d7d1f3ef-f306-45fd-9f5d-ec66756ccb78", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d28bed6-b420-4177-a957-99164f1f3b8e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "2687d7da21a9c5a72dfdb4fbdd54993893c7267ced64a91a141b0c870a5b8f1b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "382d0fa1-99c2-49b3-8138-b29689495db6", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "5797920a9699d48883a7fb90cdfb721da17e2129b6be135fbb784e9c605356c2", "class_name": "RelatedNodeInfo"}}, "text": "The removal of the data balancing strategy causes unstable and uneven\ntraining, as different datasets achieve peak performance at drastically different training steps. The\nlack of synchronized progress over multiple datasets harms the overall performance.\n3.3 Qualitative Evaluation\nBesides the systematic evaluation on public benchmarks, we further qualitatively examine Instruct-\nBLIP with more diverse images and instructions. As illustrated in Figure 1, InstructBLIP demonstrates\nits capacity for complex visual reasoning. For example, it can reasonably infer from the visual scene\nwhat could have happened and deduce the type of disaster from the location of the scene, which\nit extrapolates based on visual evidence like the palm trees. Moreover, InstructBLIP is capable\nof connecting visual input with embedded textual knowledge and generate informative responses,\nsuch as intruducing a famous painting. Furthermore, in descriptions of the overall atmosphere,\nInstructBLIP exhibits the ability to comprehend metaphorical implications of the visual imagery.\nFinally, we show that InstructBLIP can engage in multi-turn conversations, effectively considering\nthe dialog history when making new responses.\nIn Appendix B, we qualitatively compare InstructBLIP with concurrent multimodal models (GPT-\n4 [33], LLaV A [ 25], MiniGPT-4 [ 52]). Although all models are capable of generating long-form\nresponses, InstructBLIP\u2019s outputs generally contains more proper visual details and exhibits logically\ncoherent reasoning steps. Importantly, we argue that long-form responses are not always preferable.\nFor example, in Figure 2 of the Appendix, InstructBLIP directly addresses the user\u2019s intent by\nadaptively adjusting the response length, while LLaV A and MiniGPT-4 generate long and less\n7", "start_char_idx": 2501, "end_char_idx": 4290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b22e5180-934e-4c10-8007-fb78a7fed541": {"__data__": {"id_": "b22e5180-934e-4c10-8007-fb78a7fed541", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b83f35e1-2afe-456a-b61b-a3ae742985bb", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "f55914395cc83f60fb8c6e3268e252041b31e18d919e8c9bdaba0d916143675d", "class_name": "RelatedNodeInfo"}}, "text": "relevant sentences. These advantages of InstructBLIP are a result of the diverse instruction tuning\ndata and an effective architectural design.\n3.4 Instruction Tuning vs. Multitask Learning\n40 45 50 5546.1\n46.3\n45.5\n46.8\n52.9\n60 75 90 10567.8\n92.5\n89.0\n93.7\n93.8BLIP-2 Zer o-shot\nTrain w/ Plain Input\nEval w/ Instruction\nTrain w/ Dataset Name\nEval w/ Instruction\nTrain w/ Dataset Name\nEval w/ Dataset Name\nInstructBLIPMulti-task\nHeld-out Avg. Held-in Avg.\nFigure 4: Comparison of instruction tuning and multitask training based on BLIP-2 FlanT5 XL\nbackbone. For held-in evaluation, we compute the average score across all held-in datasets. For\nheld-out evaluation, we compute the average score across GQA, TextVQA, VSR, HatefulMemes,\nIconQA, ScienceQA, iVQA, VizWiz.\nA direct analogue to instruction tuning is multitask learning, a widely used method that involves\nthe simultaneous training of multiple datasets with the goal of improving the performance of each\nindividual dataset. To investigate whether the improvement in zero-shot generalization observed in\ninstruction tuning is mainly from the formatting of instructions or merely from multitasking, we\nconduct a comparative analysis between these two approaches under identical training settings.\nFollowing [ 46], we consider two multitask training approaches. In the first approach, the model is\ntrained using the vanilla input-output format of the training datasets without instructions. During\nevaluation, instructions are still provided to the model, indicating the specific task to be performed.\nHowever, an exception is made for image captioning, as the model achieves better scores when only\nreceiving the image as input. For the second approach, we take a step towards instruction tuning by\nprepending a [Task:Dataset] identifier to the text input during training. For example, we prepend\n[Visual question answering:VQAv2] for the VQAv2 dataset. During evaluation, we explore\nboth instructions and this identifier. Particularly, for the identifier of held-out datasets, we only use\nthe task name since the model never sees the dataset name.\nThe results are shown in Figure 4, including BLIP-2 zero-shot, multitask training, and instruction\ntuning. All of these models are based on the BLIP-2 FlanT5 XLbackbone and adhere to the identical\ntraining configurations delineated in Section 2. Overall, we can conclude two insights from the\nresults. Firstly, instruction tuning and multitask learning exhibit similar performance on the held-in\ndatasets. This suggests that the model can fit these two different input patterns comparably well, as\nlong as it has been trained with such data. On the other hand, instruction tuning yields a significant\nimprovement over multitask learning on unseen held-out datasets, whereas multitask learning still\nperforms on par with the original BLIP-2. This indicates that instruction tuning is the key to enhance\nthe model\u2019s zero-shot generalization ability.\n3.5 Finetuning InstructBLIP on Downstream Tasks\nWe further finetune the InstructBLIP models to investigate its performance on learning a specific\ndataset. Compared to most previous methods (e.g., Flamingo, BLIP-2) which increase the input\nimage resolution and finetune the visual encoder on downstream tasks, InstructBLIP maintains the\nsame image resolution (224 \u00d7224) during instruction tuning and keeps the visual encoder frozen\nduring finetuning. This significantly reduces the number of trainable parameters from 1.2B to 188M,\nthus greatly improves finetuning efficiency.\n8", "start_char_idx": 0, "end_char_idx": 3531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6751b41-c8fd-435b-96c8-eca5b6e95391": {"__data__": {"id_": "e6751b41-c8fd-435b-96c8-eca5b6e95391", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a81a760d-f444-4b72-a2a7-50810d061857", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "b0b497610c92f2b22d076623482c362a07fa2a2d45b2029c8fa35086aecd5597", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f204945-3cca-4ccb-8176-aa27b08aebcc", "node_type": "1", "metadata": {}, "hash": "4fdf0d99dbee16b56c9e40b30ceb6d49e4244ebe339f1b0ac9ddc3cbd3d0a186", "class_name": "RelatedNodeInfo"}}, "text": "ScienceQA\nIMGOCR-VQA OKVQAA-OKVQA\nDirect Answer Multi-choice\nVal Test Val Test\nPrevious SOTALLaV A [25]\n89.0GIT [43]\n70.3PaLM-E(562B) [9]\n66.1[15]\n56.3[37]\n61.6[15]\n73.2[37]\n73.6\nBLIP-2 (FlanT5 XXL) 89.5 72.7 54.7 57.6 53.7 80.2 76.2\nInstructBLIP (FlanT5 XXL) 90.7 73.3 55.5 57.1 54.8 81.0 76.7\nBLIP-2 (Vicuna-7B) 77.3 69.1 59.3 60.0 58.7 72.1 69.0\nInstructBLIP (Vicuna-7B) 79.5 72.8 62.1 64.0 62.1 75.7 73.4\nTable 3: Results of finetuning BLIP-2 and InstructBLIP on downstream datasets. Compared to\nBLIP-2, InstructBLIP provides a better weight initialization model and achieves SOTA performance\non three out of four datasets.\nThe results are shown in Table 3. Compared to BLIP-2, InstructBLIP leads to better finetuning\nperformance on all datasets, which validates InstructBLIP as a better weight initialization model\nfor task-specific finetuning. InstructBLIP sets new state-of-the-art finetuning performance on Sci-\nenceQA (IMG), OCR-VQA, A-OKVQA, and is outperformed on OKVQA by PaLM-E [ 9] with\n562B parameters.\nAdditionally, we observe that the FlanT5-based InstructBLIP is superior at multi-choice tasks,\nwhereas Vicuna-based InstructBLIP is generally better at open-ended generation tasks. This disparity\ncan be primarily attributed to the capabilities of their frozen LLMs, as they both employ the same\nimage encoder. Although FlanT5 and Vicuna are both instruction-tuned LLMs, their instruction data\nsignificantly differ. FlanT5 is mainly finetuned on NLP benchmarks containing many multi-choice\nQA and classification datasets, while Vicuna is finetuned on open-ended instruction-following data.\n4 Related Work\nInstruction tuning aims to teach language models to follow natural language instructions, which\nhas been shown to improve their generalization performance to unseen tasks. Some methods\ncollect instruction tuning data by converting existing NLP datasets into instruction format using\ntemplates [ 46,7,35,45]. Others use LLMs (e.g., GPT-3 [ 5]) to generate instruction data [ 2,13,44,40]\nwith improved diversity.\nInstruction-tuned LLMs have been adapted for vision-to-language generation tasks by injecting\nvisual information to the LLMs. BLIP-2 [ 20] uses frozen FlanT5 models, and trains a Q-Former\nto extract visual features as input to the LLMs. MiniGPT-4 [ 52] uses the same pretrained visual\nencoder and Q-Former from BLIP-2, but uses Vicuna [ 2] as the LLM and performs training using\nChatGPT [ 1]-generated image captions longer than the BLIP-2 training data. LLaV A [ 25] directly\nprojects the output of a visual encoder as input to a LLaMA/Vinuca LLM, and finetunes the LLM on\nvision-language conversational data generated by GPT-4 [ 33]. mPLUG-owl [ 50] performs low-rank\nadaption [ 14] to a LLaMA [ 41] model using both text instruction data and vision-language instruction\ndata from LLaV A. A separate work is MultiInstruct [ 48], which performs vision-language instruction\ntuning without a pretrained LLM, leading to less competitive performance.\nCompared to existing methods, InstructBLIP uses a much wider range of vision-language instruction\ndata, covering both template-based converted data and LLM-generated data. Architecture wise,\nInstructBLIP proposes an instruction-aware visual feature extraction mechanism. Furthermore, our\npaper provides a comprehensive analysis on various aspects of vision-language instruction tuning,\nvalidating its advantages on generalizing to unseen tasks.", "start_char_idx": 0, "end_char_idx": 3425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f204945-3cca-4ccb-8176-aa27b08aebcc": {"__data__": {"id_": "9f204945-3cca-4ccb-8176-aa27b08aebcc", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a81a760d-f444-4b72-a2a7-50810d061857", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "b0b497610c92f2b22d076623482c362a07fa2a2d45b2029c8fa35086aecd5597", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6751b41-c8fd-435b-96c8-eca5b6e95391", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "cd2eb004d230ecdb72b1343f2eef404c773e409defd74cf35a55fb7f55c2f6fa", "class_name": "RelatedNodeInfo"}}, "text": "mPLUG-owl [ 50] performs low-rank\nadaption [ 14] to a LLaMA [ 41] model using both text instruction data and vision-language instruction\ndata from LLaV A. A separate work is MultiInstruct [ 48], which performs vision-language instruction\ntuning without a pretrained LLM, leading to less competitive performance.\nCompared to existing methods, InstructBLIP uses a much wider range of vision-language instruction\ndata, covering both template-based converted data and LLM-generated data. Architecture wise,\nInstructBLIP proposes an instruction-aware visual feature extraction mechanism. Furthermore, our\npaper provides a comprehensive analysis on various aspects of vision-language instruction tuning,\nvalidating its advantages on generalizing to unseen tasks.\n5 Conclusion\nIn this paper, we present InstructBLIP, a simple yet novel instruction tuning framework towards gen-\neralized vision-language models. We perform a comprehensive study on vision-language instruction\ntuning and demonstrate the capability of InstructBLIP models to generalize to a wide range of unseen\ntasks with state-of-the-art performance. Qualitative examples also exhibit InstructBLIP\u2019s various\n9", "start_char_idx": 2669, "end_char_idx": 3837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec6d42bb-c7d0-4020-8512-c7aae69e3bfb": {"__data__": {"id_": "ec6d42bb-c7d0-4020-8512-c7aae69e3bfb", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac046439-d904-452e-9cbe-da9420a97f05", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "dcd430a2934fdbcc38ac667c620044fc1db802be57aa7bd1f310d1cd0b7bec6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3ef1694-aa9f-47e8-8a66-e6ce07d19e3f", "node_type": "1", "metadata": {}, "hash": "d671bacf90f2c93c8bfaa02b78d5f6ab12766ee11f793984db05b62c3b3b727a", "class_name": "RelatedNodeInfo"}}, "text": "capabilities on instruction following, such as complex visual reasoning, knowledge-grounded image\ndescription, and multi-turn conversations. Furthermore, we show that InstructBLIP can serve as an\nenhanced model initialization for downstream task finetuning, achieving state-of-the-art results. We\nhope that InstructBLIP can spur new research in general-purpose multimodal AI and its applications.\nReferences\n[1] Chatgpt. https://openai.com/blog/chatgpt , 2023. 9\n[2] Vicuna. https://github.com/lm-sys/FastChat , 2023. 3, 6, 9\n[3]Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In ICCV , pages\n8948\u20138957, 2019. 3, 16\n[4]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,\nTengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud,\nAndy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Miko\u0142 aj Bi \u00b4nkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Kar\u00e9n Simonyan. Flamingo: a visual language model for few-shot learning. In\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, NeurIPS , 2022. 3, 6\n[5]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020. 9\n[6]Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation.\narXiv preprint arXiv:2102.02779 , 2021. 1\n[7]Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y .\nZhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416 , 2022. 1, 3, 6, 9\n[8]Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F. Moura, Devi Parikh,\nand Dhruv Batra. Visual dialog. In CVPR , 2017.", "start_char_idx": 0, "end_char_idx": 2746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3ef1694-aa9f-47e8-8a66-e6ce07d19e3f": {"__data__": {"id_": "f3ef1694-aa9f-47e8-8a66-e6ce07d19e3f", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac046439-d904-452e-9cbe-da9420a97f05", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "dcd430a2934fdbcc38ac667c620044fc1db802be57aa7bd1f310d1cd0b7bec6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec6d42bb-c7d0-4020-8512-c7aae69e3bfb", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ed90c7205b7b33b7e12f9566b5d0f87405b35f8a07420b18c4c0d405f367ca1b", "class_name": "RelatedNodeInfo"}}, "text": "Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416 , 2022. 1, 3, 6, 9\n[8]Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F. Moura, Devi Parikh,\nand Dhruv Batra. Visual dialog. In CVPR , 2017. 3, 16\n[9]Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet,\nDaniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff,\nAndy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model, 2023. 9\n[10] Yuxin Fang, Wen Wang, Binhui Xie, Quan-Sen Sun, Ledell Yu Wu, Xinggang Wang, Tiejun Huang,\nXinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale.\nArXiv , abs/2211.07636, 2022. 6\n[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In CVPR , July 2017. 3, 16\n[12] Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P.\nBigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR , 2018. 3, 16\n[13] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language\nmodels with (almost) no human labor. ArXiv , abs/2212.09689, 2022. 9\n[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR , 2022. 9\n[15] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A. Smith, and Jiebo Luo. Promptcap: Prompt-\nguided task-aware image captioning, 2023. 9\n[16] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In CVPR , 2019. 3, 16\n[17] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2015. 16\n[18] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia,\nand Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In\nNeurIPS , 2020. 3, 16\n10", "start_char_idx": 2347, "end_char_idx": 4889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af63cc9f-3ed5-49d0-bb5c-01881023f736": {"__data__": {"id_": "af63cc9f-3ed5-49d0-bb5c-01881023f736", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f920d13a-e0d1-450b-8b67-1a59681e69a3", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ec67d0c60d819dc3b42d8b1caa7865f5956e6a76322a3fa4164023d73a17abe2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bef711b3-dbce-45fb-a965-c317a14f6402", "node_type": "1", "metadata": {}, "hash": "d8034080e6ec949aa142ed92960601a7d50a7cb4000b3ceafcfcdf808eb3de05", "class_name": "RelatedNodeInfo"}}, "text": "[19] Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven C. H. Hoi. Lavis: A library\nfor language-vision intelligence, 2022. 6\n[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. In ICML , 2023. 1, 3, 4, 6, 9, 16\n[21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In ICML , 2022. 5, 16\n[22] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. Align before fuse: Vision and language representation learning with momentum distillation. In\nNeurIPS , 2021. 5\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV , 2014. 3, 16\n[24] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the\nAssociation for Computational Linguistics , 2023. 3\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. 2023. 3, 7, 9, 16\n[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR , 2019. 6\n[27] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision\nand language representation learning. In CVPR , 2020. 1\n[28] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. In NeurIPS , 2022. 3, 16\n[29] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and\nSong-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language\nreasoning. In NeurIPS Track on Datasets and Benchmarks , 2021. 3, 16\n[30] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. In CVPR , 2019. 3, 16\n[31] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question\nanswering by reading text in images. In ICDAR , 2019. 3, 16\n[32] Vishvak Murahari, Dhruv Batra, Devi Parikh, and Abhishek Das. Large-scale pretraining for visual dialog:\nA simple state-of-the-art baseline. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael\nFrahm, editors, ECCV , 2020. 6, 7\n[33] OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023. 7, 9\n[34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research , 2020.", "start_char_idx": 0, "end_char_idx": 2893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bef711b3-dbce-45fb-a965-c317a14f6402": {"__data__": {"id_": "bef711b3-dbce-45fb-a965-c317a14f6402", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f920d13a-e0d1-450b-8b67-1a59681e69a3", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ec67d0c60d819dc3b42d8b1caa7865f5956e6a76322a3fa4164023d73a17abe2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af63cc9f-3ed5-49d0-bb5c-01881023f736", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ff9e93eb7d7e42447e03f05be1f19427c9937028cd5131e4292587b5d9b612bf", "class_name": "RelatedNodeInfo"}}, "text": "3, 16\n[32] Vishvak Murahari, Dhruv Batra, Devi Parikh, and Abhishek Das. Large-scale pretraining for visual dialog:\nA simple state-of-the-art baseline. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael\nFrahm, editors, ECCV , 2020. 6, 7\n[33] OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023. 7, 9\n[34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research , 2020. 3, 6\n[35] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, An-\ntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea\nSantilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas\nWolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In ICLR ,\n2022. 9\n[36] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-\nokvqa: A benchmark for visual question answering using world knowledge. In Shai Avidan, Gabriel\nBrostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, and Tal Hassner, editors, ECCV , 2022. 3, 16\n[37] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics\nfor knowledge-based visual question answering. Computer Vision and Pattern Recognition (CVPR) , 2023.\n9\n[38] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image\ncaptioningwith reading comprehension. 2020. 3, 16\n[39] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach.\nTowards vqa models that can read. In CVPR , pages 8317\u20138326, 2019. 3, 16\n[40] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.\ncom/tatsu-lab/stanford_alpaca , 2023. 9\n11", "start_char_idx": 2311, "end_char_idx": 4687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2be6baf-755e-4ef2-88ef-92fbee2bfa55": {"__data__": {"id_": "f2be6baf-755e-4ef2-88ef-92fbee2bfa55", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a860a07-9471-4ede-aeb8-20d98007a3e1", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "a8a635ece2f7d1b6ab472481a4365545a44970aa8e0326c72946e7c534c9a8ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2992647c-a0da-491b-a2e1-8b3aa6064f60", "node_type": "1", "metadata": {}, "hash": "a1b5a00dd3369da72021dc74ad830536bb849b074ba9f4a0b4f8ed9b60635904", "class_name": "RelatedNodeInfo"}}, "text": "[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023. 3, 6, 9\n[42] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages\n4566\u20134575, 2015. 6\n[43] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and\nLijuan Wang. Git: A generative image-to-text transformer for vision and language, 2022. 9\n[44] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. ArXiv ,\nabs/2212.10560, 2022. 9\n[45] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis\nKaramanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi,\nKuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney,\nPhani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur\nSampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-\nNaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In EMNLP , 2022.\n9\n[46] Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V . Le. Finetuned language models are zero-shot learners. In ICLR , 2022. 1, 5, 8, 9\n[47] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video\nquestion answering via gradually refined attention over appearance and motion. In Proceedings of the 25th\nACM International Conference on Multimedia , page 1645\u20131653, 2017. 3, 16\n[48] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via\ninstruction tuning. ArXiv , abs/2212.10773, 2022. 9\n[49] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer\nquestions from millions of narrated videos. In ICCV , pages 1686\u20131697, 2021. 3, 6, 16\n[50] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng\nShi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Chao Zhang, and\nFeiyan Huang. mplug-owl: Modularization empowers large language models with multimodality. 2023. 9\n[51] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.", "start_char_idx": 0, "end_char_idx": 2869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2992647c-a0da-491b-a2e1-8b3aa6064f60": {"__data__": {"id_": "2992647c-a0da-491b-a2e1-8b3aa6064f60", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a860a07-9471-4ede-aeb8-20d98007a3e1", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "a8a635ece2f7d1b6ab472481a4365545a44970aa8e0326c72946e7c534c9a8ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2be6baf-755e-4ef2-88ef-92fbee2bfa55", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "f0c204ebcdec82251cfa7a5ade33bf2c90d449138537742d274cf160610f913a", "class_name": "RelatedNodeInfo"}}, "text": "9\n[49] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer\nquestions from millions of narrated videos. In ICCV , pages 1686\u20131697, 2021. 3, 6, 16\n[50] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng\nShi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Chao Zhang, and\nFeiyan Huang. mplug-owl: Modularization empowers large language models with multimodality. 2023. 9\n[51] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Transactions of the\nAssociation for Computational Linguistics , 2, 2014. 3, 16\n[52] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models, 2023. 7, 9\n12", "start_char_idx": 2307, "end_char_idx": 3242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4dc81e3-77aa-4752-b9b5-48561c1a5231": {"__data__": {"id_": "c4dc81e3-77aa-4752-b9b5-48561c1a5231", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e97950f3-7089-482d-9562-744934d0b313", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "e9362ff2e0adec55fe60a3bbfef33e45e8fe81c179de342ae3d8508c71f5d9b3", "class_name": "RelatedNodeInfo"}}, "text": "A Broader Impact\nInstructBLIP uses off-the-shelf frozen LLMs. Therefore it inherits some of the shortcomings from the\noriginal LLMs, such as hallucinating ungrounded text or generating outputs with bias. We mitigate\nsuch shortcomings by improving the model\u2019s grounding on the vision and instruction input, and\nperforming vision-language instruction tuning on a diverse set of high-quality datasets. Nevertheless,\nwe do not recommend applying InstructBLIP models to any downstream applications without a prior\nassessment on safety and fairness specific to that application.\nB More Case Studies\n[Input Image]\nWhat is unusual about this image?\n[Model Response]\nInstructBLIP\nThe unusual aspect of this image is that a man is ironing clothes on the\nback of a yellow taxi cab while it is driving down a bus city street. He is\nstanding on top of the car's trunk, holding an iron and using it to press his\nclothing items. This situation is not only unconventional but also\npotentially hazardous, as the man could be at risk of falling of f the vehicle\nor causing an accident due to his actions.\nLLaV AThe unusual aspect of this image is a man ironing clothes on the back of a\nminivan or van. This is not a typical place to perform this activity , as one\nwould usually iron clothes in a more stationary and safe location, such as\na home, using a regular ironing board. The scene depicted in the image is\npeculiar as it involves a makeshift ironing setup on a vehicle, which can\nbe both unsafe and unconventional. Additionally , it is not clear how the\nman is able to maintain balance and stability while ironing clothes in such\nan unstable environment.\nMiniGPT -4 The image shows a man standing on the back of a yellow taxi, with a\nyellow shirt and black pants, and a blue backpack on his back. The taxi is\ndriving on a city street with othr cars and taxis in the background.GPT-4 The unusual thing about this image is that a man is ironing clothes on an\nironing board attached to the roof of a moving taxi.\n[Input Instruction]\nFigure 5: The response from InstructBLIP is more comprehensive than GPT-4, more visually-\ngrounded than LLaV A, and more logical than MiniGPT-4. The responses of GPT-4 and LLaV A\nare obtained from their respective papers, while the official demo is used for MiniGPT-4. Source:\nhttps://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg\n13", "start_char_idx": 0, "end_char_idx": 2377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4c12ecc-b531-4069-8937-f43d4a15cd58": {"__data__": {"id_": "f4c12ecc-b531-4069-8937-f43d4a15cd58", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51476963-e70d-43e5-a6a4-6848457832a9", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "e5e9ef70879c68e9c60090c30fb7c56f10b62e93f1890e17b2d6fcfb3829f40f", "class_name": "RelatedNodeInfo"}}, "text": "[Input Image]\nDo you know who drew this painting?\n[Model Response]\nInstructBLIP\nLLaV A\nMiniGPT -4The painting depicts a woman, commonly believed to be Mona Lisa, the\nfamous artwork by Leonardo da Vinci. It is a portrait painting that\nshowcases the woman's enigmatic smile and has become one of the most\nfamous and iconic art pieces in the world. The original work is displayed\nin the Louvre Museum in Paris, and it is known for its intricate details,\nuse of oil paint, and the artist's innovative techniques that contributed to\nits enduring appeal and mystery .\nThe painting is the Mona Lisa, also known as the Mona Lisa, by Leonardo\nda Vinci. It is one of the most famous paintings in the world and is known\nfor its enigmatic smile and the mysterious identity of the sitter . The\npainting is housed in the Louvre Museum in Paris, France.Leonardo da Vinci.[Input Instruction]Figure 6: In this example, we illustrate that long-form responses are not always preferable. Instruct-\nBLIP can directly address the user\u2019s intent by adaptively adjusting the response length, while other\nmodels tend to generate lengthy paragraphs with less-relevant sentences. The response from LLaV A\nis taken from the paper, and for MiniGPT-4, we utilize its official demo.\n14", "start_char_idx": 0, "end_char_idx": 1253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebe2528f-d00a-43ec-b384-ffe25dfcbe20": {"__data__": {"id_": "ebe2528f-d00a-43ec-b384-ffe25dfcbe20", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad60ec12-2434-488c-89a7-6f9a21c74503", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "af4c36cd6ff945590eb4125b096b5c748471e8c135cd50808e62e2e8c330d480", "class_name": "RelatedNodeInfo"}}, "text": "[Input Image]\n[Input Instruction] Introduce me this painting in detail.\n[Model Response]\nInstructBLIP\nLLaV A\nMiniGPT -4The painting features a woman with a headscarf, possibly of Dutch origin,\ndressed in blue and white clothing. She is wearing a lar ge blue hat and has\na red mouth, which adds a vibrant touch to the portrait. The woman is\nlooking directly at the viewer , creating a sense of connection. The\nbackground of the painting is predominantly blue, further emphasizing the\ncolor theme of the piece. The overall composition and attire of the woman\nsuggest that the painting might be inspired by the style of Rembrandt.\nThe painting depicts a young woman with a pearl earring, wearing a blue\ndress and a white apron. She is shown with a serious expression, looking\ndirectly at the viewer . The background is a dark, neutral tone, with no\nvisible objects or elements. The overall mood of the painting is somber\nand contemplative.\nThe painting is well-executed, with a high level of detail and realism in\nthe depiction of the woman\u2019 s features and clothing. The use of shading\nand lighting creates a sense of depth and dimension, making the woman\nappear three-dimensional. The artist has used a limited color palette, with\nthe woman\u2019 s dress and apron providing the only sources of color in the\npainting.\nThe painting is well-composed, with the woman\u2019 s head and shoulders\npositioned in the center of the canvas, creating a sense of balance and\nsymmetry . The use of negative space around the woman\u2019 s figure adds to\nthe overall somber mood of the painting.\nOverall, this is a well-executed and thought-provoking painting that\neffectively conveys a sense of contemplation and introspection.The painting depicts a young girl with a pearl earring, also known as \"The\nGirl with the Pearl Earring\" or \"Girl with a Pearl Earring\". She is wearing\na blue dress and has a pearl earring on her left ear . Her hair is pulled back\ninto a bun, and she is looking directly at the viewer . This painting is a\nfamous work of art by Dutch painter Johannes Vermeer , created in 1665.\nIt is considered one of his masterpieces and is widely recognized for its\nintricate details and realistic portrayal of the young girl\u2019 s facial\nexpression.\nFigure 7: InstructBLIP shows the ability to connect textual knowledge with visual input, while the\nother models depict the picture plainly. MiniGPT-4 exhibits poorer results, which may be due to its\ntraining with only long captions. Responses of LLaV A and MiniGPT-4 are generated by their official\ndemos.\n15", "start_char_idx": 0, "end_char_idx": 2537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75e847c2-5502-48b1-8e0d-5d41e41c7716": {"__data__": {"id_": "75e847c2-5502-48b1-8e0d-5d41e41c7716", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "809a37e7-8f8e-4a6d-8b66-6276a0665c73", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c9993f1cb22eb09a1ff8502cfebc15c3391ee0009d143e7e4a91e875d8b04d74", "class_name": "RelatedNodeInfo"}}, "text": "C Instruction Tuning Datasets\nDataset Name Held-out Dataset Description\nCOCO Caption [23] \u2717 We use the large-scale COCO dataset for the image captioning task. Specifically, Karpathy\nsplit [ 17] is used, which divides the data into 82K/5K/5K images for the train/val/test sets.\nWeb CapFilt \u2717 14M image-text pairs collected from the web with additional BLIP-generated synthetic\ncaptions, used in BLIP [21] and BLIP-2 [20].\nNoCaps [3] \u2713(val) NoCaps contains 15,100 images with 166,100 human-written captions for novel object\nimage captioning.\nFlickr30K [51] \u2713(test) The Flickr30k dataset consists of 31K images collected from Flickr, each image has five\nground truth captions. We use the test split as the held-out which contains 1K images.\nTextCaps [38] \u2717 TextCaps is an image captioning dataset that requires the model to comprehend and reason\nthe text in images. Its train/val/test sets contain 21K/3K/3K images, respectively.\nVQAv2 [11] \u2717 VQAv2 is dataset for open-ended image question answering. It is split into 82K/40K/81K\nfor train/val/test.\nVizWiz [12] \u2713(test-dev) A dataset contains visual questions asked by people who are blind. 8K images are used for\nthe held-out evaluation.\nGQA [16] \u2713(test-dev) GQA contains image questions for scene understanding and reasoning. We use the balanced\ntest-dev set as held-out.\nVisual Spatial Reasoning \u2713(test) VSR is a collection of image-text pairs, in which the text describes the spatial relation of\ntwo objects in the image. Models are required to classify true/false for the description. We\nuse the zero-shot data split given in its official github repository.\nIconQA [29] \u2713(test) IconQA measures the abstract diagram understanding and comprehensive cognitive rea-\nsoning abilities of models. We use the test set of its multi-text-choice task for held-out\nevaluation.\nOKVQA [30] \u2717 OKVQA contains visual questions that require outside knowledge to answer. It has been\nsplit into 9K/5K for train and test.\nA-OKVQA [36] \u2717 A-OKVQA is a successor of OKVQA with more challenging and diverse questions. It has\n17K/1K/6K questions for train/val/test.\nScienceQA [28] \u2713(test) ScienceQA covers diverse science topics with corresponding lectures and explanations. In\nout settings, we only use the part with image context (IMG).\nVisual Dialog [8] \u2713(val) Visual dialog is a conversational question answering dataset. We use the val split as the\nheld-out, which contains 2,064 images and each has 10 rounds.\nOCR-VQA [31] \u2717 OCR-VQA contains visual questions that require models to read text in the image. It has\n800K/100K/100K for train/val/test, respectively.\nTextVQA [39] \u2713(val) TextVQA requires models to comprehend visual text to answer questions.\nHatefulMemes [18] \u2713(val) A binary classification dataset to justify whether a meme contains hateful content.\nLLaV A-Instruct-150K [25] \u2717 An instruction tuning dataset which has three parts: detailed caption (23K), reasoning\n(77K), conversation (58K).\nMSVD-QA [47] \u2713(test) We use the test set (13K video QA pairs) of MSVD-QA for held-out testing.\nMSRVTT-QA [47] \u2713(test) MSRVTT-QA has more complex scenes than MSVD, with 72K video QA pairs as the test\nset.\niVQA [49] \u2713(test) iVQA is a video QA dataset with mitigated language biases. It has 6K/2K/2K samples for\ntrain/val/test.\nTable 4: Description of datasets in our held-in instruction tuning and held-out zero-shot evaluations.\n16", "start_char_idx": 0, "end_char_idx": 3366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e2cfce8-713f-4e23-be73-13a2e249139f": {"__data__": {"id_": "8e2cfce8-713f-4e23-be73-13a2e249139f", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10919fbc-8e30-4272-9692-40b17cbd4248", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "7e671a438eee50543282f931afe8fff032f75b04f1932fdce21c3b1dc4bf4b39", "class_name": "RelatedNodeInfo"}}, "text": "D Instruction Templates\nTask Instruction Template\nImage\nCaptioning<Image>A short image caption:\n<Image>A short image description:\n<Image>A photo of\n<Image>An image that shows\n<Image>Write a short description for the image.\n<Image>Write a description for the photo.\n<Image>Provide a description of what is presented in the photo.\n<Image>Briefly describe the content of the image.\n<Image>Can you briefly explain what you see in the image?\n<Image>Could you use a few words to describe what you perceive in the photo?\n<Image>Please provide a short depiction of the picture.\n<Image>Using language, provide a short account of the image.\n<Image>Use a few words to illustrate what is happening in the picture.\nVQA<Image>{Question}\n<Image>Question: {Question}\n<Image>{Question} A short answer to the question is\n<Image>Q: {Question} A:\n<Image>Question: {Question} Short answer:\n<Image>Given the image, answer the following question with no more than three words. {Question}\n<Image>Based on the image, respond to this question with a short answer: {Question}. Answer:\n<Image>Use the provided image to answer the question: {Question} Provide your answer as short as possible:\n<Image>What is the answer to the following question? \"{Question}\"\n<Image>The question \"{Question}\" can be answered using the image. A short answer is\nVQG<Image>Given the image, generate a question whose answer is: {Answer}. Question:\n<Image>Based on the image, provide a question with the answer: {Answer}. Question:\n<Image>Given the visual representation, create a question for which the answer is \"{Answer}\".\n<Image>From the image provided, craft a question that leads to the reply: {Answer}. Question:\n<Image>Considering the picture, come up with a question where the answer is: {Answer}.\n<Image>Taking the image into account, generate an question that has the answer: {Answer}. Question:\nTable 5: Instruction templates used for transforming held-in datasets into instruction tuning data. For\ndatasets with OCR tokens, we simply add \u201cOCR tokens:\u201d after the image query embeddings.\nE Instructions for Zero-shot Inference\nWe provide instructions used for zero-shot inference. Note that for instructions with options, we\nseparate options with the alphabetical order, e.g. (a) blue (b) yellow (c) pink (d) black.\nGQA, VizWiz, iVQA, MSVD, MSRVTT <Image> Question: {} Short answer:\nNoCaps, Flickr30k <Image> A short image description:\nTextVQA <Image> OCR tokens: {}. Question: {} Short answer:\nIconQA <Image> Question: {} Options: {}. Short answer:\nScienceQA <Image> Context: {} Question: {} Options: {}. Answer:\nHatefulMemes <Image> This is an image with: \"{}\" written on it. Is it hateful? Answer:\nVSR <Image> Based on the image, is this statement true or false? \"{}\" Answer:\nVisual Dialog <Image> Dialog history: {}\\n Question: {} Short answer:\n17", "start_char_idx": 0, "end_char_idx": 2812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f208b55-e402-4c7f-98ff-453712472e7a": {"__data__": {"id_": "7f208b55-e402-4c7f-98ff-453712472e7a", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d9374f3-8713-4746-88b7-6de3b8ccb2a0", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "1aab5d05cea7bf8dca235f6cdedd2cb939e18ada1680ac5c68aaf1369c5f7a3d", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n1S a l e s f o r c e \uc758  I n s t r u c t B L I P\n\uc5bc\ub9c8  \uc804\uc5d0  Salesf or ce \uc5d0\uc11c  BLIP2 \ub97c  \uacf5\uac1c\ud588\uc5c8\uc2b5\ub2c8\ub2e4 . \ub192\uc740  \uc131\ub2a5\uc744  \ubcf4\uc774\ub294  Vision-Language  \nLLM \uc774\uc5c8\uace0 , \ud14c\uc2a4\ud2b8\ub97c  \ud574\n\ubd24\uc744  \ub54c  \uaf64  \ud765\ubbf8\ub85c\uc6b4  \uacb0\uacfc  ( \uc608\ub97c  \ub4e4\uc5b4  \uc774\ubbf8\uc9c0  \ub0b4\uc758  \uc0ac\ub78c\uc774\ub098  \uac1d\uccb4  \uac2f\uc218\ub97c  \uc798  \uce74\uc6b4\ud305\ud558\ub294  \ub4f1 ) \ub97c  \n\ubcf4\uc5ec\uc8fc\uae30\ub3c4  \ud574\uc11c  \ubc1c\uc804  \uc18d\ub3c4\uac00  \ub180\ub78d\ub2e4\uace0  \uc0dd\uac01\uc744\n\ud588\uc5c8\uc2b5\ub2c8\ub2e4 . \uc774\ubc88\uc5d0  Salesf or ce \uc5d0\uc11c  \uadf8  \ud6c4\uc18d\uc73c\ub85c  Instr cutBLIP \uc744  \uacf5\uac1c\ud558\uc600\uc2b5\ub2c8\ub2e4 . \\ - \ucf54\ub4dc :\nht tps://github.com/salesf or ce/LA VIS/tr ee/main/pr oject s/instructblip  \uae30\uc874\uc758  BLIP2\n\uae30\ubc18\uc73c\ub85c  \uc5ec\ub7ec  \uac00\uc9c0  public dat aset \uc744  \uac00\uc838\ub2e4\uac00  instruct tuning \uc774  \uac00\ub2a5\ud55c  \ud615\ud0dc\ub85c  \ub9cc\ub4e4\uc5b4\uc11c  \nfine-tuning \uc744\n\uc9c4\ud589\ud55c  \uac83\uc774\uace0 , \ub9e4\uc6b0  \ud070  Vision-Language \ub370\uc774\ud130\uc14b\uc778  Flamingo \ub97c  \ud3ec\ud568\ud55c  \uc5ec\ub7ec  \ub370\uc774\ud130\uc5d0  \ub300\n\ud574\uc11c  z er o-shot \uc5d0\uc11c\nSO T A\ub97c  \ub2ec\uc131\ud558\uc600\ub2e4\uace0  \ud569\ub2c8\ub2e4 . \ub17c\ubb38\uc744  \uc0b4\ud3b4\ubcf4\uba74  \uad6c\uc870  \uc790\uccb4\ub294  \uae30\uc874  BLIP2 \uc640  \ub2e4\ub97c  \uac83\uc774  \ubcc4\ub85c  \uc5c6\n\uc2b5\ub2c8\ub2e4 . BLIP \uc758  \ud575\uc2ec\uc778  Q-f or mer\n\ubd80\ubd84\uc5d0  \uae30\uc874\uc5d0\ub294  Quer y \ue09d T e x t \ud615\ud0dc\ub85c  \ub123\ub358  \uac83\uc5d0\uc11c  t e x t \ub97c  instruction \uc73c\ub85c  \ud55c  \uc815\ub3c4\ub9cc  \ucc28\uc774\ub77c\n\uace0  \ubcfc  \uc218  \uc788\uc2b5\ub2c8\ub2e4 .\n(Instruct -tuning \uc774\ubbc0\ub85c  \ub2f9\uc5f0\ud55c  \uac83\uc785\ub2c8\ub2e4\ub9cc ... \u314e\u314e ) \uc774\ubc88  \uc5f0\uad6c\ub294  \ubaa8\ub378  \uc544\ud0a4\ud14d\uccd0  \ub4f1\uc774  \uac1c\uc120\ub418\uc5c8\n\ub2e4\uae30  \ubcf4\ub2e4\ub294 , \uc5ec\ub7ec  \ub370\uc774\ud130\uc14b\uc5d0\n\ub300\ud574\uc11c  fine-tuning \uc744  \uc9c4\ud589\ud558\uace0  \uc774  \ubaa8\ub378\uc744  \uacf5\uac1c\ud55c  \uac83\uc5d0  \uc758\uc758\uac00  \uc788\ub2e4\uace0  \ubcfc  \uc218  \uc788\uc2b5\ub2c8\ub2e4 . \uc5ec\uae30\uc11c\n\ub294  Vicuna \uc640  Flan T5 \uae30\ubc18\uc73c\ub85c  \ub41c\n\ubaa8\ub378\uc744  \uacf5\uac1c\ud558\uc600\ub294\ub370  MiniGPT 4 \ue081\nht tps://github.com/Vision-C AIR/MiniGPT \ue0884 \ue082 \uc640  \ube44\uc2b7\ud558\ub124\uc694 . \uc5b4\ucc0c\ub418\uc5c8\uac74\n\ucd5c\uadfc\uc5d0  Vision/Language \ub97c  \uac19\uc774  \ub2e4\ub8e8\ub294  LLM \ub4e4\uc774  \ub9ce\uc544\uc9c0\uace0  \uc788\uace0  \uc810\uc810  \ub354  \uc5ec\ub7ec  modalit y \ub85c  \n\ud655\uc7a5\ub418\uc5b4  \uac00\ub294  \uac83  \uac19\uc2b5\ub2c8\ub2e4 .\n\uadf8\uc640\ub294  \ubcc4\uac1c\ub85c  Met a \uc758  OPT , LLaMA \ub098  Google \uc758  Flan T5 \ub4f1\uc740  \uc624\ud508\uc18c\uc2a4  \ucabd\uc5d0  \uc815\ub9d0  \ud070  \uae30\uc5ec\n\ub97c  \ud558\uace0  \uc788\ub294  \uac83  \uac19\uc2b5\ub2c8\ub2e4 .", "start_char_idx": 0, "end_char_idx": 1260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"acf592e6-9295-43cf-bbe5-9c8a014b88e5": {"node_ids": ["7eb0cc9d-5f11-47ec-97bb-688ba2146a76"], "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}}, "6554357d-4926-41bb-ba09-2a30ad8ab2b7": {"node_ids": ["161d833c-af25-4eb3-910e-a34688815a0e", "530b4333-1c1d-459c-9fd6-f31605152fc7"], "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}}, "7350dff5-ca52-492b-9708-b7b75bfd56db": {"node_ids": ["7f509bea-bf88-4dc4-bf7f-cdecc69b51aa", "2b7b9716-5929-49cf-8265-478b14c4d4ad", "39964f9b-a72a-4ec6-bbba-a745f799bdeb", "0b281146-76ed-4052-b392-f4866df7a9f9", "78d4d6dd-f2e9-4781-95dc-04b3a03f848f", "86d0853d-c2a3-46e1-ac1c-e829a085c814", "ee303cf7-04f5-42ae-9181-8b4f9d225214"], "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}}, "ac547bcb-2d29-43e1-ab9b-b2dfc38e2757": {"node_ids": ["7d628fe1-636e-414e-8473-cdbb5222e352", "773858b9-6987-4c5f-85ff-09360d93b19f", "11c66f9d-b201-410b-9bc5-403b02e1fd39", "3438890e-9827-4b9a-b240-daaacafe4f4e"], "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}}, "ef2eb8b6-8e82-4ab5-9926-3541b5aa7d8e": {"node_ids": ["f44313e1-0f4d-4ddf-a22a-00ee400bceec", "8dfaba2a-f2da-41f1-a929-7abfcef5a112", "c39324ab-fda1-4623-8062-418b3f6249b1", "ff5492a3-4e5d-41a8-bc14-8bd7828667fb"], "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}}, "fb4ea85d-82fb-46f1-bb96-06835299f948": {"node_ids": ["bbe0f3d3-22e0-42c8-bc46-0c8da58a9254"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "a0df6c12-be5a-438a-bea1-e3142635b74e": {"node_ids": ["40e718a6-fcc6-4777-9461-16f7e65759a3"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "3eed4cb8-08a2-4d0b-8393-36f2c3eceb8b": {"node_ids": ["873c6f4d-f39b-4882-9e3b-59204bc8ee70"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "bdb898f8-0891-45bc-ae70-c691039e0a17": {"node_ids": ["60608057-4f7e-4585-917e-0e6005242122"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "f61369ac-29e5-4502-a61c-592a8c2fcf67": {"node_ids": ["fac5522a-de1d-49c1-9a80-fafc453d8291"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "36e4c4a0-8bab-4984-95e5-6730b539c32c": {"node_ids": ["ba0e7b24-3a37-456a-a2ce-a71d59d3605f", "a36f4b89-16f5-4e52-b40d-27435ba5e371"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "9d28bed6-b420-4177-a957-99164f1f3b8e": {"node_ids": ["382d0fa1-99c2-49b3-8138-b29689495db6", "d7d1f3ef-f306-45fd-9f5d-ec66756ccb78"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "b83f35e1-2afe-456a-b61b-a3ae742985bb": {"node_ids": ["b22e5180-934e-4c10-8007-fb78a7fed541"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "a81a760d-f444-4b72-a2a7-50810d061857": {"node_ids": ["e6751b41-c8fd-435b-96c8-eca5b6e95391", "9f204945-3cca-4ccb-8176-aa27b08aebcc"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "ac046439-d904-452e-9cbe-da9420a97f05": {"node_ids": ["ec6d42bb-c7d0-4020-8512-c7aae69e3bfb", "f3ef1694-aa9f-47e8-8a66-e6ce07d19e3f"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "f920d13a-e0d1-450b-8b67-1a59681e69a3": {"node_ids": ["af63cc9f-3ed5-49d0-bb5c-01881023f736", "bef711b3-dbce-45fb-a965-c317a14f6402"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "7a860a07-9471-4ede-aeb8-20d98007a3e1": {"node_ids": ["f2be6baf-755e-4ef2-88ef-92fbee2bfa55", "2992647c-a0da-491b-a2e1-8b3aa6064f60"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "e97950f3-7089-482d-9562-744934d0b313": {"node_ids": ["c4dc81e3-77aa-4752-b9b5-48561c1a5231"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "51476963-e70d-43e5-a6a4-6848457832a9": {"node_ids": ["f4c12ecc-b531-4069-8937-f43d4a15cd58"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "ad60ec12-2434-488c-89a7-6f9a21c74503": {"node_ids": ["ebe2528f-d00a-43ec-b384-ffe25dfcbe20"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "809a37e7-8f8e-4a6d-8b66-6276a0665c73": {"node_ids": ["75e847c2-5502-48b1-8e0d-5d41e41c7716"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "10919fbc-8e30-4272-9692-40b17cbd4248": {"node_ids": ["8e2cfce8-713f-4e23-be73-13a2e249139f"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "4d9374f3-8713-4746-88b7-6de3b8ccb2a0": {"node_ids": ["7f208b55-e402-4c7f-98ff-453712472e7a"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}}}