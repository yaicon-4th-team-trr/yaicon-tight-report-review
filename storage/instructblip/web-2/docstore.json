{"docstore/metadata": {"c9ef9b7c-6dd0-4935-a552-9b3674a6f388": {"doc_hash": "0aabd50dc4c5ab13f3a917f150b8976adb00326a2a0654b2e879c8710f3cc572"}, "9e1bb884-42d3-4a53-b6fd-0c763c91b8ff": {"doc_hash": "0dddf5bf69a835a468364a576ba8cf9cfe899cf6170088b0f0a5fd5cae2a8af7", "ref_doc_id": "c9ef9b7c-6dd0-4935-a552-9b3674a6f388"}, "7344652d-6789-4c15-96cc-ea8f7128b966": {"doc_hash": "253ce19eb04bcc376e45f7bedf37452dfa4fe826a9be2ed58e6f3f656efa976a"}, "e504fc55-ba53-482d-8ee8-08eb0be8e6f2": {"doc_hash": "db8c5cc228bad7229d7a3112e3fd3f3d79bf0dedcd650b40ef74102a1a46b269", "ref_doc_id": "7344652d-6789-4c15-96cc-ea8f7128b966"}, "951cb1fd-c1c1-48e4-9e60-c1db7e07330d": {"doc_hash": "31d4fe5e986c94b6827098c11f56fad1bfb4ba2a672704a0f9407e14a4c143d9", "ref_doc_id": "7344652d-6789-4c15-96cc-ea8f7128b966"}, "a0be635d-53c4-4af1-aa1f-159804d24d42": {"doc_hash": "b1cc6360c81ae0ce5ba01dbc2b10cabb9c8f19642c960619187b4c5e6ca3e372"}, "c0b1c862-2f78-4caa-9563-fbf240184e19": {"doc_hash": "1aab6d60cfd80a3ef70080fa1cedf5402af6b77865c0bb3b61a101f28dc41c6c", "ref_doc_id": "a0be635d-53c4-4af1-aa1f-159804d24d42"}, "76e09f0d-354d-49eb-8183-61a2659bc563": {"doc_hash": "53a9f33a0315888a21212387226e476c731162233222299977007705018a4b22", "ref_doc_id": "a0be635d-53c4-4af1-aa1f-159804d24d42"}, "e0d9fd1d-07e8-459b-9e75-8fb3f251a5e8": {"doc_hash": "c3c046ed2a5e776f157a17f0abaaf9aa65e93ad0b36c4dde4e9d3cf3a2c2b579", "ref_doc_id": "a0be635d-53c4-4af1-aa1f-159804d24d42"}, "904ecbfa-69e9-4ab9-859c-d6fe5dbb6772": {"doc_hash": "995d066284df103c6a73e044eedada3b97ed31eb97d2935d1b015af4e5a054c2", "ref_doc_id": "a0be635d-53c4-4af1-aa1f-159804d24d42"}, "df597bea-1054-4d7c-80a9-a79b03167424": {"doc_hash": "46910d84705f387d0f3f07c0ed6a783be7f47ea827917bb362bc0b34b77b7069", "ref_doc_id": "a0be635d-53c4-4af1-aa1f-159804d24d42"}, "0e1af00a-a606-4caf-abe2-3c87351354ab": {"doc_hash": "c213d5036bfa8c305dc58c9e1dc17ef4739fb243627f754e5d18aa411c68e5f5", "ref_doc_id": "a0be635d-53c4-4af1-aa1f-159804d24d42"}, "6de95ee5-2f6b-4475-a7bc-5a0e045bbfac": {"doc_hash": "3961f979139e3c16d90b5202ee0b1ab1263d9a80675b4359a9dc6397676432eb", "ref_doc_id": "a0be635d-53c4-4af1-aa1f-159804d24d42"}}, "docstore/data": {"9e1bb884-42d3-4a53-b6fd-0c763c91b8ff": {"__data__": {"id_": "9e1bb884-42d3-4a53-b6fd-0c763c91b8ff", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-0.txt", "file_name": "output-0.txt", "file_type": "text/plain", "file_size": 61, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9ef9b7c-6dd0-4935-a552-9b3674a6f388", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-0.txt", "file_name": "output-0.txt", "file_type": "text/plain", "file_size": 61, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "0aabd50dc4c5ab13f3a917f150b8976adb00326a2a0654b2e879c8710f3cc572", "class_name": "RelatedNodeInfo"}}, "text": "# 403 Forbidden\n\nRequest forbidden by administrative rules.", "start_char_idx": 0, "end_char_idx": 59, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e504fc55-ba53-482d-8ee8-08eb0be8e6f2": {"__data__": {"id_": "e504fc55-ba53-482d-8ee8-08eb0be8e6f2", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7344652d-6789-4c15-96cc-ea8f7128b966", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "253ce19eb04bcc376e45f7bedf37452dfa4fe826a9be2ed58e6f3f656efa976a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "951cb1fd-c1c1-48e4-9e60-c1db7e07330d", "node_type": "1", "metadata": {}, "hash": "a97f315051673ece8b4a009e54ba0dcfc460b4fd8bc0946a48a068b12ec2890e", "class_name": "RelatedNodeInfo"}}, "text": "[ \ud30c\uc774\ud1a0\uce58 \ud55c\uad6d \uc0ac\uc6a9\uc790 \ubaa8\uc784 ](/)\n\n#  [Salesforce, InstructBLIP \ubaa8\ub378\uc758 \ub17c\ubb38 / \ucf54\ub4dc / \uac00\uc911\uce58 \uacf5\uac1c](/t/salesforce-\ninstructblip/1571)\n\n[ \uc77d\uc744\uac70\ub9ac&\uc815\ubcf4\uacf5\uc720 ](/c/news/14)\n\n[lavis](https://discuss.pytorch.kr/tag/lavis),\n[blip2](https://discuss.pytorch.kr/tag/blip2),\n[minigpt-4](https://discuss.pytorch.kr/tag/minigpt-4),\n[instructblip](https://discuss.pytorch.kr/tag/instructblip),\n[salesforce](https://discuss.pytorch.kr/tag/salesforce),\n[gpt-4](https://discuss.pytorch.kr/tag/gpt-4),\n[multimodal](https://discuss.pytorch.kr/tag/multimodal), [vision-\nlanguage](https://discuss.pytorch.kr/tag/vision-language)\n\n[9bow](https://discuss.pytorch.kr/u/9bow) (\ubc15\uc815\ud658)  5\uc6d4 17, 2023, 9:59\uc624\uc804  1\n\nSalesforce\uc5d0\uc11c BLIP-2 \ubaa8\ub378\uc5d0 \uc774\uc5b4 InstructBLIP \ubaa8\ub378\uc758 \ub17c\ubb38\uacfc \uad6c\ud604, \uadf8\ub9ac\uace0 \ud559\uc2b5\ub41c \uac00\uc911\uce58\ub97c \uacf5\uac1c\ud588\uc2b5\ub2c8\ub2e4.\n\n##  InstructBLIP: Towards General-purpose Vision-Language Models with\nInstruction Tuning\n\n[![image](https://discuss.pytorch.kr/uploads/default/original/2X/7/720b1a5fdeccd3fb8e52ace10a474304e32beee9.jpeg)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\n\uc544\ub798\uc640 \uac19\uc774 Vicuna, T5\ub97c \uc0ac\uc6a9\ud55c 2 \uc885\ub958\uc758 \ubaa8\ub378\uc774 \uc788\uc73c\uba70,\n\n    \n    \n    # ==================================================\n    # Architectures                  Types\n    # ==================================================\n    # blip2_vicuna_instruct          vicuna7b, vicuna13b\n    # blip2_t5_instruct              flant5xl, flant5xxl\n    \n\n~~[Salesforce\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 LAVIS \ud328\ud0a4\uc9c0](https://github.com/salesforce/LAVIS)(`pip\ninstall salesforce-lavis`)\ub97c \uc124\uce58\ud558\uc5ec \ubc14\ub85c \uc0ac\uc6a9\ud574 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.~~  \n\uc544\uc9c1 PyPI\uc758 \ud328\ud0a4\uc9c0\uc5d0\ub294 InstructBLIP \ubaa8\ub378\ub4e4\uc774 \ubc18\uc601\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc544\uc11c, GitHub\uc5d0\uc11c \uc9c1\uc811 \uc124\uce58\ud558\uc154\uc57c \ud569\ub2c8\ub2e4.\n\n    \n    \n    git clone https://github.com/salesforce/LAVIS.git\n    cd LAVIS\n    pip install -e .\n    \n    \n    \n    from lavis.models import load_model_and_preprocess\n    # loads InstructBLIP model\n    model, vis_processors, _ = load_model_and_preprocess(name=\"blip2_vicuna_instruct\", model_type=\"vicuna7b\", is_eval=True, device=device)\n    # prepare the image\n    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n    \n\n  \n\ucf54\ub4dc\uc640 \uc0ac\uc6a9\ubc95\uc774 \uad81\uae08\ud558\uc2e0 \ubd84\ub4e4\uaed8\uc11c\ub294 GitHub \uc800\uc7a5\uc18c\uc5d0\uc11c,\n\n![](https://github.githubassets.com/favicons/favicon.svg)\n[github.com](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\n### [LAVIS/projects/instructblip at main \u00b7\nsalesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\n[main/projects/instructblip](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)\n\nLAVIS - A One-stop Library for Language-Vision Intelligence\n\n  \n\ub17c\ubb38\uc774 \uad81\uae08\ud558\uc2e0 \ubd84\ub4e4\uaed8\uc11c\ub294 arXiv\uc5d0\uc11c \ubc14\ub85c \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \n~~(\uc800\ub294 \uc624\ub298\ub3c4 \uc77d\uae30 \ud050\uc5d0 \ub123\uae30\ub9cc \ud558\uace0\n\uc788\uc2b5\ub2c8\ub2e4;;;![:sweat_smile:](https://discuss.pytorch.kr/images/emoji/apple/sweat_smile.png?v=12)\n)~~\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/c/c683569a48ce1952ba841c851ae3b1f282d4b00f.png)\n[arXiv.org](https://arxiv.org/abs/2305.06500)\n\n!", "start_char_idx": 0, "end_char_idx": 2748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "951cb1fd-c1c1-48e4-9e60-c1db7e07330d": {"__data__": {"id_": "951cb1fd-c1c1-48e4-9e60-c1db7e07330d", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7344652d-6789-4c15-96cc-ea8f7128b966", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "253ce19eb04bcc376e45f7bedf37452dfa4fe826a9be2ed58e6f3f656efa976a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e504fc55-ba53-482d-8ee8-08eb0be8e6f2", "node_type": "1", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "db8c5cc228bad7229d7a3112e3fd3f3d79bf0dedcd650b40ef74102a1a46b269", "class_name": "RelatedNodeInfo"}}, "text": "~~(\uc800\ub294 \uc624\ub298\ub3c4 \uc77d\uae30 \ud050\uc5d0 \ub123\uae30\ub9cc \ud558\uace0\n\uc788\uc2b5\ub2c8\ub2e4;;;![:sweat_smile:](https://discuss.pytorch.kr/images/emoji/apple/sweat_smile.png?v=12)\n)~~\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/c/c683569a48ce1952ba841c851ae3b1f282d4b00f.png)\n[arXiv.org](https://arxiv.org/abs/2305.06500)\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/8/86a2c8ac52804c90ede238d99cb944037c6f82f6.png)\n\n### [InstructBLIP: Towards General-purpose Vision-Language Models with\nInstruction...](https://arxiv.org/abs/2305.06500)\n\nGeneral-purpose language models that can solve various language-domain tasks\nhave emerged driven by the pre-training and instruction-tuning pipeline.\nHowever, building general-purpose vision-language models is challenging due to\nthe increased task...\n\n  \n\ubb38\uc11c\uc758 \ub9c8\uc9c0\ub9c9\uc5d0 \uc81c\uc2dc\ub41c \uc774\ubbf8\uc9c0\uc758 \uc774\uc0c1\ud55c \ubd80\ubd84\uc744 \uc124\uba85\ud558\ub77c\ub294 \uc9c0\ubb38\uc5d0 \ub300\ud55c `InstructBLIP`\uacfc `GPT-4`, `miniGPT-4` \ub4f1\uc758\n\ub2f5\ubcc0\uc744 \ube44\uad50\ud574 \ub450\uc5c8\ub294\ub370 \uc778\uc0c1\uc801\uc774\ub124\uc694.\n![:monkey:](https://discuss.pytorch.kr/images/emoji/apple/monkey.png?v=12)\n\n![image](https://discuss.pytorch.kr/uploads/default/original/2X/e/ea9a956b0e6f6adb323ec01f0234cdac379b4d68.png)\n\n[[\ubb34\ub8cc/\uc628\ub77c\uc778/\uc601\uc5b4] ChatGPT\uc640 CLIP\uc744 \uc0ac\uc6a9\ud55c Semantic Visual Search\n\ub9cc\ub4e4\uae30](https://discuss.pytorch.kr/t/chatgpt-clip-semantic-visual-search/1731)\n\n[9bow](https://discuss.pytorch.kr/u/9bow) (\ubc15\uc815\ud658)  5\uc6d4 17, 2023, 10:04\uc624\uc804  2\n\n\ub17c\ubb38\uc5d0 \uc694\ub7f0 \uc0ac\ub840\ub3c4 \ucca8\ubd80\ub418\uc5b4 \uc788\ub124\uc694\n![:smiley:](https://discuss.pytorch.kr/images/emoji/apple/smiley.png?v=12)\n\n![image](https://discuss.pytorch.kr/uploads/default/original/2X/5/5f4b2578db249d9572ae390976096877be804d16.png)\n\n  * [\ud648 ](/)\n  * [\uce74\ud14c\uace0\ub9ac ](/categories)\n  * [FAQ/\uac00\uc774\ub4dc\ub77c\uc778 ](/guidelines)\n  * [\uc774\uc6a9\uc57d\uad00 ](/tos)\n  * [\uac1c\uc778\uc815\ubcf4 \ucde8\uae09\ubc29\uce68 ](/privacy)\n\n[Discourse](https://www.discourse.org)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. JavaScript\uac00 \ud65c\uc131\ud654\ub41c \uc0c1\ud0dc\uc5d0\uc11c \uac00\uc7a5 \uc798\n\ubcf4\uc785\ub2c8\ub2e4.", "start_char_idx": 2473, "end_char_idx": 4128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0b1c862-2f78-4caa-9563-fbf240184e19": {"__data__": {"id_": "c0b1c862-2f78-4caa-9563-fbf240184e19", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0be635d-53c4-4af1-aa1f-159804d24d42", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "b1cc6360c81ae0ce5ba01dbc2b10cabb9c8f19642c960619187b4c5e6ca3e372", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76e09f0d-354d-49eb-8183-61a2659bc563", "node_type": "1", "metadata": {}, "hash": "cb74bb9c8b1ff581cc1a7f9f4ea29431a022cec4c0e48143aa32ea34b37d9073", "class_name": "RelatedNodeInfo"}}, "text": "#  [ IBOK ](https://bo-10000.tistory.com/ \"IBOK\")\n\n  * [\ud648](/)\n\n\ud83c\udf0c Deep Learning/\ub17c\ubb38 \ub9ac\ubdf0 [KOR]\n\n## \uce74\uce74\uc624\ube0c\ub808\uc778 Multimodal LLM Honeybee \ub17c\ubb38 \ub9ac\ubdf0\n\n\ubcf5\ub9cc 2024\\. 3. 2. 16:09\n\n\uce74\uce74\uc624\ube0c\ub808\uc778\uc5d0\uc11c \uc791\ub144 \ub9d0 Multimodal LLM\uc778 Honeybee\ub97c \ubc1c\ud45c\ud588\ub2e4. \uc544\uc27d\uac8c\ub3c4 \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc740 \uc544\ub2c8\uace0 \uc601\uc5b4 \ubaa8\ub378\uc774\uace0, 5\uac1c\uc758\n\ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c SoTA\ub97c \ub2ec\uc131\ud588\ub2e4\uace0 \ud574\uc11c \ub274\uc2a4\uac00 \uc5c4\uccad \ub9ce\uc774 \ub098\uc654\ub2e4.\n\n\n\n\ub17c\ubb38: <https://arxiv.org/pdf/2312.06742.pdf>\n\n\uae43\ud5d9: <https://github.com/kakaobrain/honeybee>\n\n[  \n\nGitHub - kakaobrain/honeybee: The official implementation of project\n\"Honeybee\"\n\nThe official implementation of project \"Honeybee\". Contribute to\nkakaobrain/honeybee development by creating an account on GitHub.\n\ngithub.com\n\n](https://github.com/kakaobrain/honeybee)\n\n\n\n\n\n\n\n\n\n## **1\\. \ubc30\uacbd**\n\n\n\n**MLLM (Multimodal LLM)** \uc740 _**vision encoder, LLM, projector**_ \uc138\uac00\uc9c0\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/2LHDY/btsFoEXC9jw/KGnS1YJYP9zSFjO7MqM7D1/img.png)\n\n\n\n**vision encoder** \uacfc **LLM** \uc740 \uac01\uac01 \ub530\ub85c\ub530\ub85c \uc0ac\uc804\ud559\uc2b5\ub41c \uac83\uc744 \uc0ac\uc6a9\ud55c\ub2e4. \ub530\ub77c\uc11c \ub450 \ubaa8\ub378\uc744 \uc5f0\uacb0\ud574\uc8fc\uae30 \uc704\ud574\n**projector** \uac00 \ud544\uc694\ud558\ub2e4.  **projector\uc740 vision encoder\uc5d0\uc11c \ub098\uc628 visual feature\uc744 LLM\uc758\nfeature space\ub85c \ub9e4\ud551** \ud574\uc8fc\ub294 \uc5ed\ud560\uc744 \ud55c\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c **vision encoder\uacfc LLM\uc740 \uace0\uc815\ud574\ub450\uace0 projector\uc744\n\ud559\uc2b5** \ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\uc774 \uc9c4\ud589\ub41c\ub2e4.\n\n\n\n\n\n\ub530\ub77c\uc11c \uc774 projector\uc758 \uc5ed\ud560\uc774 \ub9e4\uc6b0 \uc911\uc694\ud55c\ub370, **\ud06c\uac8c \ub450 \uac00\uc9c0 \ud0c0\uc785\uc73c\ub85c \ub098\ub20c \uc218 \uc788\ub2e4.**\n\n\n\n![](https://blog.kakaocdn.net/dn/bsC2Wt/btsFnX4bBfH/qwb0kaNXJkxbezxzCkzTx0/img.png)\n\n\n\n\uccab\ubc88\uc9f8\ub294 LLaVA \ub4f1\uc5d0\uc11c \uc0ac\uc6a9\ud55c _**linear projector**_ \uc774\ub2e4. \ub9d0\uadf8\ub300\ub85c linear layer\uc744 \uc774\uc6a9\ud574 image\nfeature\uc744 \ubcc0\ud658\ud558\ub294 \ubc29\uc2dd\uc778\ub370, \uc774 \ubc29\ubc95\uc740 **feature\uc744 \uc77c\ub300\uc77c \ub9e4\ud551\ud574\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 \uacc4\uc0b0\ub7c9\uc774 \ub9ce\ub2e4** \ub294 \ub2e8\uc810\uc774 \uc788\ub2e4.\n\n\n\n\ub2e4\ub978 \ud558\ub098\ub294 _**Abstractor**_ \ub77c\uace0 \ubd88\ub9ac\ub294 \uae30\ubc95\uc73c\ub85c, InstructBLIP, BLIP-2, miniGPT-4 \ub4f1\uc5d0\uc11c \uc0ac\uc6a9\ud55c\n\ubc29\ubc95\uc774\ub2e4. \uc774\ub4e4\uc740 \uc815\ud574\uc9c4 \uc218\uc758 visual token\uc744 \ucd94\ucd9c\ud574 \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c, **visual token\uc758 \uc218\ub97c \uc801\uc808\ud558\uac8c \uc870\uc808\ud560 \uc218 \uc788\uc5b4\nflexibility\uc640 efficiency\uac00 \ub192\uc73c\ub098 information loss\uac00 \uc788\uc744 \uc218 \uc788\ub2e4.** Abstractor \ubc29\uc2dd\uc740\nresampler, Q-former \ub4f1\uc774 \uc788\ub2e4.\n\n\n\n\n\n\uc774\ub7ec\ud55c efficiency\uc640 flexibility \ub54c\ubb38\uc5d0 **\ucd5c\uadfc abstractor \ubc29\uc2dd\uc774 \ub9ce\uc774 \uc0ac\uc6a9\ub418\uace0 \uc788\ub2e4.** \uadf8\ub7ec\ub098\nabstractor\uc740 **locality preservation\uc774 \uc57d\ud558\ub2e4** \ub294 \ub2e8\uc810\uc774 \uc788\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/cPex1B/btsFoaWz72R/NVLm1YtB7iQAmc8vBnLNAk/img.png)\n\n\n\n\uc704 \uadf8\ub9bc\uc744 \ubcf4\uba74 \ud070 feature\uc778 man\ub9cc \uc7a1\uc544\ub0b4\uace0 pizza, glass \uac19\uc740 \uc560\ub4e4\uc740 \ubabb \uc7a1\uc544 \ub0b4\uace0 \uc788\ub294\uac78 \ubcfc \uc218 \uc788\ub2e4. \ub530\ub77c\uc11c\nspatial understanding \ub2a5\ub825\uc774 \ub5a8\uc5b4\uc9c4\ub2e4.", "start_char_idx": 0, "end_char_idx": 1960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76e09f0d-354d-49eb-8183-61a2659bc563": {"__data__": {"id_": "76e09f0d-354d-49eb-8183-61a2659bc563", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0be635d-53c4-4af1-aa1f-159804d24d42", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "b1cc6360c81ae0ce5ba01dbc2b10cabb9c8f19642c960619187b4c5e6ca3e372", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0b1c862-2f78-4caa-9563-fbf240184e19", "node_type": "1", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "1aab6d60cfd80a3ef70080fa1cedf5402af6b77865c0bb3b61a101f28dc41c6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0d9fd1d-07e8-459b-9e75-8fb3f251a5e8", "node_type": "1", "metadata": {}, "hash": "c093dbd8492d212560a0ab72e6f1692902b6d95fbe836f33eacc9616503c00a3", "class_name": "RelatedNodeInfo"}}, "text": "\uc774\ub7ec\ud55c efficiency\uc640 flexibility \ub54c\ubb38\uc5d0 **\ucd5c\uadfc abstractor \ubc29\uc2dd\uc774 \ub9ce\uc774 \uc0ac\uc6a9\ub418\uace0 \uc788\ub2e4.** \uadf8\ub7ec\ub098\nabstractor\uc740 **locality preservation\uc774 \uc57d\ud558\ub2e4** \ub294 \ub2e8\uc810\uc774 \uc788\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/cPex1B/btsFoaWz72R/NVLm1YtB7iQAmc8vBnLNAk/img.png)\n\n\n\n\uc704 \uadf8\ub9bc\uc744 \ubcf4\uba74 \ud070 feature\uc778 man\ub9cc \uc7a1\uc544\ub0b4\uace0 pizza, glass \uac19\uc740 \uc560\ub4e4\uc740 \ubabb \uc7a1\uc544 \ub0b4\uace0 \uc788\ub294\uac78 \ubcfc \uc218 \uc788\ub2e4. \ub530\ub77c\uc11c\nspatial understanding \ub2a5\ub825\uc774 \ub5a8\uc5b4\uc9c4\ub2e4.\n\n\n\n\ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \ub2e8\uc810\uc744 \uadf9\ubcf5\ud558\uae30 \uc704\ud574 _**local context\ub97c \ubcf4\uc874\ud560 \uc218 \uc788\ub294 abstractor \ubc29\uc2dd**_ \uc744 \uc0c8\ub86d\uac8c\n\uc81c\uc548\ud558\uace0, \uc774\ub97c \uc801\uc6a9\ud55c MLLM\uc778 _**Honeybee**_ \ub97c \ubc1c\ud45c\ud588\ub2e4.\n\n\n\n\n\n\n\n## **\b2. Honeybee**\n\n\n\n![](https://blog.kakaocdn.net/dn/bHHahh/btsFsaapgzl/M2JR8DM52SpjwzxaQqKKVK/img.png)\n\n\n\nHoneybee\uc758 \uc804\uccb4 \uad6c\uc870\ub294 \uc704\uc640 \uac19\ub2e4. **vision encoder** \uc5d0\uc11c visual feature\uc744 \ucd94\ucd9c \ud6c4\n**projector** \uc744 \uac70\uccd0 visual token\uc73c\ub85c \ubcc0\ud658\ud558\uace0, text token\uacfc \ud568\uaed8 **LLM** \uc758 input\uc73c\ub85c \ub123\ub294\ub2e4.\n\n\n\n\uc5ec\uae30\uae4c\uc9c0\ub294 \uc5ec\ud0c0 MLLM\ub4e4\uacfc \ub3d9\uc77c\ud55c \uad6c\uc870\uc774\uace0, \ud575\uc2ec \uad6c\uc870\ub294 \uc0c8\ub86d\uac8c \uc81c\uc548\ud55c projector\uc778 _**C-abstractor**_ \uacfc\n_**D-abstractor**_ \uc774\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bMUtIq/btsFodZ9BI0/FCgCK1ItgJOt7BQYu50GK1/img.png)\n\n\n\n_**C-Abstractor**_ \uc740 local context\ub97c \uc798 \ud3ec\ucc29\ud558\ub294 convolution\uc744 \uc774\uc6a9\ub2e4. ResNet\uc744 \uc5ec\ub7ec\uac1c \uc313\uc544\nvisual token\uc744 \ucd94\ucd9c\ud55c\ub2e4.\n\n**_D-Abstractor_** \uc740 [DETR](https://arxiv.org/pdf/2005.12872.pdf)\uc5d0\uc11c \uc81c\uc548\ud55c\ndeformable attention\uc744 \uc774\uc6a9\ud558\uc5ec visual token\uc744 \ucd94\ucd9c\ud55c\ub2e4.\n\n\n\n\n\n\n\n## **3\\. \ud559\uc2b5\ubc29\ubc95**\n\n\n\n\ud559\uc2b5\uc740 \ub450\ub2e8\uacc4\ub85c \uc9c4\ud589\ub41c\ub2e4. \uccab\ubc88\uc9f8\ub85c vision encoder\uacfc LLM\uc740 freeze\ud558\uace0 abstractor\ub9cc \ud559\uc2b5\ud55c\ub2e4. \uadf8 \ub2e4\uc74c\uc73c\ub85c\nfreeze\ub97c \ud480\uace0 \ubaa8\ub4e0 parameter\uc744 \uc138\ubd80 \uc870\uc815\ud558\ub294 \ub2e8\uacc4\ub97c \uac70\uce5c\ub2e4.\n\n\n\nLLM\uc73c\ub85c\ub294 Vicuna-v1.5 (7B, 13B) \ub450\uac00\uc9c0 \ud06c\uae30\uc758 \ubaa8\ub378\uc744 \uc774\uc6a9\ud588\uace0, vision encoder\uc740 CLIP ViT-L/14\n\ubaa8\ub378\uc744 \uc774\uc6a9\ud588\ub2e4.\n\n\n\n\n\n\n\n## **4\\. \uc2e4\ud5d8\uacb0\uacfc**\n\n\n\n\uacb0\uacfc \uc694\uc57d - **5\uac1c bench\uc5d0\uc11c SoTA\ub97c \ub2ec\uc131\ud588\ub2e4.**\n\n\n\n![](https://blog.kakaocdn.net/dn/cwKCS4/btsFmmiWfD3/KbhYskCikJTU8e9SekjgvK/img.png)\n\n\n\n\ucc38\uace0\ub85c \uac01 bench\uc758 \uc608\uc2dc\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bCNB3J/btsFm7y7kIA/Z43wTCdauhfptj3yR94I31/img.png)\n\n\n\n\uc194\uc9c1\ud788 \uc0ac\ub78c\uc774 \ubd10\ub3c4 \uc880 \uc5b4\ub835\ub2e4.\n\n\n\n\ubcf4\ub2e4 \uc790\uc138\ud55c \uacb0\uacfc \uc9c0\ud45c\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/wrSvo/btsFuHS6tv0/AzYweMkg2mjAKsPnjyOczK/img.png)", "start_char_idx": 1642, "end_char_idx": 3374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0d9fd1d-07e8-459b-9e75-8fb3f251a5e8": {"__data__": {"id_": "e0d9fd1d-07e8-459b-9e75-8fb3f251a5e8", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0be635d-53c4-4af1-aa1f-159804d24d42", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "b1cc6360c81ae0ce5ba01dbc2b10cabb9c8f19642c960619187b4c5e6ca3e372", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76e09f0d-354d-49eb-8183-61a2659bc563", "node_type": "1", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "53a9f33a0315888a21212387226e476c731162233222299977007705018a4b22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "904ecbfa-69e9-4ab9-859c-d6fe5dbb6772", "node_type": "1", "metadata": {}, "hash": "c6186340f522325ca540a135eeab2069f385018c17ec1ee761adb73bba2cb6a2", "class_name": "RelatedNodeInfo"}}, "text": "![](https://blog.kakaocdn.net/dn/cwKCS4/btsFmmiWfD3/KbhYskCikJTU8e9SekjgvK/img.png)\n\n\n\n\ucc38\uace0\ub85c \uac01 bench\uc758 \uc608\uc2dc\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bCNB3J/btsFm7y7kIA/Z43wTCdauhfptj3yR94I31/img.png)\n\n\n\n\uc194\uc9c1\ud788 \uc0ac\ub78c\uc774 \ubd10\ub3c4 \uc880 \uc5b4\ub835\ub2e4.\n\n\n\n\ubcf4\ub2e4 \uc790\uc138\ud55c \uacb0\uacfc \uc9c0\ud45c\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/wrSvo/btsFuHS6tv0/AzYweMkg2mjAKsPnjyOczK/img.png)\n\n\n\nQwen\uc774\ub098 LLaVA \ub4f1\uc740 \ub354 \ud070 vision encoder / image resolution / \ub354 \ub9ce\uc740 visual token\uc744\n\uc774\uc6a9\ud588\uc9c0\ub9cc Honeybee\uc758 \uc131\ub2a5\uc774 \ub354 \ub192\uc558\ub2e4\uace0 \ud55c\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/beW08S/btsFuNMzzrQ/binNxMunkybjmob5oJGtl1/img.png)\n\n\n\nHoneybee\ub3c4 \uc774\ub807\uac8c image resolution\uacfc visual token \uc218\ub97c \ub192\uc774\uba74 \uc131\ub2a5\uc774 \ub354 \uc0c1\uc2b9\ud55c\ub2e4\uace0 \ud55c\ub2e4.\n\n\n\n\n\n\ub2e4\uc74c\uc740 \uc2e4\ud5d8\ub2e8\uacc4\uc5d0\uc11c \uc138\uc6b4 \uac01 \uac00\uc124\uc5d0 \ub300\ud55c \uac80\uc99d\uc774\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/btmjs6/btsFn9i79Ck/4sL3JZmXCk62THWgXrCxS1/img.png)\n\n\n\nC/D-abstractor\uc774 local context preservation\uc5d0 \uc88b\ub2e4\ub294 \uac83\uc744 \ubcf4\uc774\uae30 \uc704\ud574 spatial\nunderstanding capability\ub97c \ubcfc \uc218 \uc788\ub294 task\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \uce21\uc815\ud588\ub2e4\uace0 \ud55c\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bVFBIL/btsFsaha1TU/65IB1parTaujQC7s2P1bT0/img.png)\n\n\n\n\uc704\ub294 performance\uc640 efficiency\uc5d0 \ub300\ud55c \ube44\uad50\uc774\ub2e4. linear\uc740 \uc55e\uc11c \ub9d0\ud588\ub4ef\uc774 \uc77c\ub300\uc77c \ub300\uc751\uc774\ub77c flexibility\uac00 \uc544\uc608\n\uc5c6\ub2e4. resampler\uacfc C-abstractor\uc740 flexible\ud558\uac8c \ub514\uc790\uc778\ud560 \uc218 \uc788\uc73c\uba70, visual token \uc218\uac00 \ub298\uc5b4\ub0a0\uc218\ub85d \uc131\ub2a5\uc774\n\uc99d\uac00\ud558\ub294 \uc591\uc0c1\uc744 \ubcf4\uc774\ub098 C-abstractor\uc758 \uc131\ub2a5\uc774 \ud6e8\uc52c \uc88b\ub2e4.\n\n\n\n\n\n\ub9c8\uc9c0\ub9c9\uc73c\ub85c Honeybee\uac00 \uc0dd\uc131\ud55c \ub2f5\ubcc0\uc758 \uc608\uc2dc\ub4e4\uc774\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bzQkIe/btsFoaWAfZe/FhzUQNAnB8V5QjIHVdpokk/img.png)\n![](https://blog.kakaocdn.net/dn/vpM1G/btsFqNzNeoe/RjiwgyCKodTVzGjsI8FwTK/img.png)", "start_char_idx": 3044, "end_char_idx": 4377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "904ecbfa-69e9-4ab9-859c-d6fe5dbb6772": {"__data__": {"id_": "904ecbfa-69e9-4ab9-859c-d6fe5dbb6772", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0be635d-53c4-4af1-aa1f-159804d24d42", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "b1cc6360c81ae0ce5ba01dbc2b10cabb9c8f19642c960619187b4c5e6ca3e372", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0d9fd1d-07e8-459b-9e75-8fb3f251a5e8", "node_type": "1", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "c3c046ed2a5e776f157a17f0abaaf9aa65e93ad0b36c4dde4e9d3cf3a2c2b579", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df597bea-1054-4d7c-80a9-a79b03167424", "node_type": "1", "metadata": {}, "hash": "d25457c36a5ef85fc0c7733b33d36ec9381399282eba582e42aaca7df69eb450", "class_name": "RelatedNodeInfo"}}, "text": "\ub9c8\uc9c0\ub9c9\uc73c\ub85c Honeybee\uac00 \uc0dd\uc131\ud55c \ub2f5\ubcc0\uc758 \uc608\uc2dc\ub4e4\uc774\ub2e4.\n\n\n\n![](https://blog.kakaocdn.net/dn/bzQkIe/btsFoaWAfZe/FhzUQNAnB8V5QjIHVdpokk/img.png)\n![](https://blog.kakaocdn.net/dn/vpM1G/btsFqNzNeoe/RjiwgyCKodTVzGjsI8FwTK/img.png)\n\n\n\n\ucc38 \uc798\ud558\ub124..\n\n\ubc18\uc751\ud615\n\n\uacf5\uc720\ud558\uae30\n\n\uac8c\uc2dc\uae00 \uad00\ub9ac\n\n_\uad6c\ub3c5\ud558\uae30_ **IBOK**\n\n#### '[\ud83c\udf0c Deep Learning](/category/%F0%9F%8C%8C%20Deep%20Learning) > [\ub17c\ubb38 \ub9ac\ubdf0\n[KOR]](/category/%F0%9F%8C%8C%20Deep%20Learning/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0%20%5BKOR%5D)'\n\uce74\ud14c\uace0\ub9ac\uc758 \ub2e4\ub978 \uae00\n\n[[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] MeZO: Fine-Tuning Language Models with Just Forward Passes\n(NeurIPS 2023)](/206)  (2) | 2024.01.28  \n---|---  \n[[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] AIM: Scalable Pre-training of Large Autoregressive Image Models\n(Apple, 2024)](/205)  (0) | 2024.01.21  \n[Apple\uc758 Multimodal LLM Ferret \ub17c\ubb38 \ub9ac\ubdf0](/203)  (2) | 2024.01.07  \n[[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-\ninvariant Weights (Naver AI Lab, ICLR 2021)](/195)  (0) | 2023.07.23  \n[[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] Audio-Visual Speech Enhancement Using Multimodal Deep\nConvolutional Neural Networks](/168)  (1) | 2022.09.21  \n  \n### Tag\n\n[mllm](/tag/mllm), [Multimodal](/tag/Multimodal)\n\n### '\ud83c\udf0c Deep Learning/\ub17c\ubb38 \ub9ac\ubdf0 [KOR]'\uc758 \ub2e4\ub978\uae00\n\n  * [\uc774\uc804\uae00 **[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] MeZO: Fine-Tuning Language Models with Just Forward Passes (NeurIPS 2023)**](/206)\n  * \ud604\uc7ac\uae00 **\uce74\uce74\uc624\ube0c\ub808\uc778 Multimodal LLM Honeybee \ub17c\ubb38 \ub9ac\ubdf0**\n  * \n\n### \uad00\ub828\uae00\n\n  * [ **[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] MeZO: Fine-Tuning Language Models with Just Forward Passes (NeurIPS 2023)** 2024.01.28 ](/206?category=948904)\n  * [ **[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] AIM: Scalable Pre-training of Large Autoregressive Image Models (Apple, 2024)** 2024.01.21 ](/205?category=948904)\n  * [ **Apple\uc758 Multimodal LLM Ferret \ub17c\ubb38 \ub9ac\ubdf0** 2024.01.07 ](/203?category=948904)\n  * [ **[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights (Naver AI Lab, ICLR 2021)** 2023.07.23 ](/195?category=948904)\n\n\ub313\uae00 1\n\n  * \n\n\ube44\ubc00\uae00 \ub4f1\ub85d\n\n!", "start_char_idx": 4177, "end_char_idx": 5994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df597bea-1054-4d7c-80a9-a79b03167424": {"__data__": {"id_": "df597bea-1054-4d7c-80a9-a79b03167424", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0be635d-53c4-4af1-aa1f-159804d24d42", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "b1cc6360c81ae0ce5ba01dbc2b10cabb9c8f19642c960619187b4c5e6ca3e372", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "904ecbfa-69e9-4ab9-859c-d6fe5dbb6772", "node_type": "1", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "995d066284df103c6a73e044eedada3b97ed31eb97d2935d1b015af4e5a054c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e1af00a-a606-4caf-abe2-3c87351354ab", "node_type": "1", "metadata": {}, "hash": "697a21797801cda05c0908a47976b6ea6fddc204c310322cc10495416648bc51", "class_name": "RelatedNodeInfo"}}, "text": "[\ud504\ub85c\ud544\uc0ac\uc9c4](https://tistory1.daumcdn.net/tistory/3487102/attach/fb04976601014f93b22d0aff6d652500)\n\n\ud83d\udc2c\n\n  * [ \ubd84\ub958 \uc804\uccb4\ubcf4\uae30 (174) ](/category)\n    * [ \ud83c\udf0c Deep Learning (50) ](/category/%F0%9F%8C%8C%20Deep%20Learning)\n      * [ \ub17c\ubb38 \ub9ac\ubdf0 [KOR] (24) ](/category/%F0%9F%8C%8C%20Deep%20Learning/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0%20%5BKOR%5D)\n      * [ Paper Review [ENG] (0) ](/category/%F0%9F%8C%8C%20Deep%20Learning/Paper%20Review%20%5BENG%5D)\n      * [ DL & ML \uc870\uac01 \uc9c0\uc2dd (4) ](/category/%F0%9F%8C%8C%20Deep%20Learning/DL%20%26%20ML%20%EC%A1%B0%EA%B0%81%20%EC%A7%80%EC%8B%9D)\n      * [ Overview (6) ](/category/%F0%9F%8C%8C%20Deep%20Learning/Overview)\n      * [ Dataset (3) ](/category/%F0%9F%8C%8C%20Deep%20Learning/Dataset)\n      * [ \ud3c9\uac00 (3) ](/category/%F0%9F%8C%8C%20Deep%20Learning/%ED%8F%89%EA%B0%80)\n      * [ Implementation (6) ](/category/%F0%9F%8C%8C%20Deep%20Learning/Implementation)\n      * [ Etc. (4) ](/category/%F0%9F%8C%8C%20Deep%20Learning/Etc.)\n    * [ \ud83d\udc0d Python & library (49) ](/category/%F0%9F%90%8D%20Python%20%26%20library)\n      * [ Python (5) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Python)\n      * [ PyTorch (18) ](/category/%F0%9F%90%8D%20Python%20%26%20library/PyTorch)\n      * [ PyTorch Lightning (2) ](/category/%F0%9F%90%8D%20Python%20%26%20library/PyTorch%20Lightning)\n      * [ Tensorflow (1) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Tensorflow)\n      * [ Flax (0) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Flax)\n      * [ HuggingFace (5) ](/category/%F0%9F%90%8D%20Python%20%26%20library/HuggingFace)\n      * [ Scikit-Learn (4) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Scikit-Learn)\n      * [ numpy (2) ](/category/%F0%9F%90%8D%20Python%20%26%20library/numpy)\n      * [ librosa (4) ](/category/%F0%9F%90%8D%20Python%20%26%20library/librosa)\n      * [ SimpleITK (4) ](/category/%F0%9F%90%8D%20Python%20%26%20library/SimpleITK)\n      * [ Etc. (4) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Etc.)", "start_char_idx": 5994, "end_char_idx": 7945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e1af00a-a606-4caf-abe2-3c87351354ab": {"__data__": {"id_": "0e1af00a-a606-4caf-abe2-3c87351354ab", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0be635d-53c4-4af1-aa1f-159804d24d42", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "b1cc6360c81ae0ce5ba01dbc2b10cabb9c8f19642c960619187b4c5e6ca3e372", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df597bea-1054-4d7c-80a9-a79b03167424", "node_type": "1", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "46910d84705f387d0f3f07c0ed6a783be7f47ea827917bb362bc0b34b77b7069", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6de95ee5-2f6b-4475-a7bc-5a0e045bbfac", "node_type": "1", "metadata": {}, "hash": "da64866a7a89523f284b44d3c9eeeb931a197bde145b7cb35a294f5342f37ced", "class_name": "RelatedNodeInfo"}}, "text": "(4) ](/category/%F0%9F%90%8D%20Python%20%26%20library/Etc.)\n    * [ \ud83d\udc7b OS & Tools (33) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools)\n      * [ Ubuntu (14) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/Ubuntu)\n      * [ Mac (3) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/Mac)\n      * [ Windows (1) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/Windows)\n      * [ VSCode (3) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/VSCode)\n      * [ Git (3) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/Git)\n      * [ LaTeX (8) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/LaTeX)\n      * [ Tools (1) ](/category/%F0%9F%91%BB%20OS%20%26%20Tools/Tools)\n    * [ \ud83d\udc7d Language & Frameworks (6) ](/category/%F0%9F%91%BD%20Language%20%26%20Frameworks)\n      * [ Matlab (1) ](/category/%F0%9F%91%BD%20Language%20%26%20Frameworks/Matlab)\n      * [ Spark (5) ](/category/%F0%9F%91%BD%20Language%20%26%20Frameworks/Spark)\n    * [ \ud83d\udd2c Medical Image (13) ](/category/%F0%9F%94%AC%20Medical%20Image)\n      * [ MRI (3) ](/category/%F0%9F%94%AC%20Medical%20Image/MRI)\n      * [ Processing (5) ](/category/%F0%9F%94%AC%20Medical%20Image/Processing)\n      * [ \ub17c\ubb38 \ub9ac\ubdf0 (5) ](/category/%F0%9F%94%AC%20Medical%20Image/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)\n    * [ \ud83d\udca9 \uc5d0\ub7ec \ud574\uacb0 (7) ](/category/%F0%9F%92%A9%20%EC%97%90%EB%9F%AC%20%ED%95%B4%EA%B2%B0)\n    * [ \ud83d\udd11 CS (11) ](/category/%F0%9F%94%91%20CS)\n      * [ \ucf54\ub529\ud14c\uc2a4\ud2b8 (10) ](/category/%F0%9F%94%91%20CS/%EC%BD%94%EB%94%A9%ED%85%8C%EC%8A%A4%ED%8A%B8)\n      * [ \uc54c\uace0\ub9ac\uc998 (1) ](/category/%F0%9F%94%91%20CS/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)\n    * [ \ud83e\udd68 \uc774\uac83\uc800\uac83 (5) ](/category/%F0%9F%A5%A8%20%EC%9D%B4%EA%B2%83%EC%A0%80%EA%B2%83)\n    * [ \ud83d\udcd6 \ub3c5\ud6c4\uac10 (0) ](/category/%F0%9F%93%96%20%EB%8F%85%ED%9B%84%EA%B0%90)\n\n\ubc18\uc751\ud615\n\n### \ucd5c\uadfc\uae00\uacfc \uc778\uae30\uae00\n\n  * \ucd5c\uadfc\uae00\n  * \uc778\uae30\uae00\n\n  * [\n\n**\uce74\uce74\uc624\ube0c\ub808\uc778 Multimodal LLM Honeybee \ub17c\ubb38 \ub9ac\ubdf0** 2024.03.02 16:09 ](/207)\n\n  * [\n\n**[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] MeZO: Fine-Tuning Language Models with Just Forward Passes \u22ef**\n2024.01.", "start_char_idx": 7886, "end_char_idx": 9778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6de95ee5-2f6b-4475-a7bc-5a0e045bbfac": {"__data__": {"id_": "6de95ee5-2f6b-4475-a7bc-5a0e045bbfac", "embedding": null, "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0be635d-53c4-4af1-aa1f-159804d24d42", "node_type": "4", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "b1cc6360c81ae0ce5ba01dbc2b10cabb9c8f19642c960619187b4c5e6ca3e372", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e1af00a-a606-4caf-abe2-3c87351354ab", "node_type": "1", "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}, "hash": "c213d5036bfa8c305dc58c9e1dc17ef4739fb243627f754e5d18aa411c68e5f5", "class_name": "RelatedNodeInfo"}}, "text": "03.02 16:09 ](/207)\n\n  * [\n\n**[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] MeZO: Fine-Tuning Language Models with Just Forward Passes \u22ef**\n2024.01.28 22:42 ](/206)\n\n  * [\n\n**[\ub525\ub7ec\ub2dd \ub17c\ubb38\ub9ac\ubdf0] AIM: Scalable Pre-training of Large Autoregressive Image Mo\u22ef**\n2024.01.21 23:03 ](/205)\n\n  * [\n\n**\ub0b4\uac00 \ubcf4\ub824\uace0 \uc815\ub9ac\ud558\ub294 LaTex \uc790\uc8fc \uc4f0\ub294 \uc218\uc2dd \uc815\ub9ac** 2021.11.16 15:29 ](/97)\n\n  * [\n\n**LaTex \ud45c \uad00\ub828 \ud301 (\ud45c \uc790\ub3d9 \uc0dd\uc131\uae30, \ud3f0\ud2b8 \ud06c\uae30 \uc870\uc815, \uc140 \ub108\ube44, \ud45c \ub0b4\ubd80 \uc5ec\ubc31, footnote \ub2ec\uae30)** 2022.04.26\n22:14 ](/128)\n\n  * [\n\n**[matplotlib] matplotlib.pyplot\uc744 \uc774\uc6a9\ud55c \uc774\ubbf8\uc9c0 \uc2dc\uac01\ud654 \ucd1d\uc815\ub9ac** 2022.08.29 15:58 ](/161)\n\n### \ucd5c\uadfc\ub313\uae00\n\n  * [ **\ub17c\ubb38\uc744 \ubcf4\uba74 ResNet-D\ub294 ResNet-B\uc5d0 average pooling\uc744 \ucd94\uac00\ud55c \ud615\ud0dc\u22ef**\n\n\ud589\uc778\n\n](/133#comment21315301)\n\n  * [ **https://m.blog.naver.com/edennnie/223155243141**\n\nhmm\n\n](/73#comment21295201)\n\n  * [ **nvidia-smi\ub294 GPU\ub4dc\ub77c\uc774\ubc84\uac00 \uc9c0\uc6d0\ud558\ub294 \ucd5c\uc2e0 CUDA\ubc84\uc804\uc774\uace0, nvcc --vers\u22ef**\n\nhmm\n\n](/73#comment21295174)\n\n### \ubc29\ubb38\uc790\uc218Total\n\n412,621\n\n  * Today : 37\n  * Yesterday : 867\n\n### Calendar\n\n[\u00ab](/archive/202404 \"1\uac1c\uc6d4 \uc55e\uc758 \ub2ec\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\")   [2024/05](/archive/202405 \"\ud604\uc7ac \ub2ec\uc758\n\ub2ec\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\")   [\u00bb](/archive/202406 \"1\uac1c\uc6d4 \ub4a4\uc758 \ub2ec\ub825\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\") \uc77c | \uc6d4 | \ud654 | \uc218 | \ubaa9 | \uae08\n| \ud1a0  \n---|---|---|---|---|---|---  \n|  |  | 1 | 2 | 3 | 4  \n5 | 6 | 7 | 8 | 9 | 10 | 11  \n12 | 13 | 14 | 15 | 16 | 17 | 18  \n19 | 20 | 21 | 22 | 23 | 24 | 25  \n26 | 27 | 28 | 29 | 30 | 31 |  \n  \nCopyright \u00a9 Kakao Corp. All rights reserved.\n\n\uad00\ub828\uc0ac\uc774\ud2b8\n\n  * [Github](https://github.com/bo-10000)\n\n## \ud2f0\uc2a4\ud1a0\ub9ac\ud234\ubc14\n\n**IBOK** _\uad6c\ub3c5\ud558\uae30_", "start_char_idx": 9666, "end_char_idx": 10981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"c9ef9b7c-6dd0-4935-a552-9b3674a6f388": {"node_ids": ["9e1bb884-42d3-4a53-b6fd-0c763c91b8ff"], "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-0.txt", "file_name": "output-0.txt", "file_type": "text/plain", "file_size": 61, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}}, "7344652d-6789-4c15-96cc-ea8f7128b966": {"node_ids": ["e504fc55-ba53-482d-8ee8-08eb0be8e6f2", "951cb1fd-c1c1-48e4-9e60-c1db7e07330d"], "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-1.txt", "file_name": "output-1.txt", "file_type": "text/plain", "file_size": 4805, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}}, "a0be635d-53c4-4af1-aa1f-159804d24d42": {"node_ids": ["c0b1c862-2f78-4caa-9563-fbf240184e19", "76e09f0d-354d-49eb-8183-61a2659bc563", "e0d9fd1d-07e8-459b-9e75-8fb3f251a5e8", "904ecbfa-69e9-4ab9-859c-d6fe5dbb6772", "df597bea-1054-4d7c-80a9-a79b03167424", "0e1af00a-a606-4caf-abe2-3c87351354ab", "6de95ee5-2f6b-4475-a7bc-5a0e045bbfac"], "metadata": {"file_path": "/nfs/home/seil/yaicon/data/instructblip/output-2.txt", "file_name": "output-2.txt", "file_type": "text/plain", "file_size": 13531, "creation_date": "2024-05-08", "last_modified_date": "2024-05-08"}}}}