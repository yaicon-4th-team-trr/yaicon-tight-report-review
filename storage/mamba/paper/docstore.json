{"docstore/data": {"514e8902-9112-4d6e-815a-9c89ec369adc": {"__data__": {"id_": "514e8902-9112-4d6e-815a-9c89ec369adc", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e601a121-939c-42de-ab20-bd854caf9563", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "fe2cce0182e33dad9ef74adf49b1d084705b51e863786885f396f0a0cdbdda74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93e1f66e-dd05-46d0-b978-3a3fee314407", "node_type": "1", "metadata": {}, "hash": "ab117f2bf0b23f3f2dffd8b5af3e229f2cd33ae4683bb8f5e6fd68f0821c5945", "class_name": "RelatedNodeInfo"}}, "text": "\ubcf8\ubb38 \ubc14\ub85c\uac00\uae30\n\n# [Ostin X](https://ostin.tistory.com/)\n\n\uba54\ub274\n\n  * [ \ubd84\ub958 \uc804\uccb4\ubcf4\uae30 (483) ](/category)\n    * [ \ub17c\ubb38 \ub9ac\ubdf0 (0) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)\n      * [ Language Model (122) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Language%20Model)\n      * [ Diffusion Model (136) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Diffusion%20Model)\n      * [ Vision Transformer (62) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Vision%20Transformer)\n      * [ Mamba (7) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Mamba)\n      * [ GAN (20) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/GAN)\n      * [ etc. (57) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/etc.)\n      * [ Concept (5) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Concept)\n      * [ \ub17c\ubb38 \ubd84\ub958 (7) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/%EB%85%BC%EB%AC%B8%20%EB%B6%84%EB%A5%98)\n    * [ \ucf54\ub4dc \ub9ac\ubdf0 (8) ](/category/%EC%BD%94%EB%93%9C%20%EB%A6%AC%EB%B7%B0)\n      * [ Diffusion (8) ](/category/%EC%BD%94%EB%93%9C%20%EB%A6%AC%EB%B7%B0/Diffusion)\n    * [ Deep Learning (34) ](/category/Deep%20Learning)\n      * [ GAN (14) ](/category/Deep%20Learning/GAN)\n      * [ Fine Tuning (10) ](/category/Deep%20Learning/Fine%20Tuning)\n      * [ Diffusion (4) ](/category/Deep%20Learning/Diffusion)\n      * [ Memo or etc. (6) ](/category/Deep%20Learning/Memo%20or%20etc.)\n    * [ Code, Error, Tip, Etc. (4) ](/category/Code%2C%20Error%2C%20Tip%2C%20Etc.)\n    * [ Output (8) ](/category/Output)\n      * [ Model (4) ](/category/Output/Model)\n      * [ Small Things (4) ](/category/Output/Small%20Things)\n    * [ \uc0ac\uc124 (13) ](/category/%EC%82%AC%EC%84%A4)\n      * [ X (0) ](/category/%EC%82%AC%EC%84%A4/X)\n      * [ \ub3c5\ud6c4\uac10 (3) ](/category/%EC%82%AC%EC%84%A4/%EB%8F%85%ED%9B%84%EA%B0%90)\n\n\ube14\ub85c\uadf8 \ub0b4 \uac80\uc0c9 \uac80\uc0c9\n\n* * *\n\n\ub17c\ubb38 \ub9ac\ubdf0/Mamba\n\n# Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\nOstin 2024\\. 1. 15. 15:07\n\n\\+ [Mamba\uc5d0 \ub300\ud574 \ub9e4\uc6b0 \uc798 \uc124\uba85\ub418\uc5b4 \uc788\ub294 \uae00](https://tulip-\nphalange-a1e.notion.site/05f977226a0e44c6b35ed9bfe0076839) <- \uadf8\ub0e5 \uc774\uac70 \ubcf4\uc138\uc694\n\n\n\nSSM\uc5d0 \uc120\ud0dd\uc131\uc744 \ubd80\uc5ec\ud558\uace0 \ud558\ub4dc\uc6e8\uc5b4 \ucd5c\uc801\ud654\n\n\n\n[[Github](https://github.com/state-spaces/mamba)]\n\n[[arXiv](https://arxiv.org/abs/2312.00752)](2023/12/01 version v1)", "start_char_idx": 0, "end_char_idx": 2170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "93e1f66e-dd05-46d0-b978-3a3fee314407": {"__data__": {"id_": "93e1f66e-dd05-46d0-b978-3a3fee314407", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e601a121-939c-42de-ab20-bd854caf9563", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "fe2cce0182e33dad9ef74adf49b1d084705b51e863786885f396f0a0cdbdda74", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "514e8902-9112-4d6e-815a-9c89ec369adc", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "4b1a26908a7e5a98f08f1a1eb127222113f4d73d031166a31e9382c8ec7db812", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b5424f0-a83f-4033-8353-04207a8e8b2a", "node_type": "1", "metadata": {}, "hash": "7e5e9f6d7c7fef6e04a8ec2083828ba6b2587ee96f322138cf581f20554429fe", "class_name": "RelatedNodeInfo"}}, "text": "SSM\uc5d0 \uc120\ud0dd\uc131\uc744 \ubd80\uc5ec\ud558\uace0 \ud558\ub4dc\uc6e8\uc5b4 \ucd5c\uc801\ud654\n\n\n\n[[Github](https://github.com/state-spaces/mamba)]\n\n[[arXiv](https://arxiv.org/abs/2312.00752)](2023/12/01 version v1)\n\n\n\n\n\n\n\n## __**Abstract**__\n\nTransformer \uae30\ubc18 \ubaa8\ub378\ub4e4\uc774 \uae34 \uc2dc\ud000\uc2a4 \ucc98\ub9ac\uc5d0\uc11c \ubcf4\uc5ec\uc8fc\ub294 \uacc4\uc0b0 \ube44\ud6a8\uc728\uc131\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574 Mamba\ub77c\ub294 \uc0c8\ub85c\uc6b4 \uc2e0\uacbd\ub9dd \uad6c\uc870\ub97c \uc81c\uc548\n\n\n\n\n\n\n\n## __**State Space Models**__\n\n[\ud544\ub3c5!!! S4 model](https://ostin.tistory.com/379)\n\n[  \n\nEfficiently Modeling Long Sequences with Structured State Spaces (S4)\n\n[arXiv](2022/08/05 version v3) \uc601\uc5b4 \uc798\ud558\uc2dc\uba74 \uc774\uac70 \ubcf4\uc138\uc694.\nhttps://srush.github.io/annotated-s4/ \uadfc\ub370 \uc194\uc9c1\ud788 \uc6d0\uc5b4\ubbfc\ub3c4 \uc774\uac70 \ubcf4\uace0 \uc774\ud574 \ubabb \ud560 \ub4ef; The\nAnnotated S4 srush.github.io \uc2dc\uc791\ud558\uae30 \uc804\uc5d0 \ub9d0\ud558\uc790\uba74 \uc774 \ub17c\ubb38\uc5d0 \uad00\n\nostin.tistory.com\n\n](https://ostin.tistory.com/379)\n\n\n\n\ud45c\uae30\uac00 \uc0b4\uc9dd \ub2e4\ub974\uace0(state h(t), input x) \ud53c\ub77c\ubbf8\ud130\uac00 4\uac1c\uc785\ub2c8\ub2e4. (\u2206, A, B, C)\n\n![](https://blog.kakaocdn.net/dn/TWWHe/btsDmWz0nbf/bisXpSd3Zm3eO1HCVMxMo0/img.png)\n\n\n\n__Discretization__\n\n\uc774\uc0b0 \uc2dc\uac04 \ud53c\ub77c\ubbf8\ud130\ub294 \uc5f0\uc18d \uc2dc\uac04 \ud53c\ub77c\ubbf8\ud130 A, B\ub85c \ub098\ud0c0\ub0bc \uc218 \uc788\ub2e4.\n\n![](https://blog.kakaocdn.net/dn/lwCH3/btsDqaRh5xL/UpdPoSmPqTKlWKyNBpBEN1/img.png)\n\n\n\n__Computation__\n\n\uc804\uccb4 \uc785\ub825 \uc2dc\ud000\uc2a4\ub97c \ubbf8\ub9ac \ubcfc \uc218 \uc788\ub294 \uacbd\uc6b0 convolution mode(3a, 3b)\ub97c \uc0ac\uc6a9\ud558\uace0 \ud55c \ubc88\uc5d0 \ud55c timestep\ub9cc \ubcfc \uc218 \uc788\ub294\n\uacbd\uc6b0 recurrent mode(2a, 2b)\ub85c \uc804\ud658\ud55c\ub2e4.\n\n\n\n__Linear Time Invariance (LTI)__\n\n\ubc29\uc815\uc2dd (1~3)\uc758 \uc911\uc694\ud55c \ud2b9\uc131\uc740 \ubaa8\ub378\uc758 dynamics\uac00 \uc2dc\uac04\uc5d0 \ub530\ub77c \uc77c\uc815\ud558\ub2e4\ub294 \uac83\uc774\ub2e4.\n\n\uc774\ub97c [Linear Time-Invariant](https://keyboard-lover.tistory.com/18)(LTI,\n\uc2dc\ubd88\ubcc0\uc131)\uc774\ub77c\uace0 \ud55c\ub2e4.\n\n![](https://blog.kakaocdn.net/dn/bW4u2f/btsDqaEOWf6/cbjazLVuA2CkqvRbA7qtoK/img.png)\n\n\n\n\uc9c0\uae08\uae4c\uc9c0 \ubaa8\ub4e0 structured SSM\uc740 LTI\uc600\ub2e4.\n\n\uadf8\ub7ec\ub098 \ubcf8 \ub17c\ubb38\uc758 \ud575\uc2ec \ud1b5\ucc30\uc740 LTI \uc81c\uc57d\uc744 \uc81c\uac70\ud558\ub294 \uac83\uacfc \uad00\ub828\uc774 \uc788\ub2e4.\n\n\n\n__Structure and Dimensions__\n\nStructured SSM\uc774\ub77c\ub294 \uc774\ub984\uc774 \ubd99\uc5ec\uc9c4 \uc774\uc720\ub294 SSM\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \uacc4\uc0b0\ud558\uae30 \uc704\ud574 A\uc5d0 \uad6c\uc870\ub97c \ubd80\uc5ec\ud574\uc57c \ud588\uae30 \ub54c\ubb38\uc774\ub2e4. \uac00\uc7a5 \ub110\ub9ac\n\uc0ac\uc6a9\ub418\ub294 \uad6c\uc870\ub294 \ub300\uac01\uc120\uc774\uba70, \ubcf8\ubb38\uc5d0\uc11c\ub3c4 \uc774\ub97c \uc0ac\uc6a9\ud55c\ub2e4.\n\n\n\n\uadf8\ub7f4 \uacbd\uc6b0 A \u2208 \u211dN\u00d7N, B \u2208 \u211dN\u00d71, C \u2208 \u211d1\u00d7N\uc774\uace0 batch size B, length L, channels D\uc778 \uc2dc\ud000\uc2a4\nx\uc5d0 \ub300\ud574 \uc791\ub3d9\ud558\ub824\uba74 SSM\uc774 \uac01 \ucc44\ub110\uc5d0 \ub3c5\ub9bd\uc801\uc73c\ub85c \uc801\uc6a9\ub418\uc5b4\uc57c \ud558\uba70 hidden state\uac00 DN\uc774 \ub418\uace0, \uc774\ub294 \ubcd1\ubaa9 \ud604\uc0c1\uc758 \uadfc\uc6d0\uc774\ub2e4.\n\uc694\ucee8\ub300 \ucc28\uc6d0\uc774 \ubd80\uc871\ud574\uc11c.\n\n\n\n\n\n\n\n## __**Selective State Space Models**__\n\n  1. Motivation: Selection as a Means of Compression\n  2. Improving SSMs with Selection\n  3. Efficient Implementation of Selective SSMs\n  4. A Simplified SSM Architecture\n  5. Properties of Selection Mechanisms", "start_char_idx": 2026, "end_char_idx": 3983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4b5424f0-a83f-4033-8353-04207a8e8b2a": {"__data__": {"id_": "4b5424f0-a83f-4033-8353-04207a8e8b2a", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e601a121-939c-42de-ab20-bd854caf9563", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "fe2cce0182e33dad9ef74adf49b1d084705b51e863786885f396f0a0cdbdda74", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93e1f66e-dd05-46d0-b978-3a3fee314407", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "b61eb4bc8ae8be31b738e46961a0e1867b3f5453ce233b9f2190b9e2f2bc6d8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c60956e-1965-45c9-a1a8-ea49e2ddcf7f", "node_type": "1", "metadata": {}, "hash": "87e4f21578be943d19a4d26f9d9a42c0d20de0598ac5abdc77d04dd2866c737b", "class_name": "RelatedNodeInfo"}}, "text": "\uadf8\ub7f4 \uacbd\uc6b0 A \u2208 \u211dN\u00d7N, B \u2208 \u211dN\u00d71, C \u2208 \u211d1\u00d7N\uc774\uace0 batch size B, length L, channels D\uc778 \uc2dc\ud000\uc2a4\nx\uc5d0 \ub300\ud574 \uc791\ub3d9\ud558\ub824\uba74 SSM\uc774 \uac01 \ucc44\ub110\uc5d0 \ub3c5\ub9bd\uc801\uc73c\ub85c \uc801\uc6a9\ub418\uc5b4\uc57c \ud558\uba70 hidden state\uac00 DN\uc774 \ub418\uace0, \uc774\ub294 \ubcd1\ubaa9 \ud604\uc0c1\uc758 \uadfc\uc6d0\uc774\ub2e4.\n\uc694\ucee8\ub300 \ucc28\uc6d0\uc774 \ubd80\uc871\ud574\uc11c.\n\n\n\n\n\n\n\n## __**Selective State Space Models**__\n\n  1. Motivation: Selection as a Means of Compression\n  2. Improving SSMs with Selection\n  3. Efficient Implementation of Selective SSMs\n  4. A Simplified SSM Architecture\n  5. Properties of Selection Mechanisms\n\n\n\n#### **Motivation: Selection as a Means of Compression**\n\n\uc2dc\ud000\uc2a4 \ubaa8\ub378\uc740 efficiency vs effectiveness\uc758 trade-off\uac00 \uc911\uc694\ud558\ub2e4.\n\n\n\nAttention\uc740 context\ub97c \uc804\ud600 \uc555\ucd95\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 \ud6a8\uacfc\uc801\uc774\uc9c0\ub9cc \ube44\ud6a8\uc728\uc801\uc774\ub2e4.\n\n\n\nLTI \ubaa8\ub378\uc740 \ud6a8\uc728\uc801\uc774\uc9c0\ub9cc \ub0b4\uc6a9 \uc778\uc2dd\uc774 \ubd80\uc871\ud558\uc5ec \uc544\ub798 \uadf8\ub9bc\uc758 \uc624\ub978\ucabd\uacfc \uac19\uc774 \uc785\ub825\uacfc \ucd9c\ub825 \uc0ac\uc774\uc758 \uac04\uaca9\uc774 \ub2e4\uc591\ud558\uace0 \uc815\ubcf4\ub97c \uc120\ud0dd\uc801\uc73c\ub85c \ucde8\ud569\ud574\uc57c\n\ud558\ub294 \uacbd\uc6b0\ub97c \ubaa8\ub378\ub9c1\ud560 \uc218 \uc5c6\ub2e4.\n\n![](https://blog.kakaocdn.net/dn/zEbrH/btsDrHO0JsM/CyUVFgWPQPcCm0QAmxmeT0/img.png)\n\n\n\n#### **Improving SSMs with Selection**\n\n\ucd94\uac00 \ucc28\uc6d0\uc744 \ubc1b\uc544\ub4e4\uc774\uace0 \uc2dc\uac00\ubcc0\uc131, \uc120\ud0dd\uc131\uc744 \ubd80\uc5ec\ud558\uae30 \uc704\ud574 linear projection\uc744 \ub3c4\uc785\ud558\uc600\ub2e4. \uc774\uc804\uae4c\uc9c0\uc758 \ud53c\ub77c\ubbf8\ud130\ub294 \ub2e8\uc21c \ud589\ub82c.\n\n![](https://blog.kakaocdn.net/dn/bbNP1w/btsDqOVvjlA/jgRK5DR2hHDMyvFKc7FLt1/img.png)\n![](https://blog.kakaocdn.net/dn/TVVkA/btsDpbcPVrC/AgrMuIZ8NLC4nPsoF5ZTaK/img.png)\n\n\n\n#### **Efficient Implementation of Selective SSMs**\n\n__Motivation of Prior Models__\n\nHidden state dimension\uc774 \ud070 \ubaa8\ub378\uc740 \ud6a8\uacfc\uc801\uc774\uc9c0\ub9cc \ube44\ud6a8\uc728\uc801\uc774\ub2e4. \ud6a8\uc728\uc131\uc744 \uc800\ud558\uc2dc\ud0a4\uc9c0 \uc54a\uace0 hidden state\ndimension\uc744 \ucd5c\ub300\ud654\ud558\uace0\uc790 \ud55c\ub2e4.\n\n\n\nRecurrent mode\ub294 convolution mode\ubcf4\ub2e4 \uc720\uc5f0\ud558\uc9c0\ub9cc hidden state\ub97c \uacc4\uc0b0\ud574\uc57c \ud558\ubbc0\ub85c \uc774\ub97c \uc6b0\ud68c\ud560 \uc218 \uc788\ub294\nconvolution mode\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \ub354 \ud6a8\uc728\uc801\uc774\ub2e4.\n\n\n\n__Overview of Selective Scan: Hardware-Aware State Expansion__\n\nRecurrent\ub294 O(BLDN) FLOPs\ub97c \uc0ac\uc6a9\ud558\uace0 convolution\uc740 O(BLDlog(L)) FLOPs\ub97c \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0 L\uc774\n\ucda9\ubd84\ud788 \ud06c\uace0 N\uc774 \ud06c\uc9c0 \uc54a\uc740 \uacbd\uc6b0, recurrent\uac00 \uc2e4\uc81c\ub85c \ub354 \uc801\uc740 FLOPs\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\ub2e4.\n\n\n\n\ud55c \uac00\uc9c0 \ubb38\uc81c\ub294, recurrent\uc758 hidden state \uacc4\uc0b0\uc73c\ub85c \uc778\ud574 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\uc774 \ub9ce\ub2e4\ub294 \uc810\uc774\ub2e4.", "start_char_idx": 3558, "end_char_idx": 5178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2c60956e-1965-45c9-a1a8-ea49e2ddcf7f": {"__data__": {"id_": "2c60956e-1965-45c9-a1a8-ea49e2ddcf7f", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e601a121-939c-42de-ab20-bd854caf9563", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "fe2cce0182e33dad9ef74adf49b1d084705b51e863786885f396f0a0cdbdda74", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b5424f0-a83f-4033-8353-04207a8e8b2a", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e3f22f1a49d5698c44b86828b2ce447d13983e970bd836889e494c747a05a08b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c166faa4-3864-41b1-b200-0ac816dcc8b3", "node_type": "1", "metadata": {}, "hash": "7bfb54f871774d2967b6861a0fbbb943b61012c67dca2d1a0e9317f297e5dae3", "class_name": "RelatedNodeInfo"}}, "text": "Recurrent mode\ub294 convolution mode\ubcf4\ub2e4 \uc720\uc5f0\ud558\uc9c0\ub9cc hidden state\ub97c \uacc4\uc0b0\ud574\uc57c \ud558\ubbc0\ub85c \uc774\ub97c \uc6b0\ud68c\ud560 \uc218 \uc788\ub294\nconvolution mode\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \ub354 \ud6a8\uc728\uc801\uc774\ub2e4.\n\n\n\n__Overview of Selective Scan: Hardware-Aware State Expansion__\n\nRecurrent\ub294 O(BLDN) FLOPs\ub97c \uc0ac\uc6a9\ud558\uace0 convolution\uc740 O(BLDlog(L)) FLOPs\ub97c \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0 L\uc774\n\ucda9\ubd84\ud788 \ud06c\uace0 N\uc774 \ud06c\uc9c0 \uc54a\uc740 \uacbd\uc6b0, recurrent\uac00 \uc2e4\uc81c\ub85c \ub354 \uc801\uc740 FLOPs\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\ub2e4.\n\n\n\n\ud55c \uac00\uc9c0 \ubb38\uc81c\ub294, recurrent\uc758 hidden state \uacc4\uc0b0\uc73c\ub85c \uc778\ud574 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\uc774 \ub9ce\ub2e4\ub294 \uc810\uc774\ub2e4.\n\n\n\n  * Kernel Fusion: \uc2a4\uce94 \uc785\ub825 (A\u0304, B\u0304)\ub97c \ub290\ub9b0 GPU HBM\uc5d0\uc11c \uc900\ube44\ud558\ub294 \ub300\uc2e0 SRAM\uc73c\ub85c \ud53c\ub77c\ubbf8\ud130 (\u2206, A, B, C)\ub97c \uc9c1\uc811 \ub85c\ub4dc\ud558\uace0 \uc774\uc0b0\ud654, \uc2a4\uce94, C\uc640\uc758 \uacf1\uc148\uc744 \ud558\ub098\uc758 \ucee4\ub110\ub85c \uc735\ud569\ud558\uc5ec \ubaa8\ub450 SRAM\uc5d0\uc11c \uc218\ud589\ud55c \ub4a4 \ucd9c\ub825\uc744 HBM\uc5d0 \uae30\ub85d\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c memory I/O\ub97c \ud06c\uac8c \uc904\uc778\ub2e4.\n  * Recomputation: \uc21c\uc804\ud30c \uc2dc \uc5ed\uc804\ud30c\uc5d0 \ud544\uc694\ud55c intermediate state\ub97c \uc800\uc7a5\ud558\uc9c0 \uc54a\uace0 \uc5ed\uc804\ud30c \uc2dc \uc7ac\uacc4\uc0b0\ud568\uc73c\ub85c\uc368 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \uc904\uc778\ub2e4.\n\n![](https://blog.kakaocdn.net/dn/ck1HQW/btsDp7nTUBl/mIjZGVNCerkH0hFxZecytK/img.png)\n\n\n\n#### **A Simplified SSM Architecture**\n\n![](https://blog.kakaocdn.net/dn/wf5kO/btsDs4wN2JE/dtYK0EqEvxVm3sGhwsKRz0/img.png)\n\n\uc785\ub825 projection\uc5d0\uc11c \ubaa8\ub378 \ucc28\uc6d0\uc744 \ud655\uc7a5\ud55c \ub450 \uac1c\uc758 mamba block stack\uc774 MHA\uacfc MLP\uac00 \uc788\ub294 transformer\nblock \ud558\ub098\uc758 \ud53c\ub77c\ubbf8\ud130 \uc218\uc640 \ub9de\uba39\ub294\ub2e4. transformer\ucc98\ub7fc \uc6d0\ud558\ub294 \ub9cc\ud07c \uc313\uc73c\uba74 \ub418\ub294 \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4.\n\n\ub300\ubd80\ubd84\uc758 \ud53c\ub77c\ubbf8\ud130\ub294 projection\uc5d0 \uc874\uc7ac\ud558\uace0 SSM\uc758 \ud53c\ub77c\ubbf8\ud130\uc758 \ube44\uc911\uc740 \ud6e8\uc52c \uc801\uc73c\uba70,\n[SiLU](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html) \ud65c\uc131\ud654 \uc0ac\uc6a9.\n\n\n\n#### **\uc2e4\ud5d8 \ubc0f \uc7a1\ub2e4\ud55c \uc774\uc57c\uae30\ub4e4 \uc0dd\ub7b5~**", "start_char_idx": 4807, "end_char_idx": 5954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c166faa4-3864-41b1-b200-0ac816dcc8b3": {"__data__": {"id_": "c166faa4-3864-41b1-b200-0ac816dcc8b3", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e601a121-939c-42de-ab20-bd854caf9563", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "fe2cce0182e33dad9ef74adf49b1d084705b51e863786885f396f0a0cdbdda74", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c60956e-1965-45c9-a1a8-ea49e2ddcf7f", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0b2bdc0b31c077221590ee8f557393e1011a28e2a66ccee90215ac86919aa614", "class_name": "RelatedNodeInfo"}}, "text": "#### **\uc2e4\ud5d8 \ubc0f \uc7a1\ub2e4\ud55c \uc774\uc57c\uae30\ub4e4 \uc0dd\ub7b5~**\n\n\n\n\uacf5\uc720\ud558\uae30\n\n\uac8c\uc2dc\uae00 \uad00\ub9ac\n\n_\uad6c\ub3c5\ud558\uae30_ **Ostin X**\n\n[ \uc800\uc791\uc790\ud45c\uc2dc ](https://creativecommons.org/licenses/by/4.0/deed.ko)\n\n#### '[\ub17c\ubb38 \ub9ac\ubdf0](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0) >\n[Mamba](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Mamba)' \uce74\ud14c\uace0\ub9ac\uc758 \ub2e4\ub978 \uae00\n\n[Jamba: A Hybrid Transformer-Mamba Language Model](/498)  (0) | 2024.04.01  \n---|---  \n[Zoology: Measuring and Improving Recall in Efficient Language Models](/459)\n(0) | 2024.02.28  \n[VMamba: Visual State Space Model](/396)  (0) | 2024.01.24  \n[Vision Mamba: Efficient Visual Representation Learning with Bidirectional\nState Space Model](/393)  (0) | 2024.01.22  \n[MoE-Mamba: Efficient Selective State Space Models with Mixture of\nExperts](/382)  (0) | 2024.01.15  \n[Efficiently Modeling Long Sequences with Structured State Spaces (S4)](/379)\n(0) | 2024.01.12  \n  \n## **'\ub17c\ubb38 \ub9ac\ubdf0/Mamba'** Related Articles\n\n  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdj2TCO%2FbtsDSsYnFEQ%2Fe14raSZMw3FrxqobzqSX20%2Fimg.png) VMamba: Visual State Space Model ](/396?category=1117576)\n  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FblAUR7%2FbtsDHnjEdOj%2Fm64E5WJGpwKWCzMuSOjX8k%2Fimg.png) Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model ](/393?category=1117576)\n  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FDWEAK%2FbtsDxTBbg6g%2FwxyCHXE1ZcCelz4nSARp91%2Fimg.png) MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts ](/382?category=1117576)\n  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fri9GO%2FbtsDpa4Wv4w%2FA0Nr1rBKlYMbKwx5pqWXU1%2Fimg.png) Efficiently Modeling Long Sequences with Structured State Spaces (S4) ](/379?category=1117576)\n\nSecret\n\n\ub313\uae00\n\n\ub313\uae00\ub2ec\uae30\n\n* * *\n\nDESIGN BY TISTORY [\uad00\ub9ac\uc790](https://ostin.tistory.com/manage)\n\n## \ud2f0\uc2a4\ud1a0\ub9ac\ud234\ubc14", "start_char_idx": 5928, "end_char_idx": 8091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e1664d56-4bdd-4636-b9c1-f4e36c5e525b": {"__data__": {"id_": "e1664d56-4bdd-4636-b9c1-f4e36c5e525b", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0866d40be966c52ba54298d64048331916bd0fee0cff09d6db7ddd3335a3a67a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "node_type": "1", "metadata": {}, "hash": "636db02ae79ef313109fa9afef03c10bd7298fe3e081d43c6f3783028ab6c9c1", "class_name": "RelatedNodeInfo"}}, "text": "[ ![\ubaa8\ub450\uc758\uc5f0\uad6c\uc18c](https://modulabs.co.kr/wp-content/uploads/2022/11/logo-\nsignature-4x.png) ](https://modulabs.co.kr/)\n\n  * [\uc2a4\ud1a0\ub9ac](https://modulabs.co.kr/ceo_message/)\n  * [\uc624\ub984\ud074\ub798\uc2a4](https://orm.im/)\n  * [\ub7a9](https://modulabs.co.kr/apply_lab/)\n  * [\ud480\uc78e\uc2a4\ucfe8](https://modulabs.co.kr/apply-flip/)\n  * [\ud398\uc774\ud37c\uc0f5](https://modulabs.co.kr/papershop/)\n  * [\ube14\ub85c\uadf8](/blog/)\n  * [\ubb38\uc758\ud558\uae30](https://modulabs.co.kr/contact_us/)\n  * [\ub85c\uadf8\uc778](https://modulabs.co.kr/login/)\n  * [\uc2ac\ub799 \uc785\uc7a5\ud558\uae30](https://join.slack.com/t/modulabs/shared_invite/zt-2h89rp904-EH9BNYchSqRzBR7MvG0Lqg)\n\n[ ](https://modulabs.co.kr/cart/)\n\n\ud398\uc774\uc9c0 \uc120\ud0dd\n\n\ubd80\ud2b8\ucea0\ud504\uc640 \ub2e4\ub978 AI\ud559\uad50,  \nAI\ub294 \uc544\uc774\ud3a0\uc5d0\uc11c \ubc30\uc6b0\uc138\uc694\n\n[ \uc9c0\uae08 \ubb34\ub8cc \uc9c0\uc6d0\ud558\uae30\n](https://www.aiffel.io?utm_source=modulabs&utm_medium=on_banner_all&utm_campaign=kdt_23_03&utm_content=m_tf_codingedu_introducing-\nmamba)\n\n#\uc778\uacf5\uc9c0\ub2a5\n\n# Mamba : \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \ub300\uccb4\ud560 \ucc28\uc138\ub300 \uc544\ud0a4\ud14d\ucc98\uc758 \ub4f1\uc7a5\n\n\ud2b8\ub79c\uc2a4\ud3ec\uba38\uc758 \ub300\ud56d\ub9c8\ub77c \ubcfc \uc218 \uc788\ub294 Mamba \ubaa8\ub378\uc774 \ub098\uc654\uc2b5\ub2c8\ub2e4. Mamba\ub294 State Space Model\uc744 \uae30\ubc18\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc9c4 \uc544\ud0a4\ud14d\uccd0\ub77c\n\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ube44\ub85d \uc544\uc9c1\uc740 \ubbf8\ud761\ud558\uae34 \ud558\uc9c0\ub9cc \ucd94\ud6c4\uc5d0 \ubc1c\uc804\uc758 \uc5ec\uc9c0\uac00 \ucda9\ubd84\ud788 \uc788\ub2e4\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4.\n\n2024-03-23 | \uc774\uc601\ube48\n\n![](https://modulabs.co.kr/wp-content/uploads/2024/03/mamba-\ntransformer-e1711105598320.png)\n\n## \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc758 \ub300\uc548?\n\n\ud604\uc7ac\ub294 \ub525\ub7ec\ub2dd \uc544\ud0a4\ud14d\uccd0\ub294 \uc0ac\uc2e4\uc0c1 [\ud2b8\ub79c\uc2a4\ud3ec\uba38\uc758 \uc804\uc131\uc2dc\ub300](https://modulabs.co.kr/blog/google-research-\nintro/)\ub77c\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud604\uc7ac AI\uc758 \ud2b8\ub80c\ub4dc\ub97c \uc774\ub04c\uace0 \uc788\ub294 LLM\ub3c4 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc544\ud0a4\ud14d\uccd0\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uc73c\uba70 \uadf8\ub9bc\uc744 \uc0dd\uc131\ud558\ub294\n\ub514\ud4e8\uc804\ubaa8\ub378 \ub610\ud55c \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \ucc28\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \n\ubfd0\ub9cc \uc544\ub2c8\ub77c \uc2dc\uacc4\uc5f4, \ucd94\ucc9c\uc2dc\uc2a4\ud15c\uacfc \uac19\uc740 \ub2e4\uc591\ud55c \ubd84\uc57c\uc5d0\uc11c \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc544\ud0a4\ud14d\uccd0\ub97c \uc0ac\uc6a9\ud558\uace0 \uc788\ub294 \uc0c1\ud669\uc785\ub2c8\ub2e4.  \n\uc774\ub7f0 \uc0c1\ud669\uc5d0\uc11c\ub3c4 \ub9ce\uc740 \uc5f0\uad6c\uc790\ub4e4\uc774 \ud604\uc7ac \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \ub300\uccb4\ud558\uae30 \uc704\ud55c \uc2dc\ub3c4\ub97c \uc9c4\ud589\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \n\uadf8\uc911\uc5d0\uc11c \ub300\ud45c\uc801\uc774\uace0 \uc720\ub825\ud55c \ubc29\uc2dd\uc73c\ub85c \ub5a0\uc624\ub974\uace0 \uc788\ub294 \uc544\ud0a4\ud14d\uccd0\ub294 State Space Model\uc774\uba70 \ud2b9\ud788 \uc8fc\ubaa9\ubc1b\uae30 \uc2dc\uc791\ud55c \uac83\uc740 Mamba :\nLinear-Time Sequence Modeling with Selective State Spaces \ub17c\ubb38\uacfc \ubaa8\ub378\uc774 \uacf5\uac1c\ub41c \uac83\uc774\ub77c \ubd05\ub2c8\ub2e4.\n\nMamba\ub294 \ub17c\ubb38\uacfc \ubaa8\ub378\uc774 \ub098\uc654\uc744\ub54c\ubcf4\ub2e4 [ICLR 2024\uc5d0 \ucd5c\uc885\uc801\uc73c\ub85c \ub17c\ubb38 \uac8c\uc7ac\uac00 \ub418\uc9c0 \uc54a\uc558\ub2e4\ub294\n\uc810](https://openreview.net/forum?id=AL1fq05o7H)\uc5d0\uc11c \ub354 \ud070 \uc774\uc288\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.", "start_char_idx": 0, "end_char_idx": 1630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a": {"__data__": {"id_": "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0866d40be966c52ba54298d64048331916bd0fee0cff09d6db7ddd3335a3a67a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1664d56-4bdd-4636-b9c1-f4e36c5e525b", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "6041c0654c23f6a4b03cfa61ecc59b7ab2032bfe796986e1843d6add95ca378a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "924b1243-7403-4c09-95ee-813cd598c40c", "node_type": "1", "metadata": {}, "hash": "40ff1b379a4ef2a63623e2384245299e49a847222be0dee5a04007f12cb4bbca", "class_name": "RelatedNodeInfo"}}, "text": "\uadf8\uc911\uc5d0\uc11c \ub300\ud45c\uc801\uc774\uace0 \uc720\ub825\ud55c \ubc29\uc2dd\uc73c\ub85c \ub5a0\uc624\ub974\uace0 \uc788\ub294 \uc544\ud0a4\ud14d\uccd0\ub294 State Space Model\uc774\uba70 \ud2b9\ud788 \uc8fc\ubaa9\ubc1b\uae30 \uc2dc\uc791\ud55c \uac83\uc740 Mamba :\nLinear-Time Sequence Modeling with Selective State Spaces \ub17c\ubb38\uacfc \ubaa8\ub378\uc774 \uacf5\uac1c\ub41c \uac83\uc774\ub77c \ubd05\ub2c8\ub2e4.\n\nMamba\ub294 \ub17c\ubb38\uacfc \ubaa8\ub378\uc774 \ub098\uc654\uc744\ub54c\ubcf4\ub2e4 [ICLR 2024\uc5d0 \ucd5c\uc885\uc801\uc73c\ub85c \ub17c\ubb38 \uac8c\uc7ac\uac00 \ub418\uc9c0 \uc54a\uc558\ub2e4\ub294\n\uc810](https://openreview.net/forum?id=AL1fq05o7H)\uc5d0\uc11c \ub354 \ud070 \uc774\uc288\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.  \n\ub54c\ub9c8\uce68 NeurIPS\uc5d0\uc11c 10\ub144\uc804\uc5d0 NeurIPS\uc5d0 \uac8c\uc7ac\uc2b9\uc778\ub41c \ub17c\ubb38\uc911\uc5d0\uc11c \uac00\uc7a5 \uc601\ud5a5\ub825\uc774 \uc788\ub294 \ub17c\ubb38\uc0c1\uc73c\ub85c Word2Vec\uac00 \ubf51\ud614\uc2b5\ub2c8\ub2e4.  \nWord2Vec \ub17c\ubb38\uc774 \uacf5\uad50\ub86d\uac8c\ub3c4 ICLR 2013\uc5d0\uc11c \ub124\ubc88\uc758 \uac15\ub825\uac70\uc808\uc744 \ub2f9\ud55c \uc774\ub825\uc774 \uc788\ub2e4\ubcf4\ub2c8 Mamba\uac00 \uc5f0\uc0c1\uc791\uc6a9\uc744 \ud588\ub2e4\uace0\ub3c4 \uc0dd\uac01\ud569\ub2c8\ub2e4.  \n\uadf8\ub807\ub2e4\uba74 \uc0ac\ub78c\ub4e4\uc774 \uc65c Mamba\uc5d0 \ub300\ud574 \uc5f4\uad11\ud558\ub294\uc9c0 \uadf8\ub9ac\uace0 \uc774 \uc544\ud0a4\ud14d\uccd0\uc758 \ud2b9\uc9d5\uc744 \ud558\ub098\uc529 \uc0b4\ud3b4\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n## Mamba \uc544\ud0a4\ud14d\uccd0\ub97c \ubd84\uc11d\ud574\ubcf4\uc790!\n\nMamba\uc758 \uc544\ud0a4\ud14d\uccd0\ub97c \uc774\ud574\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc6b0\uc120 State Space Model\uc774 \uc5b4\ub5bb\uac8c \ub525\ub7ec\ub2dd\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294\uc9c0\ub97c \ud30c\uc545\ud558\uace0 Mamba\uc5d0\uc11c\uc758\nSSM\uc740 \ub2e4\ub978 SSM\uacfc \uc5b4\ub5bb\uac8c \ub2e4\ub978\uc9c0 \uc124\uba85\ud560 \uc608\uc815\uc785\ub2c8\ub2e4.  \n\uc774\ud6c4\uc5d0 Mamba \ub9cc\uc774 \uac16\uace0 \uc788\ub294 Selective \uba54\ucee4\ub2c8\uc998\uacfc Selective Scan\uc5d0 \ub300\ud574 \uc774\uc57c\uae30\ud560 \uc5d0\uc815\uc774\uba70 \ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc804\uccb4\nMamba\uc758 \uc544\ud0a4\ud14d\uccd0\uc5d0 \ub300\ud574 \uc124\uba85\ud560 \uc5d0\uc815\uc785\ub2c8\ub2e4.\n\n### Mamba\uc758 \ubf08\ub300 : State Space Models\n\n    ![$$ h\\\\prime\\(x\\) = Ah\\(t\\) + Bx\\(t\\), y\\(t\\) = Ch\\(t\\) $$](https://modulabs.co.kr/wp-content/ql-cache/quicklatex.com-0fbe432483e61bc480c8855886547437_l3.png)\n\n\n\nMamba\uc758 \ubf08\ub300\ub97c \uc774\ub8e8\uace0 \uc788\ub294 \uc544\ud0a4\ud14d\uccd0\ub294 State Space Model\uc774\ub77c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \nState Space Model (SSM)\uc740 \ud754\ud788 \uc81c\uc5b4\uc774\ub860\uc5d0\uc11c \uc0ac\uc6a9\ub418\uace0 \uc788\ub294 \uc0c1\ud0dc\uacf5\uac04\ubc29\uc815\uc2dd\uacfc \ub3d9\uc77c\ud558\uac8c \uc0ac\uc6a9\ub429\ub2c8\ub2e4.  \n\ub2e4\ub9cc \uc81c\uc5b4\uc774\ub860\uc5d0\uc11c\uc758 \uc0c1\ud0dc\uacf5\uac04\ubc29\uc815\uc2dd\uc758 \uacbd\uc6b0\uc5d0\ub294 \uc5f0\uc18d\ud615 \ubcc0\uc218\ub97c \uac00\uc815\ud574\uc11c \uc0ac\uc6a9\ud558\ub294 \ubc29\uc815\uc2dd\uc778\ub370 \ub525\ub7ec\ub2dd\uc758 \uacbd\uc6b0 \ubcc0\uc218\uac00 \uc804\ubd80 \uc774\uc0b0\ud615 \ubcc0\uc218\uc774\uae30\uc5d0 \uc774\uc0b0\ud654\ub97c\n\uac70\uccd0\uc57c \ud569\ub2c8\ub2e4.  \n\uc774\ub54c \uc774\uc0b0\ud654\ub97c \uac70\uce58\ub294 \ubc29\uc2dd\uc740 \ud06c\uac8c 3\uac00\uc9c0\ub85c \uc624\uc77c\ub7ec \ubc29\uc2dd, ZOH \ubc29\uc2dd, \uc774\uc911\uc120\ud615\ubc29\uc2dd\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uac01\uc790 \ubc29\uc2dd\uc740 \uc7a5\ub2e8\uc810\uc744 \uac16\uace0 \uc788\uc9c0\ub9cc\nMamba\uc758 \uacbd\uc6b0 ZOH \ubc29\uc2dd\uc744 \ucc44\ud0dd\ud569\ub2c8\ub2e4.\n\n    ![$$ h_t = \\\\bar{A}h_{t-1}+\\\\bar{B}x_t, y_t = Ch\\(t\\), \\\\bar{A} = \\\\exp\\(\\\\Delta{A}\\), \\\\bar{B} = \\(\\\\Delta{A}\\)^{-1}\\(\\\\exp\\(\\\\Delta{A}\\)-I\\)\\\\cdot\\\\Delta{B} $$](https://modulabs.co.kr/wp-content/ql-cache/quicklatex.com-fdcbbe5bd360232e745acbe9cee09a66_l3.png)", "start_char_idx": 1355, "end_char_idx": 2865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "924b1243-7403-4c09-95ee-813cd598c40c": {"__data__": {"id_": "924b1243-7403-4c09-95ee-813cd598c40c", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0866d40be966c52ba54298d64048331916bd0fee0cff09d6db7ddd3335a3a67a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "81487674a2a423858455dd79569ac4e4589d96023535233ad08aef614b272173", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "node_type": "1", "metadata": {}, "hash": "5e75bdb0a7f12fac832984ad35b123b3735aefeb0d3d208d032bf594c1d514c2", "class_name": "RelatedNodeInfo"}}, "text": "\uc774\uc0b0\ud654\uc2dc\ud0a8 \ubaa8\ub378\uc744 \ubcf4\uac8c \ub41c\ub2e4\uba74 t\uac00 \ucee4\uc9c0\uba74 \ucee4\uc9c8\uc218\ub85d B \uc640 C\ub294 \uace0\uc815\ub41c \uc0c1\ud0dc\uc5d0\uc11c A \ub9cc \uacc4\uc18d\ud574\uc11c \uacf1\ud574\uc9c0\ub294 \uacbd\ud5a5\uc774 \ubc1c\uc0dd\ud569\ub2c8\ub2e4.  \n\uadf8\ub807\uae30\uc5d0 \uc774\ub97c \ucee4\ub110 K\ub85c \uc815\uc758\ud574\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \uae30\ubc18\uc73c\ub85c \ucee8\ubc8c\ub8e8\uc158 \ud615\uc2dd\uc73c\ub85c \uc791\uc131\ud558\uac8c \ub418\uba74 \uc544\ub798 \uc2dd\ucc98\ub7fc \ub098\ud0c0\ub0bc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n    ![$$ \\\\bar{K} = \\(C\\\\bar{B}, C\\\\bar{A}\\\\bar{B}, \\\\cdots ,C\\\\bar{A^k}\\\\bar{B},\\\\cdots\\), y = x \\\\ast\\\\bar{K} $$](https://modulabs.co.kr/wp-content/ql-cache/quicklatex.com-c8a3009e6cc9f055f90aa3f096ce8b60_l3.png)\n\n\n\n\uccab\ubc88\uc9f8 \ubc29\uc2dd\uc740 \ud558\ub098\uc529 \uc7ac\uadc0\ub85c \ub4e4\uc5b4\uac00\uba74\uc11c \ubc18\ubcf5\ub418\uae30 \ub54c\ubb38\uc5d0 RNN\uacfc \uac19\uc774 \ube44\ud6a8\uc728\uc801\uc778 \ubc29\ubc95\uc73c\ub85c \ubc18\ubcf5\ub418\uc9c0\ub9cc  \n2\ubc88\uc9f8 \ubc29\uc2dd\uc758 \uacbd\uc6b0\uc5d0\ub294 \uc7ac\uadc0 \ub300\uc2e0 \ucee8\ubc8c\ub8e8\uc158 \ud615\uc2dd\uc73c\ub85c \uacc4\uc0b0\ud558\uae30 \ub54c\ubb38\uc5d0 \ubc18\ubcf5\uc774 \uc5c6\uc744 \ubfd0\ub9cc \uc544\ub2c8\ub77c \ud6a8\uc728\uc801\uc778 \ubcd1\ub82c\ud559\uc2b5\ub3c4 \uac00\ub2a5\ud569\ub2c8\ub2e4.  \n\uadf8\ub798\uc11c \ucee8\ubc8c\ub8e8\uc158 \ud615\uc2dd\uc73c\ub85c \ubc14\uafb8\ub294 \ud615\uc2dd\uc744 \ub450\uace0 \ubcf4\ud1b5 \uc120\ud615 \uc2dc\ubd88\ubcc0(Linear Time invariance)\uc774\ub77c\uace0\ub3c4 \ubd80\ub985\ub2c8\ub2e4.  \n\ub2e4\ub9cc \ucee8\ubc8c\ub8e8\uc158\uc758 \ubc29\uc2dd\uc758 \uacbd\uc6b0 \uc2dc\ud000\uc2a4 \ucd94\ub860\uc744 \uc9c4\ud589\ud560 \ub54c \ud55c\uaebc\ubc88\uc5d0 \uacb0\uacfc\uac12\uc774 \ub098\uc624\uae30 \ub54c\ubb38\uc5d0 \uc774 \uacbd\uc6b0\uc5d0\ub294 \uccab\ubc88\uc9f8 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n### Mamba\uc758 \ud2b9\uc9d5 1 : Selective Mechanism\n\n![Mamba Selective Copying\uacfc Inductive Heads](https://modulabs.co.kr/wp-\ncontent/uploads/2024/03/Screenshot-from-2024-03-22-16-57-45.png)\n\nMamba\uc758 \uac00\uc7a5 \ub300\ud45c\uc801\uc778 \ud2b9\uc9d5\uc740 Selective Mechanism\uc774 \uc788\uc2b5\ub2c8\ub2e4.  \n\uc800\uc790\ub4e4\uc740 \uae30\uc874 SSM\uc740 \uacfc\ub3c4\ud558\uac8c \ubaa8\ub4e0 \ud1a0\ud070\ub4e4\uacfc \uacc4\uc0b0\ud558\ub294 \ubc29\uc2dd\uc744 \ucde8\ud558\uace0 \uc788\uc5c8\uae30 \ub54c\ubb38\uc5d0 \ube44\ud6a8\uc728\uc801\uc774\ub77c\uace0 \ubcf4\uc558\uc2b5\ub2c8\ub2e4.  \n\uadf8\ub798\uc11c Mamba \uc544\ud0a4\ud14d\uccd0\uc758 \uacbd\uc6b0 \u0394 \uac12\uc73c\ub85c \uc81c\uc5b4\ud560 \uc218 \uc788\ub294 \ubc18\uacbd\uc744 \uc881\ud788\ub294 \ubc29\uc2dd\uc744 \ucc44\ud0dd\ud588\uc2b5\ub2c8\ub2e4.  \n\uc774 \ubc29\uc2dd\uc744 \ud0dd\ud55c \uc774\uc720\ub294 Large Sequence Modeling\uc758 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 task\ub85c Selective Copying \ud14c\uc2a4\ud06c\uc640\nInduction Heads \ud14c\uc2a4\ud06c\uac00 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.  \nSelective Copying \ud14c\uc2a4\ud06c\ub294 \uc554\uae30\ud560 \ud1a0\ud070 \uc704\uce58\ub97c \uc120\ud0dd\uc801\uc73c\ub85c \ubcf5\uc81c\ud574\uc11c \ub9de\ucd94\ub294 \ud14c\uc2a4\ud06c\ub85c \ud1a0\ud070\uc744 \uc554\uae30\ud558\uace0 \uad00\ub828\uc5c6\ub294 \ud1a0\ud070\uc744 \uac78\ub7ec\ub0b4\ub294\n\ucf58\ud150\uce20 \uc778\uc2dd \ucd94\ub860\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.  \nInduction Heads \ud14c\uc2a4\ud06c\ub294 LLM\uc758 \ub9e5\ub77d \ub0b4 \ud559\uc2b5 \ub2a5\ub825\uc758 \ub300\ubd80\ubd84\uc744 \uc124\uba85\ud558\uae30 \uc704\ud574 \uac00\uc124\ud654\ub41c \ud14c\uc2a4\ud06c\uc774\uba70 \ubb38\ub9e5 \uc778\uc2dd \ucd94\ub860\ub2a5\ub825\uc774\n\ud544\uc218\uc801\uc785\ub2c8\ub2e4. Selective mechanism\uc744 \ud65c\uc6a9\ud558\uba74 \uc704\uc5d0 \uc788\ub294 \ud14c\uc2a4\ud06c\uc758 \uc131\ub2a5\ud5a5\uc0c1\uc744 \uae30\ub300\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n![S4\uc640 Mamba S6\ube44\uad50](https://modulabs.co.kr/wp-\ncontent/uploads/2024/03/Screenshot-from-2024-03-22-17-18-24.png)\n\n\uc774 \uae30\ubc95\uc744 \uc218\ud589\ud558\uae30 \uc704\ud574\uc11c Mamba\ub294 \uc544\ud0a4\ud14d\uccd0\ub294 S6 \ubc29\ubc95\ub860\uc744 \ucc44\ud0dd\ud569\ub2c8\ub2e4.", "start_char_idx": 2869, "end_char_idx": 4349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "acf5d30f-b0b5-4086-91c7-6e0110bc22a4": {"__data__": {"id_": "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0866d40be966c52ba54298d64048331916bd0fee0cff09d6db7ddd3335a3a67a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "924b1243-7403-4c09-95ee-813cd598c40c", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "b76b406fc85e128ce7fd832a7541cda985830a04ccfd8053c992b2ab66a19074", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "node_type": "1", "metadata": {}, "hash": "b485da41e445e65956c06f7d885fc9f8bd54f93276694bae651b3278d148e98f", "class_name": "RelatedNodeInfo"}}, "text": "Induction Heads \ud14c\uc2a4\ud06c\ub294 LLM\uc758 \ub9e5\ub77d \ub0b4 \ud559\uc2b5 \ub2a5\ub825\uc758 \ub300\ubd80\ubd84\uc744 \uc124\uba85\ud558\uae30 \uc704\ud574 \uac00\uc124\ud654\ub41c \ud14c\uc2a4\ud06c\uc774\uba70 \ubb38\ub9e5 \uc778\uc2dd \ucd94\ub860\ub2a5\ub825\uc774\n\ud544\uc218\uc801\uc785\ub2c8\ub2e4. Selective mechanism\uc744 \ud65c\uc6a9\ud558\uba74 \uc704\uc5d0 \uc788\ub294 \ud14c\uc2a4\ud06c\uc758 \uc131\ub2a5\ud5a5\uc0c1\uc744 \uae30\ub300\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n![S4\uc640 Mamba S6\ube44\uad50](https://modulabs.co.kr/wp-\ncontent/uploads/2024/03/Screenshot-from-2024-03-22-17-18-24.png)\n\n\uc774 \uae30\ubc95\uc744 \uc218\ud589\ud558\uae30 \uc704\ud574\uc11c Mamba\ub294 \uc544\ud0a4\ud14d\uccd0\ub294 S6 \ubc29\ubc95\ub860\uc744 \ucc44\ud0dd\ud569\ub2c8\ub2e4.  \n\uae30\uc874\uc5d0 \uc788\ub358 S4\uc640 S6\uc758 \ucc28\uc774\uc810\uc740 \uae30\uc874\uc5d0\ub294 \ud30c\ub77c\ubbf8\ud130\ub85c\uc368 \uc874\uc7ac\ud588\ub358 B, C\ub97c x\uc5d0 \ub300\ud55c output feature\uac00 N\uc778 \uc120\ud615 \ud568\uc218\ub85c\n\uce58\ud658\ud569\ub2c8\ub2e4.  \n\u0394\uc758 \uacbd\uc6b0 \ud30c\ub77c\ubbf8\ud130\uc5d0 x\uc5d0 \ub300\ud55c output feature 1\uc778 \uc120\ud615\ud568\uc218\ub97c \uac70\uce58\uace0 D\uc5d0 \ub530\ub77c \ube0c\ub85c\ub4dc\uce90\uc2a4\ud305\ub41c \ud568\uc218\ub97c \uae30\uc874 \ud30c\ub77c\ubbf8\ud130\uc5d0 \ub354\ud558\uace0\nsoftplus\ud568\uc218\uc5d0 \ub123\ub294 \ubc29\uc2dd\uc73c\ub85c \ubc14\uafc9\ub2c8\ub2e4.  \n\uc774\ub807\uac8c \ubc14\uafb8\uac8c \ub418\uba74 A,B \ubaa8\ub450 \ubaa8\ub450 \uc774\uc0b0\ud654\ub97c \uc9c4\ud589\ud588\uc744 \ub54c \ucc28\uc6d0\uc218\ub3c4 \ub298\uc5b4\ub098\uac8c \ub429\ub2c8\ub2e4. \uc774\uac78 \uae30\ubc18\uc73c\ub85c SSM\uc5d0 \ub123\uac8c \ub418\uba74 Selective\nCopying\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uadf8\ub7ec\ub098 Selective \uba54\ucee4\ub2c8\uc998\uc740 \uc7a5\uc810\ub9cc \uc788\ub294 \uac83\uc774 \uc544\ub2d9\ub2c8\ub2e4.  \nSelective \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud558\uba74 \u0394 \uac12\uc5d0 \uc758\ud574 \uc120\ud0dd\uc774 \ubd80\uc5ec\ub418\uba70 \uc2dc\uac04\uc2dc\ubd88\ubcc0\uc131\uc774 \uc131\ub9bd\ud558\uc9c0 \uc54a\uac8c \ub418\uace0 \uc2dc\uac04\uc5d0 \ub9de\ucdb0\uc11c \ud559\uc2b5\uc744 \uc9c4\ud589\ud574\uc57c \ud569\ub2c8\ub2e4.  \n\uadf8\ub807\uae30\uc5d0 \uc774\uc804\uc5d0 SSM\uc758 \uac15\uc810\uc774\uc5c8\ub358 \ube60\ub978 \ubcd1\ub82c\ud559\uc2b5\uc778 \ucee8\ubc8c\ub8e8\uc158 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc9c0 \ubabb\ud558\uae30 \ub54c\ubb38\uc5d0 \uc774 \ubd80\ubd84\uc5d0\uc11c \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud560 \uc5ec\uc9c0\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc800\uc790\ub4e4\uc740 \uc2dc\ud000\uc2a4 \ubaa8\ub378\uc744 \ub9cc\ub4e4\ub54c \ud6a8\uacfc\uc131\uacfc \ud6a8\uc728\uc131\uc5d0 \ub300\ud55c \ud2b8\ub808\uc774\ub4dc\uc624\ud504\ub97c \uc5b8\uae09\ud558\uba74\uc11c Selective Copying \uae30\uc220\uc758 \uc7a5\uc810\uc744 \ubcc0\ud638\ud569\ub2c8\ub2e4.  \n\uc2dc\ud000\uc2a4 \ubaa8\ub378\uc744 \uc791\uac8c \ub9cc\ub4e4\uba74 \ub9cc\ub4e4\uc218\ub85d \uc791\uc740 \uc0c1\ud0dc\ub97c \uc720\uc9c0\ud558\uae30\uc5d0 \uc18d\ub3c4\uac00 \ube60\ub974\uc9c0\ub9cc \uc2dc\ud000\uc2a4 \ubaa8\ub378\uc744 \ud06c\uac8c \ub9cc\ub4e4\uba74 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub429\ub2c8\ub2e4.  \n\uc774\ub54c \uc800\uc790\ub294 \ud6a8\uc728\uc131\uc788\uac8c \ubaa8\ub378\uc744 \uc791\uac8c \ub9cc\ub4e4\uace0 \ud6a8\uacfc\ub97c \uc720\uc9c0\ud558\ub294 \ubc29\uc2dd\uc774 Selective Copying\uae30\uc220\uc774\uba70 Mamba\uc640 \uac19\uc740 \ubaa8\ub378\uc5d0\uc11c \uc720\uc6a9\ud558\uac8c\n\uc0ac\uc6a9\ud560 \uc218 \uc788\uc74c\uc744 \uc774\uc57c\uae30\ud569\ub2c8\ub2e4.\n\n### Mamba\uc758 \ud2b9\uc9d5 2 : Selection \uba54\ucee4\ub2c8\uc998\uc758 \ud2b9\uc131\uc744 \uc54c\uc544\ubcf4\uc790!\n\n\uc774\uc804\uc5d0 \ubd24\ub358 Selection \uba54\ucee4\ub2c8\uc998\uc744 \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\uba74 RNN\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ud558\ub294 Gating Mechanism\uacfc \uc720\uc0ac\ud558\ub2e4\ub294 \uac78 \uc54c \uc218\n\uc788\uc2b5\ub2c8\ub2e4.  \n\uc6b0\uc120 \u0394\ub97c \uc785\ub825\uc5d0 \uc758\uc874\uc801\uc73c\ub85c \ubc14\uafd4 RNN\uc5d0\uc11c\uc758 gate\uc758 \uc5ed\ud560\uc744 \uc218\ud589\ud558\ub294 \uc720\uc0ac\ud55c \ud6a8\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \n\ub2e4\ub9cc RNN\uc5d0\uc11c\uc758 Gating\uacfc \ub2ec\ub9ac Selection \uba54\ucee4\ub2c8\uc998\uc740 \uc2dc\ud000\uc2a4\ub97c \ub530\ub77c \uc815\ubcf4\uc758 \ud750\ub984\uc744 \uc120\ud0dd\uc801\uc73c\ub85c \uc870\uc808\ud558\ub294 \uac83\uc5d0 \ucc28\uc774\uac00 \uc788\uc2b5\ub2c8\ub2e4.", "start_char_idx": 4058, "end_char_idx": 5350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0cc59fcd-290a-4aa9-b3c6-71300a6a5585": {"__data__": {"id_": "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0866d40be966c52ba54298d64048331916bd0fee0cff09d6db7ddd3335a3a67a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "d59ba04957ea8b38cc713a28d76d666a85db5aaa829dab61ad3ab5bf51860d6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7777771-603f-4b46-b825-f9241a116be9", "node_type": "1", "metadata": {}, "hash": "49071e4053912a05ddea7e299d8a01538735acc27db5eac57835e519616988e5", "class_name": "RelatedNodeInfo"}}, "text": "### Mamba\uc758 \ud2b9\uc9d5 2 : Selection \uba54\ucee4\ub2c8\uc998\uc758 \ud2b9\uc131\uc744 \uc54c\uc544\ubcf4\uc790!\n\n\uc774\uc804\uc5d0 \ubd24\ub358 Selection \uba54\ucee4\ub2c8\uc998\uc744 \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\uba74 RNN\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ud558\ub294 Gating Mechanism\uacfc \uc720\uc0ac\ud558\ub2e4\ub294 \uac78 \uc54c \uc218\n\uc788\uc2b5\ub2c8\ub2e4.  \n\uc6b0\uc120 \u0394\ub97c \uc785\ub825\uc5d0 \uc758\uc874\uc801\uc73c\ub85c \ubc14\uafd4 RNN\uc5d0\uc11c\uc758 gate\uc758 \uc5ed\ud560\uc744 \uc218\ud589\ud558\ub294 \uc720\uc0ac\ud55c \ud6a8\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \n\ub2e4\ub9cc RNN\uc5d0\uc11c\uc758 Gating\uacfc \ub2ec\ub9ac Selection \uba54\ucee4\ub2c8\uc998\uc740 \uc2dc\ud000\uc2a4\ub97c \ub530\ub77c \uc815\ubcf4\uc758 \ud750\ub984\uc744 \uc120\ud0dd\uc801\uc73c\ub85c \uc870\uc808\ud558\ub294 \uac83\uc5d0 \ucc28\uc774\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uadf8\ub9ac\uace0 SSM\uc5d0\uc11c\uc758 \uac01 \ud30c\ub77c\ubbf8\ud130\ub4e4\uc5d0 \ub300\ud55c \ud574\uc11d\uc744 \ucd94\uac00\ud558\uba74 \u0394\ub294 \ud604\uc7ac \uc785\ub825 \ub370\uc774\ud130\uc5d0 \uc5bc\ub9c8\ub098 \uc2e0\uacbd\uc744 \uc4f8\uc9c0 \uc544\ub2d8 \ubb34\uc2dc\ud560\uc9c0 \uacb0\uc815\ud569\ub2c8\ub2e4.  \n\ub9cc\uc57d \u0394\uac00 \ucee4\uc9c0\uba74 \uc0c1\ud0dc h\ub97c \ucd08\uae30\ud654\ud558\uace0 \uc785\ub825\uac12\uc5d0 \uc9d1\uc911\ud558\ub294 \ubc18\uba74 \ub9cc\uc77c \u0394\uac00 \uc791\ub2e4\uba74 \uc0c1\ud0dc\ub97c \uc720\uc9c0\ud558\uace0 \ud604\uc7ac \uc785\ub825\uc744 \ubb34\uc2dc\ud569\ub2c8\ub2e4.  \nA \ud30c\ub77c\ubbf8\ud130\uc758 \uacbd\uc6b0 selective \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc9c0\ub9cc A\uc758 \uacbd\uc6b0 ZOH\uae30\ubc95\uc5d0 \uc758\ud574 \u0394\uc758 \uc0c1\ud638\uc791\uc6a9\uc5d0\ub9cc \uc601\ud5a5\uc744 \ubbf8\uce69\ub2c8\ub2e4.  \n\ub2e4\ub9cc A \ud30c\ub77c\ubbf8\ud130\ub3c4 Selection \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uac8c \ubc14\uafbc\ub2e4\uba74 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uc9c0\ub9cc \uadf8\ub9cc\ud07c \uacc4\uc0b0\uc774 \ub298\uc5b4\ub098\uac8c \ub429\ub2c8\ub2e4.  \nB,C \ud30c\ub77c\ubbf8\ud130 \ub9c8\uc800\ub3c4 \uc120\ud0dd\uc801\uc73c\ub85c \uc218\uc815\ud55c\ub2e4\uba74 \uc785\ub825\uc744 \uc0c1\ud0dc\uc5d0 \ubcf4\ub0bc\uc9c0 \ud639\uc740 \uc0c1\ud0dc \ucd9c\ub825\uc5d0 \ubcf4\ub0bc\uc9c0 \uacb0\uc815\ud560 \uc218 \uc788\uac8c \ub429\ub2c8\ub2e4.\n\n### Mamba\uc758 \ud2b9\uc9d5 3 : Hardware-aware Parallel Scan Algorithm\n\n![mamba ssm](https://modulabs.co.kr/wp-content/uploads/2024/03/Screenshot-\nfrom-2024-03-22-17-51-14.png)\n\nSelection Copying\uc744 \uc0ac\uc6a9\ud558\uac8c \ub418\uba74\uc11c SSM\uc758 \ud30c\ub77c\ubbf8\ud130\ub4e4\uc774 \uc774\uc81c input\uc5d0 \uc758\uc874\ud558\uae30 \uc2dc\uc791\ud558\uac8c \ub418\uc5c8\uace0 \uacb0\uad6d \ucee8\ubc8c\ub8e8\uc158 \ud615\ud0dc\ub85c\n\uacc4\uc0b0\ud558\ub294 \uac83\uc774 \ubd88\uac00\ub2a5\ud574\uc84c\uc2b5\ub2c8\ub2e4.  \n\uadf8\ub807\ub2e4\uace0 Recurrent \ubaa8\ub4dc\ub85c \ud559\uc2b5\uc744 \uc9c4\ud589\ud558\uac8c \ub418\uba74 \uc911\uac04 Hidden State \ud06c\uae30\uac00 \ub9e4\uc6b0 \ucee4\uc9c8 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.  \n\uc800\uc790\ub4e4\uc740 \ub2e8\uc810\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574\uc11c \uc120\ud0dd\ud55c Hardware-aware Parallel Scan Algorithm\uc744 \ucc44\ud0dd\ud588\uc2b5\ub2c8\ub2e4.\n\nHardware-aware Parallel Scan Algorithm\uc740 Hidden State\ub97c \uba54\ubaa8\ub9ac\uc5d0 \uc800\uc7a5\ud558\uc9c0 \uc54a\uace0 \ubcd1\ub82c\uc801\uc73c\ub85c scan\n\uc5f0\uc0b0\uc744 \uc218\ud589\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.  \n\uc774 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uae30 \uc704\ud574\uc11c \uc800\uc790\ub4e4\uc740 kernel fusion \uae30\ubc95\uc744 \ud65c\uc6a9\ud558\uac8c \ub429\ub2c8\ub2e4.  \nkernel fusion\uc740 \uc785\ub825\uacfc \ud30c\ub77c\ubbf8\ud130\ub97c GPU HBM\uc5d0\uc11c \uc77d\uc5b4\uc624\uace0 SRAM\uc5d0 \ub85c\ub4dc\ud569\ub2c8\ub2e4.  \nSRAM\uc5d0\uc11c \uc774\uc0b0\ud654, recurrnet \uc5f0\uc0b0, \uacf1\uc148\uacfc \uac19\uc740 \uacc4\uc0b0\uc744 \uc9c4\ud589\ud558\uace0 \ucd5c\uc885 \ucd9c\ub825\ub9cc HBM\uc5d0 \ub367\uc4f0\ub294 \ubc29\uc2dd\uc73c\ub85c \uc9c4\ud589\ud569\ub2c8\ub2e4. \uc774 \ubc29\uc2dd\uc744\n\ucc44\ud0dd\ud558\uac8c \ub418\uba74 \uc0ac\uc6a9\ub7c9\uc744 \uc785\ucd9c\ub825 \ud06c\uae30 \uc218\uc900\uc73c\ub85c \uc808\uac10\ud569\ub2c8\ub2e4.\n\n\uc5ed\uc804\ud30c\ub97c \uc9c4\ud589\ud560 \ub54c\ub3c4 hidden state\ub97c \uc800\uc7a5\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 hidden state\ub97c \ub2e4\uc2dc \uacc4\uc0b0\ud574\uc57c \ud569\ub2c8\ub2e4.  \n\uc774 \ubc29\uc2dd\uc744 recomputataion\uc774\ub77c\uace0 \ubd80\ub985\ub2c8\ub2e4. Recomputation\uc740 \ub2e4\uc2dc \uacc4\uc0b0\ud558\ub294 \uac83\uc774\uae30\uc5d0 hidden state\ub97c \uc800\uc7a5\ud558\ub294\n\uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc740 \uc904\uc5b4\ub4e4\uc9c0\ub9cc \uacc4\uc0b0\ub7c9\uc774 \uc99d\uac00\ud569\ub2c8\ub2e4.  \n\uadf8\ub7fc\uc5d0\ub3c4 activation \uc800\uc7a5\uc5d0 \ud544\uc694\ud55c \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc740 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub85c \uc720\uc9c0\ub418\uae30 \ub54c\ubb38\uc5d0 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.", "start_char_idx": 5083, "end_char_idx": 6652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f7777771-603f-4b46-b825-f9241a116be9": {"__data__": {"id_": "f7777771-603f-4b46-b825-f9241a116be9", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0866d40be966c52ba54298d64048331916bd0fee0cff09d6db7ddd3335a3a67a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "c36ce3dec943ce5c8cd4726db497e3781594d915672872375769d97e44d9b3c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61e73cb9-f9b0-4942-b43e-c23414078342", "node_type": "1", "metadata": {}, "hash": "9c59c4542b260d0857df3a0b1a4e0d2ef9e0d480cb64fae7c5244f98cb0bad43", "class_name": "RelatedNodeInfo"}}, "text": "\uc774 \ubc29\uc2dd\uc744\n\ucc44\ud0dd\ud558\uac8c \ub418\uba74 \uc0ac\uc6a9\ub7c9\uc744 \uc785\ucd9c\ub825 \ud06c\uae30 \uc218\uc900\uc73c\ub85c \uc808\uac10\ud569\ub2c8\ub2e4.\n\n\uc5ed\uc804\ud30c\ub97c \uc9c4\ud589\ud560 \ub54c\ub3c4 hidden state\ub97c \uc800\uc7a5\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 hidden state\ub97c \ub2e4\uc2dc \uacc4\uc0b0\ud574\uc57c \ud569\ub2c8\ub2e4.  \n\uc774 \ubc29\uc2dd\uc744 recomputataion\uc774\ub77c\uace0 \ubd80\ub985\ub2c8\ub2e4. Recomputation\uc740 \ub2e4\uc2dc \uacc4\uc0b0\ud558\ub294 \uac83\uc774\uae30\uc5d0 hidden state\ub97c \uc800\uc7a5\ud558\ub294\n\uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc740 \uc904\uc5b4\ub4e4\uc9c0\ub9cc \uacc4\uc0b0\ub7c9\uc774 \uc99d\uac00\ud569\ub2c8\ub2e4.  \n\uadf8\ub7fc\uc5d0\ub3c4 activation \uc800\uc7a5\uc5d0 \ud544\uc694\ud55c \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc740 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub85c \uc720\uc9c0\ub418\uae30 \ub54c\ubb38\uc5d0 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.\n\n### \ud2b9\uc9d5\ub4e4\uc744 \uc804\ubd80 \uc885\ud569\ud55c Mamba\uc758 \uc544\ud0a4\ud14d\uccd0 \ubcf4\uae30!\n\n![mamba \uc544\ud0a4\ud14d\uccd0](https://modulabs.co.kr/wp-content/uploads/2024/03/Screenshot-\nfrom-2024-03-22-17-51-38.png)\n\n\uc774\uc804\uae4c\uc9c0 Mamba\uc5d0 \ub4e4\uc5b4\uac00\ub294 SSM\uc5d0 \ub300\ud574 \ubd84\uc11d\ud588\ub2e4\uba74 \uc774\uc81c Mamba\uc758 \uc544\ud0a4\ud14d\uccd0\ub97c \ubd84\uc11d\ud558\uaca0\uc2b5\ub2c8\ub2e4.  \nMamba\uc758 \uacbd\uc6b0 H3 \uc694\uc18c\uc640 \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc5d0\uc11c \uc790\uc8fc \uc0ac\uc6a9\ud558\ub294 Gated MLP\ub97c \uc11e\uc740 \ud615\uc2dd\uc73c\ub85c \uc644\uc131\ub429\ub2c8\ub2e4.  \n\uc774\ub54c \u03c3\ub294 Swish \ud65c\uc131\ud654\ud568\uc218\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uae30\uc874\uc5d0 \uc0ac\uc6a9\ud558\ub358 H3\uac00 \uc5f0\uc18d\uc801\uc778 \uc2e0\ud638\uc5d0\uc11c \uc131\ub2a5\uc774 \uad1c\ucc2e\uc558\uc9c0\ub9cc \uc5b8\uc5b4\ubaa8\ub378\uacfc \uac19\uc740 \uc774\uc0b0\ud615\n\uc2dc\ud000\uc2a4\ubaa8\ub378\uc5d0\uc11c\ub294 \uc131\ub2a5\uc774 \uc88b\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.  \n\ubc18\uba74 \uc774\ubc88\uc5d0 \uc18c\uac1c\ud55c Mamba \uc544\ud0a4\ud14d\uccd0\ub294 \ub354 \ub113\uc740 \ubc94\uc704\uc758 \ub370\uc774\ud130\ub4e4\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\uace0 \ud2b9\ud788 \uac70\ub300 \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud55c \uc2dc\ud000\uc2a4 \ubaa8\ub378\ub9c1\uc5d0 \ud0c1\uc6d4\ud558\ub2e4\uace0\n\ud569\ub2c8\ub2e4.\n\n### \uc2e4\uc81c \uc131\ub2a5 \ube44\uad50\ud558\uae30\n\n![selective copying & inductive heads](https://modulabs.co.kr/wp-\ncontent/uploads/2024/03/Screenshot-from-2024-03-22-18-55-53.png)\n\n\uc2e4\uc81c \uc131\ub2a5\uc744 \ube44\uad50\ud558\uba74 Selective Mechanism\uc744 \ub3c4\uc785\ud55c SSM\uacfc \uadf8\ub807\uc9c0 \uc54a\uc740 SSM\uc744 \ube44\uad50\ud558\uba74 S6(S4 + Selection)\ub97c\n\ub3c4\uc785\ud55c \ubc29\uc2dd\uc774 \uae30\uc874 S4\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \n\uc989 SSM\ub0b4\ubd80\uc5d0\uc11c Selective Mechanism\uc774 \ud6e8\uc52c \uc131\ub2a5\uc774 \uc88b\ub2e4\ub294\uac78 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \nInduction Heads\uc758 \uacbd\uc6b0\uc5d0\ub3c4 \uae30\uc874\uc5d0 \uba40\ud2f0\ud5e4\ub4dc \uc5b4\ud150\uc158\uc744 \uc0ac\uc6a9\ud55c \ubaa8\ub378\ub4e4\ubcf4\ub2e4\ub3c4 Mamba\uac00 \ud6e8\uc52c \uc798 \uc720\uc9c0\ud558\ub294 \uac83\uc744 \uc131\ub2a5\uc73c\ub85c \uc798\n\ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4.\n\n![mamba \uc131\ub2a5](https://modulabs.co.kr/wp-content/uploads/2024/03/Screenshot-\nfrom-2024-03-22-19-15-43.png)\n\n\uc131\ub2a5\ud3c9\uac00 \uc9c0\ud45c\ub3c4 \uc0b4\ud3b4\ubcf4\uba74 \uc0c1\ub2f9\ud788 \uc758\ubbf8 \uc788\ub294 \uc9c0\ud45c\uac00 \ub098\uc654\uc2b5\ub2c8\ub2e4.  \nMamba\uc758 \uacbd\uc6b0 RNN\uacc4\uc5f4 \ud639\uc740 GPT-2 \uacc4\uc5f4 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud588\uc744 \ub54c \uc131\ub2a5\uc774 \ud6e8\uc52c \uc88b\uc740 \ubaa8\uc2b5\uc774 \ub098\ud0c0\ub0a9\ub2c8\ub2e4.  \n\ube44\ub85d \ud604\uc7ac \uc0ac\uc6a9\ud558\uace0 \uc788\ub294 LLM\ub4e4\uc774 \ud6e8\uc52c \ud06c\uace0 \uc131\ub2a5 \ub610\ud55c \ubcf4\uc7a5\ub418\uc5b4 \uc788\ub2e4\ubcf4\ub2c8 \uc9c0\ud45c \uc790\uccb4\uac00 \ub0ae\ub2e4\uace0 \uc0dd\uac01\ud560 \uc218 \uc788\uc9c0\ub9cc..  \n\ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \ubaa8\ub378\uc774 \uc774\uc815\ub3c4 \uc131\uacfc\ub97c \uac00\uc838\uac14\ub2e4\ub294 \uac83\uc740 \uad04\ubaa9\ud560\ub9cc\ud55c \uc131\uacfc\ub77c\uace0 \ubcf4\uc5ec\uc9d1\ub2c8\ub2e4.", "start_char_idx": 6378, "end_char_idx": 7824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "61e73cb9-f9b0-4942-b43e-c23414078342": {"__data__": {"id_": "61e73cb9-f9b0-4942-b43e-c23414078342", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0866d40be966c52ba54298d64048331916bd0fee0cff09d6db7ddd3335a3a67a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7777771-603f-4b46-b825-f9241a116be9", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "6cc8092656b7f25840cb5304759ed50bcab9623a69858ba69333ca687bb1435e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "964455f0-6c97-46bd-8f85-968255a4761f", "node_type": "1", "metadata": {}, "hash": "70eb0a16128ada1f538de9117d2d22dfd8c40bcffa9eaafcb9692fe4c85e5a4d", "class_name": "RelatedNodeInfo"}}, "text": "Mamba\uc758 \uacbd\uc6b0 RNN\uacc4\uc5f4 \ud639\uc740 GPT-2 \uacc4\uc5f4 \ubaa8\ub378\ub4e4\uacfc \ube44\uad50\ud588\uc744 \ub54c \uc131\ub2a5\uc774 \ud6e8\uc52c \uc88b\uc740 \ubaa8\uc2b5\uc774 \ub098\ud0c0\ub0a9\ub2c8\ub2e4.  \n\ube44\ub85d \ud604\uc7ac \uc0ac\uc6a9\ud558\uace0 \uc788\ub294 LLM\ub4e4\uc774 \ud6e8\uc52c \ud06c\uace0 \uc131\ub2a5 \ub610\ud55c \ubcf4\uc7a5\ub418\uc5b4 \uc788\ub2e4\ubcf4\ub2c8 \uc9c0\ud45c \uc790\uccb4\uac00 \ub0ae\ub2e4\uace0 \uc0dd\uac01\ud560 \uc218 \uc788\uc9c0\ub9cc..  \n\ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \ubaa8\ub378\uc774 \uc774\uc815\ub3c4 \uc131\uacfc\ub97c \uac00\uc838\uac14\ub2e4\ub294 \uac83\uc740 \uad04\ubaa9\ud560\ub9cc\ud55c \uc131\uacfc\ub77c\uace0 \ubcf4\uc5ec\uc9d1\ub2c8\ub2e4.\n\n## \uc55e\uc73c\ub85c\uc758 \uc804\ub9dd\n\n\ud604\uc7ac \ucf54\ub12c\ub300\ud559\uad50 \uc870\uad50\uc218\uc774\uc790 Hugging Face Researcher\uc778 Alexander Rush\uac00 SSM\uad00\ub828 \uac15\uc758\ub97c \uc2dc\uc791\ud560 \ub54c\n\"\ud2b8\ub79c\uc2a4\ud3ec\uba38\ub294 \ub300\uccb4\ub420\uc218 \uc788\ub098\uc694?\" \ub77c\ub294 \uc9c8\ubb38\uc744 \ub358\uc9d1\ub2c8\ub2e4. Rush\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \ub300\ub2f5\ud569\ub2c8\ub2e4. \"\ud604\uc7ac\ub294 \ub9de\uc9c0\ub9cc\u2026 10\ub144 \ub4a4\uc5d0\ub294 \uae00\uc384\u2026?\"  \n\ud544\uc790\ub3c4 Rush\uc758 \uc758\uacac\uc5d0 \ub3d9\uc758\ud569\ub2c8\ub2e4.\ud604\uc7ac \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub294 \uc555\ub3c4\uc801\uc778 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uace0 \uc788\ub294 \ub3d9\uc2dc\uc5d0 \ub370\uc774\ud130\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\uace0 \ud559\uc2b5\ud558\ub294 \ub2a5\ub825\uc5d0\uc11c\ub9cc\ud07c\uc740\n\uc544\uc9c1\uae4c\uc9c0 \uc774\uae38 \uc218 \uc788\ub294 \uc544\ud0a4\ud14d\uccd0\uac00 \uc5c6\ub2e4\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4.  \nAI\ubc18\ub3c4\uccb4\ub97c \ub9cc\ub4dc\ub294 \ud68c\uc0ac\ub4e4\ub3c4 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc544\ud0a4\ud14d\uccd0\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc5f0\uc0b0\ud558\uae30 \uc704\ud574\uc11c \ub178\ub825\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\uadf8\ub7ec\ub098 10\ub144 \ub4a4\uc5d0\ub294 \uc5b4\ub5bb\uac8c \ubcc0\ud560\uc9c0 \ubaa8\ub985\ub2c8\ub2e4.  \n\uc5ec\ub7ec \ud6c4\ubcf4\ub4e4\uc774 \ub098\uc62c \uac83\uc774\uba70 Mamba\uc758 SSM\ub3c4 \ucda9\ubd84\ud788 \uac00\ub2a5\uc131\uc774 \uc788\ub2e4\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4. \uadf8\ub7ec\uae30\uc5d0 Mamba \ub17c\ubb38\uacfc \ucf54\ub4dc\ub97c \ubcf4\uba74\uc11c \uc774\ud574\ud558\ub294\uac78\n\ucd94\ucc9c\ud569\ub2c8\ub2e4!\n\n![](/wp-content/uploads/modulabs-mypage/profile-\nimage/2023_03_08_02_42_20_3460.png)\n\n\uc774\uc601\ube48 \ubaa8\ub450\uc758\uc5f0\uad6c\uc18c\n\n\ud83d\udda5\ufe0f \ubaa8\ub450\uc758\uc5f0\uad6c\uc18c \uc544\uc774\ud3a0 \ud37c\uc2e4\ub9ac\ud14c\uc774\ud130  \n\ud83c\udfdb\ufe0f JAX-KR \uc624\uac70\ub098\uc774\uc800  \n\ud83d\ude0e GDG SongDo \uc624\uac70\ub098\uc774\uc800\n\n[\ubaa9\ub85d\uc73c\ub85c \ub3cc\uc544\uac00\uae30](/blog) \uacf5\uc720\ud558\uae30\n\n(\uc8fc)\ubaa8\ub450\uc758\uc5f0\uad6c\uc18c | \ub300\ud45c \uae40\uc2b9\uc77c | \uc0ac\uc5c5\uc790\ub4f1\ub85d\ubc88\ud638 517-88-00184 | [\uac1c\uc778\uc815\ubcf4\ubcf4\ud638\uc815\ucc45](/privacy_policy/) |\n[FAQ](/faqfaq/) | [1:1 \ubb38\uc758](/contact_us/) |\n[\ucc44\uc6a9](https://modulabs.career.greetinghr.com/) | [\uc624\uc2dc\ub294\n\uae38](/where_is_the_modulabs/)\n\n\uc11c\uc6b8 \uac15\ub0a8\uad6c \uac15\ub0a8\ub300\ub85c 324 (\uc5ed\uc0bc\ub514\uc624\uc288\ud398\ub9ac\uc6c0) 2\uce35: \ubaa8\ub450\uc758\uc5f0\uad6c\uc18c \uac15\ub0a8\ucea0\ud37c\uc2a4  \n\uc11c\uc6b8 \uac15\ub0a8\uad6c \uc5ed\uc0bc\ub85c 156 \ud0dc\uad11\ube4c\ub529 2\uce35: \ubaa8\ub450\uc758\uc5f0\uad6c\uc18c \uc5ed\uc0bc\ucea0\ud37c\uc2a4  \n\ub300\uc804 \uc911\uad6c \uc911\uc559\ub85c 119 \ub300\uc804\ud14c\ud06c\ub178\ud30c\ud06c \ub514\uc2a4\ud14c\uc774\uc158 12\uce35 : \ubaa8\ub450\uc758\uc5f0\uad6c\uc18c \ub300\uc804\ucea0\ud37c\uc2a4\n\n\ub300\ud45c\uc804\ud654 070-7743-5882\n\n[![](https://modulabs.co.kr/wp-\ncontent/uploads/2021/03/sns_facebook.png)](https://www.facebook.com/lab4all/)\n\n[!", "start_char_idx": 7641, "end_char_idx": 8912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "964455f0-6c97-46bd-8f85-968255a4761f": {"__data__": {"id_": "964455f0-6c97-46bd-8f85-968255a4761f", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0866d40be966c52ba54298d64048331916bd0fee0cff09d6db7ddd3335a3a67a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61e73cb9-f9b0-4942-b43e-c23414078342", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "8776ca135f8445608afe6c67c0e09ec4b9d1fc63e094a024a01fe48cc144723d", "class_name": "RelatedNodeInfo"}}, "text": "[](https://modulabs.co.kr/wp-\ncontent/uploads/2021/03/sns_facebook.png)](https://www.facebook.com/lab4all/)\n\n[![](https://modulabs.co.kr/wp-\ncontent/uploads/2021/09/instagram.png)](https://www.instagram.com/modulabs_/)\n\n[![](https://modulabs.co.kr/wp-\ncontent/uploads/2021/03/sns_youtube.png)](https://www.youtube.com/channel/UCv4U1uZvuxopMj8xiPXIBMQ)\n\n[![](https://modulabs.co.kr/wp-\ncontent/uploads/2024/04/icon_naverblog-1.png)](https://blog.naver.com/modulabs_official)\n\n[![](https://modulabs.co.kr/wp-\ncontent/uploads/2022/04/medium-1.png)](https://medium.com/modulabs)\n\n[ __](javascript:void\\(0\\);)", "start_char_idx": 8801, "end_char_idx": 9405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4ec08d13-d259-4320-ac14-c082c54fd81d": {"__data__": {"id_": "4ec08d13-d259-4320-ac14-c082c54fd81d", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "node_type": "1", "metadata": {}, "hash": "8a8738d43b46d0d48288215da28651cf5eaccfb157729d17ca87add6a3b329a5", "class_name": "RelatedNodeInfo"}}, "text": "[ **Welcome to JunYoung's blog** ](https://junia3.github.io/)\n\n[HOME](https://junia3.github.io/) [ABOUT](https://junia3.github.io/#about)\n[CONTACT](https://junia3.github.io/#contact) [CV](https://junia3.github.io/cv)\n[PROJECT](https://junia3.github.io/project)\n[POST](https://junia3.github.io/blog)\n\nCATEGORY\n\n[ DEEP LEARNING ](https://junia3.github.io/category/deep%20learning) [\nDEVELOPMENT ](https://junia3.github.io/category/development) [ GITHUB BLOG\n](https://junia3.github.io/category/github%20blog) [ PAPER REVIEW\n](https://junia3.github.io/category/paper%20review)\n\n[\n\n# Mamba modeling\uc758 \uae30\ucd08 (1) - Linear State-Space Layer (LSSL)\uc5d0 \ub300\ud558\uc5ec\n\n](/blog/lssl)\n\n* * *\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/217759c6-e7d4-4bd7-9cfa-\nfcf2dd0a22d9)\n\nMamba, Linear State-Space Layer\n\nPublished on **February 01, 2024** by [**_JunYoung_**\n](https://github.com/junia3)\n\n__ Mamba LSSL HiPPO\n\n __**21 min** READ\n\n# \uc5f0\uc18d \ub370\uc774\ud130 \uad6c\uc870\uc5d0 \ub300\ud55c DNN\uc758 \ubc1c\uc804\n\nSequential\ud55c \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud574 \ub525\ub7ec\ub2dd \ubaa8\ub378\uc740 \uc218\ub9ce\uc740 \ubcc0\ud654\uc640 \ubc1c\uc804\uc744 \uc774\ub8e8\uc5c8\ub2e4. \uadf8 \uc911 \uc694\uc998 \ub300\ud45c\uc801\uc73c\ub85c **LLM** \ubc0f\n**multimodal** \uc5f0\uad6c\uc5d0\uc11c \ud65c\ubc1c\ud558\uac8c \ud65c\uc6a9\ub418\ub294 \uac83\uc740 _Transformer \uad6c\uc870_ \uc774\uc9c0\ub9cc, \uadf8 \uc774\uc804\uc5d0\ub294 **LSTM** \uc774\ub098\n**GRU** \uac19\uc774 Long term(\uac70\ub9ac\uac00 \uba3c \ubb38\ub9e5 \uac04\uc758 \uad00\uacc4\uc131 \ud30c\uc545) \ubaa8\ub4c8\uacfc \ud568\uaed8 \uc5f0\uad6c\ub41c Recurrent Neural Network\n(RNN), \uadf8\ub9ac\uace0 \uac00\uc7a5 \ubca0\uc774\uc9c1\ud55c DNN \uad6c\uc870\uc778 CNN(Convolutional Neural Network)\ub97c temporal\ndataset\uc5d0 \uc801\uc808\ud558\uac8c \ubcc0\ud615\uc2dc\ucf1c\uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc774 \uc788\uc5c8\ub2e4(\uc608\ucee8\ub370, \ube44\ub514\uc624 \ub370\uc774\ud130\uc14b\uc5d0\ub294 temporal information \uac04\uc758 \uc815\ubcf4\ub3c4\n\uc0ac\uc6a9\ud558\uae30 \uc704\ud574 \uc2dc\uac04 \ucd95\uc744 \ucd94\uac00\ud55c 3D convolution\uc744 \uc0ac\uc6a9\ud558\uc600\ub2e4).\n\n\uc774\uc678\uc758 \ubc29\ubc95\uc73c\ub85c\ub294 \uc2e0\uacbd\ub9dd \uc790\uccb4\uc758 \ubc1c\uc804\uc73c\ub85c\ub294 \uc720\uba85\ud558\uc9c0\ub294 \uc54a\uc9c0\ub9cc _\ubcf4\ub2e4 \ubcf5\uc7a1\ud55c continuous data\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud574_ neural\ndifferential equations (NDEs)\ub97c \uc9c1\uc811 \ubaa8\ub378\ub9c1\ud558\ub294 \ubc29\ubc95\uc774 \uc8fc\ub85c \uc0ac\uc6a9\ub418\uc5c8\ub2e4.\n\n\ud558\uc9c0\ub9cc \ubaa8\ub4e0 \ub124\ud2b8\uc6cc\ud06c\ub294 **\ub098\ub984\uc758 \uc7a5\ub2e8\uc810\uc774 \ud655\uc2e4** \ud588\ub2e4. RNN (Recurrent Neural Network)\uc740 \ubaa8\ub450\uac00 \uc54c\ub2e4\uc2dc\ud53c\nLong-term module\uc758 \ubc1c\uc804\uc774 \uc788\uc5c8\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \uc5ec\uc804\ud788 \uae34 \ubb38\ub9e5\uc744 \ucc98\ub9ac\ud558\ub294\ub370 \uc5f0\uc0b0\ub7c9\uc774\ub098 \uc2dc\uac04\uc774 \ube44\ub840\ud574\uc11c \uc99d\uac00\ud55c\ub2e4\ub294 \ubb38\uc81c\uc810\uc774\n\uc788\uc5c8\uc73c\uba70, \ub610\ud55c Long-term \ubaa8\ub4c8\uc5d0 \uc758\uc874\ud558\uae30\uc5d0 \ubcf5\uc7a1\ud55c \ub370\uc774\ud130\uc5d0\uc11c\uc758 \ubb38\ub9e5 \ud30c\uc545\uc744 \ud559\uc2b5\uc2dc\ud0a4\uae30 \uc5b4\ub835\ub2e4\ub294 \uadfc\ubcf8\uc801\uc778 \ubb38\uc81c\uac00 \uc788\uc5c8\ub2e4.\n\n\ub300\uccb4\ub85c _gradient vanishing problem_ \uc774\ub098 _gradient exploding problem_ \uc740 continual\nlearning\uc5d0\uc11c\uc640 \ub354\ubd88\uc5b4 RNN\uacfc \uac19\uc740 \uc5f0\uc18d \ub370\uc774\ud130\ub97c \ud559\uc2b5\ud568\uc5d0 \uc788\uc5b4 catastrophic forgetting\uc758 \uc8fc\ub41c \uc774\uc720\ub85c \ub4f1\uc7a5\ud558\uae30\ub3c4\n\ud588\ub2e4.\n\nCNN(Convolutional Neural Network)\ub294 local\ud55c \uc815\ubcf4\uc5d0 \ub300\ud574 (\uc11c\ub85c \ucc28\uc6d0\uc774 \ubd99\uc5b4\uc788\ub294 \ud2b9\uc9d5) \ucd5c\uc801\ud654\uac00 \ube60\ub974\ub2e4\ub294\n\uc7a5\uc810\uc774 \uc788\uc73c\uba70, \uc5b4\ub290 \uc815\ub3c4 \ubb38\ub9e5\uc774 \uba85\ud655\ud55c \ube44\ub514\uc624 \ub370\uc774\ud130\uc14b\uc774\ub77c\ub358\uc9c0, \uc774\ubbf8\uc9c0\uc640 \uac19\uc740 object centric/semantic centric\n\ub370\uc774\ud130\uc5d0 \ub300\ud574 inductive bias\ub97c \uac00\uc9c4\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\uc5c8\ub2e4.", "start_char_idx": 0, "end_char_idx": 2063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "46e23f78-fbb3-42c8-805e-ea02f869ba8d": {"__data__": {"id_": "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ec08d13-d259-4320-ac14-c082c54fd81d", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "9547d067b91240424a7d294c6271490fef500d77333c745dc308b784dec67257", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "node_type": "1", "metadata": {}, "hash": "5e0689dc86ab8cc1ec6939747a1368253c4e919ef2c3ad3869ea7f568a5f5d31", "class_name": "RelatedNodeInfo"}}, "text": "\ub300\uccb4\ub85c _gradient vanishing problem_ \uc774\ub098 _gradient exploding problem_ \uc740 continual\nlearning\uc5d0\uc11c\uc640 \ub354\ubd88\uc5b4 RNN\uacfc \uac19\uc740 \uc5f0\uc18d \ub370\uc774\ud130\ub97c \ud559\uc2b5\ud568\uc5d0 \uc788\uc5b4 catastrophic forgetting\uc758 \uc8fc\ub41c \uc774\uc720\ub85c \ub4f1\uc7a5\ud558\uae30\ub3c4\n\ud588\ub2e4.\n\nCNN(Convolutional Neural Network)\ub294 local\ud55c \uc815\ubcf4\uc5d0 \ub300\ud574 (\uc11c\ub85c \ucc28\uc6d0\uc774 \ubd99\uc5b4\uc788\ub294 \ud2b9\uc9d5) \ucd5c\uc801\ud654\uac00 \ube60\ub974\ub2e4\ub294\n\uc7a5\uc810\uc774 \uc788\uc73c\uba70, \uc5b4\ub290 \uc815\ub3c4 \ubb38\ub9e5\uc774 \uba85\ud655\ud55c \ube44\ub514\uc624 \ub370\uc774\ud130\uc14b\uc774\ub77c\ub358\uc9c0, \uc774\ubbf8\uc9c0\uc640 \uac19\uc740 object centric/semantic centric\n\ub370\uc774\ud130\uc5d0 \ub300\ud574 inductive bias\ub97c \uac00\uc9c4\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\uc5c8\ub2e4. \ud558\uc9c0\ub9cc \uc5f0\uc0b0 \uc790\uccb4\uac00 sequence\uc5d0 \ub300\uc751\ud560 \uc218 \uc788\ub294 \uad6c\uc870\uac00 \uc544\ub2c8\ub2e4\ubcf4\ub2c8,\n\uae38\uc774\uac00 \uae38\uc5b4\uc9c8\uc218\ub85d RNN\uacfc \uac19\uc740 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud558\uc600\ub2e4. \uacb0\uad6d convolution \uc5f0\uc0b0 \ub610\ud55c _\uc815\ud574\uc9c4 context \ub0b4\uc5d0\uc11c\uc758 local\ninformation\ub9cc \ubf51\uc544\ub0b4\ub294 \uad6c\uc870_ \ub2e4 \ubcf4\ub2c8, context length\uc5d0 \ub530\ub77c \uc5f0\uc0b0\ub7c9\uc774\ub098 \uc2dc\uac04\uc774 \ube44\ub840\ud55c\ub2e4\ub294 \ubb38\uc81c\ub294 \ub611\uac19\uc774 \uc0dd\uae30\uac8c\n\ub418\uc5c8\ub2e4.\n\nNDE (Neural Differential Equation) \ubaa8\ub378\ub9c1\uc740 \ud2b9\uc815 modality\ub098 \uc815\ud574\uc9c4 \ubb38\uc81c\ub97c \uc218\ud559 \ubaa8\ub378\ub9c1\uc744 \ud1b5\ud574\n\uc774\ub860\ud654\ud588\uc9c0\ub9cc, \uadf8\ub9ac \ud6a8\uc728\uc801\uc774\uc9c0 \uc54a\ub2e4\ub294 \ubb38\uc81c\uac00 \uc788\ub2e4. \ub300\ud45c\uc801\uc73c\ub85c\ub294 diffusion modeling\uc744 \uc0dd\uac01\ud574\ubcfc \uc218 \uc788\ub294\ub370, \uc0dd\uc131 \ubaa8\ub378\uc778\ndiffusion\uc744 \uc774\ub7f0 \ud6a8\uc728\uc758 \ubb38\uc81c\ub97c score function\uc758 \uc774\uc0b0\ud654\ub85c \ud574\uacb0\ud588\ub2e4. Implicit model\uc758 \ubd80\ub2f4\uc744 \uc904\uc5ec\uc8fc\uc5b4\uc11c \uac04\ub2e8\ud55c\nU-Net \uad6c\uc870\ub97c \uc0ac\uc6a9\ud588\uace0, consistency modeling\uacfc \uac19\uc774 \ub610\ub2e4\ub978 implicit mapping\uc744 \ud1b5\ud574 \ud574\uacb0\ud560 \uc218 \uc788\uc5c8\uc9c0\ub9cc,\n\uc774\ub294 \uac01 \uad6c\uac04\uc5d0\uc11c\uc758 \ubbf8\ubd84 \ubc29\uc815\uc2dd solution\uc744 numerical\ud558\uac8c \uad6c\ud560 \uc218 \uc788\uc5c8\uae30 \ub54c\ubb38\uc774\uc5c8\uace0 \ubaa8\ub4e0 \ud615\ud0dc\uc758 \ubbf8\ubd84 \ubc29\uc815\uc2dd\uc5d0\uc11c **\uc77c\uad04\uc801**\n\uc73c\ub85c \uc2e0\uacbd\ub9dd\uc774 _\ud6a8\uc728\uc801\uc73c\ub85c \ud559\uc2b5\ub420 \uc218 \uc788\ub294 \uad6c\uc870\ub97c \ucc3e\ub294 \uac83_ \uc740 \ubd88\uac00\ub2a5\ud558\ub2e4.\n\n\uacb0\uad6d \uac00\uc7a5 \uc774\uc0c1\uc801\uc778 \ubaa8\ub378 \uad6c\uc870\uc758 \ubc1c\uc804 \ubc29\ud5a5\uc740\n\n  * Convolution\uacfc \uac19\uc774 \ubcd1\ub82c\ud654 \uc5f0\uc0b0\uc774 \uac00\ub2a5\ud55c \uad6c\uc870\uc5ec\uc57c \ud6a8\uc728\uc801\uc77c \uc218 \uc788\uc74c.\n  * Recurrence \ud615\ud0dc\uc758 \uc0c1\ud0dc \ucd94\ub860\uacfc\uc815\uc744 \ud1b5\ud55c \ubb38\ub9e5 \ucc98\ub9ac\uac00 \ub418\uc5b4\uc57c\ud568.\n  * Differential equation\uacfc \uac19\uc774 \uc774\uc0b0\ud654\ub41c \uc2e0\ud638\uac00 \uc544\ub2cc time-scale\uc5d0 \uc801\uc6a9 \uac00\ub2a5\ud574\uc57c\ud568.\n\n\ub85c \uc694\uc57d\ud560 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c \ubaa8\ub378\ub9c1\uc744 \ucc3e\uae30 \uc704\ud574 \ub04a\uc784\uc5c6\ub294 \uc2dc\ub3c4\uac00 \uc788\uc5c8\ub2e4.\n\n* * *\n\n# \uc5ec\ub7ec \ubaa8\ub378\ub9c1 \ubc29\ubc95\ub4e4 \uc18c\uac1c\n\n### CKConv\n\n\uadf8 \uc911 \ud558\ub098\uc778 [CKConv(Continuous Kernel\nConvolution)](https://arxiv.org/abs/2102.02611)\ub294 **\ucf58\ubcfc\ub8e8\uc158 \ucee4\ub110** \uc744 \uc77c\uc885\uc758 vector\ncontinuous function $\\psi : \\mathbb{R} \\rightarrow \\mathbb{R}^{N_{out} \\times\nN_{in}}$ \uc73c\ub85c \ubcf4\ub294 \ubc29\uc2dd\uc774\ub2e4. \uc774\ub54c \uc5f0\uc18d \ud568\uc218 $\\psi$\ub294 \uc791\uc740 \uc2e0\uacbd\ub9dd MLP\ub85c parameterize\ud558\uc5ec \ud559\uc2b5\uc2dc\ud0a4\uac8c \ub418\ub294\ub370,\nMLP\ub294 value\ub85c time-step\uc744 **\uc2a4\uce7c\ub77c \uac12** \uc73c\ub85c \ubc1b\uc544 \ud574\ub2f9 position\uc5d0\uc11c\uc758 _convolution kernel\uc744 \ubca1\ud130\ub85c\n\ub0b4\ubcf4\ub0b4\ub294 \ud615\uc2dd_ \uc774 \ub41c\ub2e4.\n\n!", "start_char_idx": 1713, "end_char_idx": 3306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad": {"__data__": {"id_": "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "8a0e01e570391253daf319242991ed40984711d710c62cfd068a367b28a30ab5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f84033da-2946-4cb7-ab76-d68300f1d759", "node_type": "1", "metadata": {}, "hash": "fb3ba5ea7dde256116294abf76211fe15769fdd33810e736b7eed4be2ff6f9ad", "class_name": "RelatedNodeInfo"}}, "text": "\uc774\ub54c \uc5f0\uc18d \ud568\uc218 $\\psi$\ub294 \uc791\uc740 \uc2e0\uacbd\ub9dd MLP\ub85c parameterize\ud558\uc5ec \ud559\uc2b5\uc2dc\ud0a4\uac8c \ub418\ub294\ub370,\nMLP\ub294 value\ub85c time-step\uc744 **\uc2a4\uce7c\ub77c \uac12** \uc73c\ub85c \ubc1b\uc544 \ud574\ub2f9 position\uc5d0\uc11c\uc758 _convolution kernel\uc744 \ubca1\ud130\ub85c\n\ub0b4\ubcf4\ub0b4\ub294 \ud615\uc2dd_ \uc774 \ub41c\ub2e4.\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/b01b6b5a-c037-4918-8185-13f272403618)\n\n### UnICORNN\n\nRNN \uacc4\uc5f4\uc5d0\uc11c ODE \uae30\ubc18\uc758 \ubaa8\ub378\ub9c1 (time-scaling\uc744 \ud1b5\ud55c long-time dependency \ud655\ubcf4)\uc5d0\uc11c\ub294\n[UnICORNN](UnICORNN)\uacfc \uac19\uc740 \uc5f0\uad6c\uac00 \uc9c4\ud589\ub418\uae30\ub3c4 \ud558\uc600\ub2e4. \uac04\ub2e8\ud558\uac8c \ubc29\ubc95\ub9cc \uc18c\uac1c\ud558\uba74 \ud574\ub2f9 RNN\uc740 2\ucc28 ODE(\uc77c\ubc18 \ubbf8\ubc29)\uc744\n\uc2dc\uac04 \ucd95\uc73c\ub85c \uc774\uc0b0\ud654 (discretization)\ud560 \uc218 \uc788\ub294 \uc624\uc77c\ub7ec \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud55c\ub2e4.\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/ac666a29-b8d4-446d-a1ef-3ef782ba7c0f)\n\n\uc704\uc758 \uadf8\ub9bc\uc5d0\uc11c \ub098\uc640\uc788\ub294 $y$\uac00 \uc5bb\uace0\uc790 \ud558\ub294 \ud568\uc218\uc774\uace0 $z$\ub294 \uc5bb\uace0\uc790 \ud558\ub294 \ud568\uc218\uc758 1\ucc28 \ubbf8\ubd84 \ud568\uc218\uc5d0 \ud574\ub2f9\ub41c\ub2e4. 2\ucc28 ODE\ub97c \uc9c1\uc811 \ud480\uc5b4\uc11c\n_\uc6d0\ud558\ub294 \ud568\uc218\ub97c \uc5bb\uae30\uac00 \ud798\ub4e4\uae30 \ub54c\ubb38_ \uc5d0 $y$\uc758 1\ucc28 \ubbf8\ubd84 \ud568\uc218\uc778 $y^\\prime$\uc744 $z$\ub77c\ub294 \uc784\uc2dc \ubcc0\uc218\ub85c \uc120\uc5b8\ud568\uc73c\ub85c\uc368 2\ucc28 \ubbf8\ubd84\nODE\ub97c $z, y$ \uac04\uc758 1\ucc28 \ubbf8\ubc29\uc73c\ub85c \ubc14\uafc0 \uc218 \uc788\ub2e4.\n\n\uc774\ub807\uac8c \ubcc0\uacbd\ub41c ODE \uc2dc\uc2a4\ud15c\uc744 **\u201cHamiltonian system\u201d** \uc774\ub77c\uace0 \ubd80\ub978\ub2e4. \uc774 Hamiltonian system\uc744 \ud480\uc5b4\ub0b4\ub294\n\uacfc\uc815\uc5d0\uc11c \uc2dc\uac04\ubcc4 input\uc5d0 \uc758\uc874\ud558\ub294 \uc5f0\uc18d \ud568\uc218\uac00 \uad6c\ud604\uc774 \ub418\uace0,\n\n[ H(y, z, t) = \\frac{\\alpha}{2} \\parallel y \\parallel^2 + \\frac{1}{2}\\parallel\nz \\parallel^2 + \\sum_{i=1}^m\\frac{1}{w_i} \\log (\\cosh (w_iy_i + (Vu(t))_i +\nb_i)) ]\n\n\uac01 \ubca1\ud130 $y, z$\uc758 \uc720\ud074\ub9ac\ub514\uc548 norm \uc5f0\uc0b0\uc778 $\\parallel \\cdot \\parallel$ \uc744 \ud1b5\ud574 \uc704\uc640 \uac19\uc774 \uc815\ub9ac\ub41c\ub2e4. \uc774 \uc5f0\uc18d\n\uc2e0\ud638 \ubbf8\ubd84 \ubc29\uc815\uc2dd\uc744 \uc624\uc77c\ub7ec \uba54\uc18c\ub4dc\ub97c \ud1b5\ud574 \uc774\uc0b0\ud654\ud558\uba74 \uc5bb\uace0\uc790 \ud558\ub294 discrete dynamical system\uc774 \ucd94\ucd9c\ub41c\ub2e4.\n\n### LMU\n\n[Parallelizing Legendre Memory Unit Training\n(LMU)](https://arxiv.org/abs/2102.11417) \uc5d0\uc11c\ub294 RNN\uc758 \ub2e8\uc810 \uc911 \ud558\ub098\uc778 \ubcd1\ub82c\ud654 \ubd88\uac00\ub2a5 \ubb38\uc81c\ub97c linear\nrecurrence convolution\uc73c\ub85c \ud574\uacb0\ud558\ub294 \uc2dc\ub3c4\ub97c \ubcf4\uc600\ub2e4. \ub9cc\uc57d \uc6b0\ub9ac\uac00 \ud2b9\uc815 input\uc758 \uc774\uc804/\uc774\ud6c4 state\ub97c \uac00\uc838\uc62c \uc218 \uc788\ub294\n\ub51c\ub808\uc774 \uad6c\uc870\uc758 \uc2dc\uc2a4\ud15c\uc744 \uad6c\ucd95\ud560 \uc218 \uc788\ub2e4\uba74, \ud574\ub2f9 \uc2dc\uc2a4\ud15c\uc758 output\uc73c\ub85c input\uc758 recurrence \uad6c\uc870\ub97c \ud655\ubcf4\ud560 \uc218 \uc788\ub2e4\ub294 \uc7a5\uc810\uc774\n\uc0dd\uae34\ub2e4. \uc6b0\ub9ac\ub294 Linear system\uc744 \ucc3e\uace0\uc790 \ud558\uae30 \ub54c\ubb38\uc5d0 (\uc560\ucd08\uc5d0 \ud559\uc2b5\ud558\uace0\uc790 \ud558\ub294 \uc2e0\uacbd\ub9dd \uc5f0\uc0b0 \uc790\uccb4\uac00 \ud150\uc11c \ubc0f \ud589\ub82c \uae30\ubc18\uc774\uae30 \ub54c\ubb38\uc774\ub77c\n\uc0dd\uac01\ud558\uba74 \ud3b8\ud558\ub2e4), \ub2e4\uc74c\uacfc \uac19\uc774 \ub124 \uac1c\uc758 matrices $<A, B, C, D>$ \ub85c \ud45c\ud604\ub418\ub294 **LTI system\uc744 \ucc3e\ub294 \uac83** \uc774\n\ubaa9\ud45c\uac00 \ub41c\ub2e4.", "start_char_idx": 3155, "end_char_idx": 4771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f84033da-2946-4cb7-ab76-d68300f1d759": {"__data__": {"id_": "f84033da-2946-4cb7-ab76-d68300f1d759", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "682bcdf4917f31639571d0ae1e7b1290a2c985bf9286ff077f650eeba9a5de0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "node_type": "1", "metadata": {}, "hash": "b0b79e63ff36f1762ae8b2f4a0be1f80c23a636c515efc53355c3274438e8686", "class_name": "RelatedNodeInfo"}}, "text": "\ub9cc\uc57d \uc6b0\ub9ac\uac00 \ud2b9\uc815 input\uc758 \uc774\uc804/\uc774\ud6c4 state\ub97c \uac00\uc838\uc62c \uc218 \uc788\ub294\n\ub51c\ub808\uc774 \uad6c\uc870\uc758 \uc2dc\uc2a4\ud15c\uc744 \uad6c\ucd95\ud560 \uc218 \uc788\ub2e4\uba74, \ud574\ub2f9 \uc2dc\uc2a4\ud15c\uc758 output\uc73c\ub85c input\uc758 recurrence \uad6c\uc870\ub97c \ud655\ubcf4\ud560 \uc218 \uc788\ub2e4\ub294 \uc7a5\uc810\uc774\n\uc0dd\uae34\ub2e4. \uc6b0\ub9ac\ub294 Linear system\uc744 \ucc3e\uace0\uc790 \ud558\uae30 \ub54c\ubb38\uc5d0 (\uc560\ucd08\uc5d0 \ud559\uc2b5\ud558\uace0\uc790 \ud558\ub294 \uc2e0\uacbd\ub9dd \uc5f0\uc0b0 \uc790\uccb4\uac00 \ud150\uc11c \ubc0f \ud589\ub82c \uae30\ubc18\uc774\uae30 \ub54c\ubb38\uc774\ub77c\n\uc0dd\uac01\ud558\uba74 \ud3b8\ud558\ub2e4), \ub2e4\uc74c\uacfc \uac19\uc774 \ub124 \uac1c\uc758 matrices $<A, B, C, D>$ \ub85c \ud45c\ud604\ub418\ub294 **LTI system\uc744 \ucc3e\ub294 \uac83** \uc774\n\ubaa9\ud45c\uac00 \ub41c\ub2e4.\n\n[ \\begin{aligned} \\dot{m} =& Am + Bu \\newline y =& Cm + Du \\end{aligned} ]\n\n\uadf8\ub9ac\uace0 I/O \uc758 \ub77c\ud50c\ub77c\uc2a4 \ubcc0\ud658 \ud615\ud0dc\uc778 $u(s), y(s)$\ub85c SISO system\uc758 transfer function $G(s)$\ub97c \uc815\uc758\ud560\n\uc218 \uc788\uac8c \ub41c\ub2e4. \ud558\uc9c0\ub9cc \ud574\ub2f9 **transfer function** \uc774 \ub0b4\ud3ec\ud558\ub294 \uc5b4\ub824\uc6c0\uc740 infinite dimensional\ud558\uba70,\ncontinous delay $\\theta$\ub97c \ubaa8\ub450 \ucee4\ubc84\uce58\uae30 \ubd88\uac00\ub2a5\ud558\ub2e4\ub294 \ubb38\uc81c\uac00 \uc0dd\uae34\ub2e4.\n\n[ G(s) = \\frac{y(s)}{u(s)} = e^{-\\theta s} ]\n\n\uc774\uc81c finite\ud558\uace0 causal\ud55c state space realization \ucc28\uc6d0\uc73c\ub85c \uac00\uc838\uc624\uae30 \uc704\ud574\uc11c\ub294 transfer function\n$G(s)$\ub97c $s$\uc5d0 \ub300\ud55c polynomial\ub85c \uad6c\uc131\uc744 \ud574\uc57c\ud55c\ub2e4. \ubcf4\ud1b5 transfer function\uc740 \ubd84\uc790\uc640 \ubd84\ubaa8\uac00 \uac01\uac01 \ud2b9\uc815 \ucc28\uc218\ub97c\n\uac00\uc9c0\ub294 $s$\uc758 \ub2e4\ud56d\uc2dd\uc73c\ub85c \ud45c\ud604\ub418\ub294\ub370, proper \ud55c dimension\uc744 \uac00\uc9c0\ub294 \uc2dc\uc2a4\ud15c\uc740 \ubd84\ubaa8\uc758 \ucc28\uc218\uac00 \ub354 \ub192\uc544\uc57c\ud55c\ub2e4(\uadf8\ub798\uc57c \uc2dc\uc2a4\ud15c\uc758\nconvergence\ub97c \ubcf4\uc7a5\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4). \uc544\ubb34\ud2bc \uc704\uc5d0 \uc788\ub294 \uc800 \uc2dd\uc744 approximation \ud574\uc57c\ud55c\ub2e4\ub294 \uacb0\ub860\uc5d0 \ub2e4\ub2e4\ub974\uac8c \ub41c\ub2e4.\n\uc774\ub97c Linear system\uc5d0\uc11c \uad6c\ud604\ud558\uae30 \uc704\ud574\uc11c \uc55e\uc11c \ud655\uc778\ud588\ub358 \uac83\ucc98\ub7fc matrices\ub97c \uad6c\ud574\uc57c\ud558\uace0, $i,~j \\in [0,d-1]$ \uc5d0\n\ub300\ud574\uc11c **\ub2e4\uc74c\uc774 \uc131\ub9bd\ud558\ub294 \ud589\ub82c \uc694\uc18c** \ub97c \uc0ac\uc6a9\ud558\uac8c \ub41c\ub2e4.\n\n\ub514\ud14c\uc77c\ud55c \ub0b4\uc6a9\uc774\ub098 \uc99d\uba85 \uacfc\uc815\uc740 \ud574\ub2f9 \ud398\uc774\ud37c\uc758 \uc774\uc804 \ub17c\ubb38\uc778\n[LMU](https://papers.nips.cc/paper_files/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf)\ub97c\n\ubcf4\uac70\ub098 \uc544\ub798\uc5d0 \uc788\ub294 \uc99d\uba85 \uacfc\uc815\uc744 \ubcf4\uba74 \ub41c\ub2e4.\n\n[ \\begin{aligned} A_{i,j} =& \\frac{(2i+1)}{\\theta}\\begin{cases} -1 & i < j\n\\newline (-1)^{i-j+1} & i \\ge j \\end{cases}\\newline B_i =&\n\\frac{(2i+1)(-1)^i}{\\theta} \\newline C_i =& (-1)^i \\sum_{l=0}^i {i \\choose\nl}{i+l \\choose j}(-1)^l \\newline D =& 0 \\end{aligned} ]\n\n\ud574\ub2f9 \ub9e4\ud2b8\ub9ad\uc2a4\ub4e4 \uc911 \uc138\ubc88\uc9f8 matrix\uc778 $C$\uac00 _\uac00\uc7a5 \uc8fc\uc694 \uc544\uc774\ub514\uc5b4_ \uc5d0 \ud574\ub2f9\ub41c\ub2e4. $C$\ub294 \ud480\uac8c \ub418\uba74 \ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd\uc73c\ub85c \ud45c\ud604\ub418\uba70,\n$D = 0$\uc774\uae30 \ub54c\ubb38\uc5d0 shifted input $u(t-\\theta)$ \uc758 \uc815\ud655\ub3c4\ub97c \ud604\uc7ac state $m_t$\ub97c \uae30\uc900\uc73c\ub85c \ud310\ubcc4\ud560 \uc218\n\uc788\ub2e4.", "start_char_idx": 4493, "end_char_idx": 6102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8": {"__data__": {"id_": "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f84033da-2946-4cb7-ab76-d68300f1d759", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "5b53a5de6cbce785ffd25c9e324fed288a8a018e9dc8a40dc3dab30dd6c29289", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f6d5450-7650-44c1-becc-83c00dc01ce6", "node_type": "1", "metadata": {}, "hash": "2bea5d3ba618ef6b44ca3904e6323bb1f0ad0671884744fc43708415c853f04e", "class_name": "RelatedNodeInfo"}}, "text": "$C$\ub294 \ud480\uac8c \ub418\uba74 \ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd\uc73c\ub85c \ud45c\ud604\ub418\uba70,\n$D = 0$\uc774\uae30 \ub54c\ubb38\uc5d0 shifted input $u(t-\\theta)$ \uc758 \uc815\ud655\ub3c4\ub97c \ud604\uc7ac state $m_t$\ub97c \uae30\uc900\uc73c\ub85c \ud310\ubcc4\ud560 \uc218\n\uc788\ub2e4. \uc608\ucee8\ub370, $\\theta^\\prime$\ub9cc\ud07c\uc758 phase\uac00 \uc774\ub3d9\ub41c \uc2e0\ud638\ub97c \uc608\uce21\ud558\uace0\uc790 \ud55c\ub2e4\uba74 \ub2e4\uc74c\uacfc \uac19\uc774 **shifted Legendre\npolynomial** \ub97c \ud1b5\ud574 \uadfc\uc0ac\ud560 \uc218 \uc788\ub2e4.\n\n[ \\begin{aligned} C_i(\\theta^\\prime) = (-1)^i \\sum_{l=0}^i {i \\choose l}{i+1\n\\choose j}&\\left(-\\frac{\\theta^\\prime}{\\theta}\\right)^l,~0 \\le \\theta^\\prime\n\\le \\theta \\newline u(t-\\theta^\\prime) \\approx& C(\\theta^\\prime)^\\top m_t\n\\end{aligned} ]\n\n\uc124\uba85\uc774 \uae38\uc5c8\uc9c0\ub9cc \ub2e4\uc2dc \ud480\uc5b4\uc11c \uc124\uba85\ud558\uc790\uba74, \uc774\uc0c1\uc801\uc778 \ub51c\ub808\uc774 \uc2dc\uc2a4\ud15c\uc744 LTI \uc2dc\uc2a4\ud15c\uc73c\ub85c \uad6c\ucd95\ud558\uc5ec \ud45c\ud604\ud55c \uac83\uc774 \uae30\uc874\uc758 Linear state\nmachine \ub514\uc790\uc778\uc774\uc5c8\uace0, \uc774\ub97c \ub2e4\uc2dc non-linear neural network system\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ud55c \uac83\uc774 LMU \uad6c\uc870\uac00\n\ub418\uaca0\ub2e4. _\ub51c\ub808\uc774 \uc2dc\uc2a4\ud15c\uc744 \uc194\ub8e8\uc158\uc73c\ub85c \uc0bc\uc544_ \ub124\ud2b8\uc6cc\ud06c\ub97c \ud559\uc2b5\ud558\ub824\uace0 \ud55c \uac83\uc774\ub2e4.\n\n### HiPPO\n\n[HiPPO](https://arxiv.org/pdf/2008.07669.pdf)\ub294 LMU\ub97c **\uc77c\ubc18\ud654\ud55c \uad6c\uc870** \uc5d0 \ud574\ub2f9\ub41c\ub2e4.\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/10fb0ea0-6866-42a6-b73b-440873745c62)\n\n**HiPPO\uc758 \ubc29\ubc95** \uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4:\n\n\uc6d0\ub798 \ud568\uc22b\uac12\uacfc \uc608\uce21\ub41c \ud568\uc22b\uac12 \uc0ac\uc774\uc758 \ucc28\uc774\ub97c measure\ud560 \uc218 \uc788\ub294 Hilbert space $\\mu$\uc0c1\uc5d0\uc11c \uac01 \uad6c\uac04\uc758 \uc5f0\uc18d \ud568\uc218\uc778 $f$\ub97c\n$g$\ub77c\ub294 subspace\ub85c \ubcf4\ub0b4\ub294 \uacfc\uc815\uc744 \uac70\uce5c \ub4a4, \uc774\ub97c \uc801\ub2f9\ud55c vector basis\uc758 coefficient\uc758 \ubc30\uc5f4\ub85c \ud45c\ud604\ud55c\ub2e4. \uadf8\ub807\uac8c\n\ub418\uba74 Continous-time ODE\ub97c LTI system\uc758 \ubbf8\ubd84 \ubc29\uc815\uc2dd\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\uac8c \ub418\uba70, \uc774\ub54c system\uc758 \uc8fc\ucd95\uc774 \ub418\ub294\n$A(t)$\uc640 $B(t)$ \ud568\uc218\uc758 \ud615\ud0dc\ub97c \uacb0\uc815\ud558\uc5ec \uc2dc\ud000\uc2a4 \uba54\ubaa8\ub9ac\uc5d0 \ub300\ud55c \uc911\uc694\ub3c4\ub97c \ub9e4\ud551\ud55c\ub2e4. \uc774\ub97c \ud1b5\ud574 \uae30\uc874 LMU\ub97c continuous-\ntime memorization\uc73c\ub85c \uc77c\ubc18\ud654\uc2dc\ucf30\ub2e4. \uc65c\ub0d0\ud558\uba74 \uae30\uc874 LMU(\ub974\uc7a5\ub4dc\ub974 \uba54\ubaa8\ub9ac \uc720\ub2db)\uc5d0\uc11c\ub294 \ud2b9\uc815 \uc2ac\ub77c\ub529 \uc708\ub3c4\uc6b0\n\ud06c\uae30($\\theta$)\ub97c \uac00\uc9c0\ub294 \uc774\uc0c1\uc801\uc778 delay system\uc758 _LTI \ubbf8\ubd84 \ubc29\uc815\uc2dd\uc744 \uadf8\ub300\ub85c \uc774\uc0b0\ud654\ud558\uc5ec \uc0ac\uc6a9_ \ud558\uae30 \ub54c\ubb38\uc774\ub2e4.\n\n* * *\n\n# LSSL \ubaa8\ub378\ub9c1\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/3e50bc00-c112-4ba5-acd9-b8c1db11a482)\n\n\uc774\ub7f0 \uc774\uc804 \ubaa8\ub4c8\ub4e4\uc758 \ubc1c\uc804\uc740 \ubaa8\ub450 \uacf5\ud1b5\uc801\uc73c\ub85c \uae30\uc874 CNN/RNN\uc758 \uad6c\uc870 \ubc0f \ub2e8\uc810\uc744 _time-step \ucc28\uc6d0\uc5d0\uc11c \uc811\uadfc\ud588\ub2e4_ \ub294 \uc810\uc774\ub2e4. \ud558\uc9c0\ub9cc\n\ubaa8\ub4e0 \ubc29\ubc95\ub4e4\uc740 **convolutional/recurrent model** \uc758 \ubb38\uc81c\uc810\uc744 \uadfc\ubcf8\uc801\uc73c\ub85c \ud574\uacb0\ud558\uc9c0 \ubabb\ud588\ub2e4\ub294 \uc810\uc774 \ud55c\uacc4\uc810\uc73c\ub85c\n\uc791\uc6a9\ud588\ub2e4.", "start_char_idx": 5995, "end_char_idx": 7580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0f6d5450-7650-44c1-becc-83c00dc01ce6": {"__data__": {"id_": "0f6d5450-7650-44c1-becc-83c00dc01ce6", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e5dbcf2efc706601f77baedde81f0557c9fe633d7edeb22782a8c74c2ccd6653", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "node_type": "1", "metadata": {}, "hash": "f9f9b6ba156c74d6775843798e509bb43aa6e493cbd40cddc407594425c414e9", "class_name": "RelatedNodeInfo"}}, "text": "* * *\n\n# LSSL \ubaa8\ub378\ub9c1\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/3e50bc00-c112-4ba5-acd9-b8c1db11a482)\n\n\uc774\ub7f0 \uc774\uc804 \ubaa8\ub4c8\ub4e4\uc758 \ubc1c\uc804\uc740 \ubaa8\ub450 \uacf5\ud1b5\uc801\uc73c\ub85c \uae30\uc874 CNN/RNN\uc758 \uad6c\uc870 \ubc0f \ub2e8\uc810\uc744 _time-step \ucc28\uc6d0\uc5d0\uc11c \uc811\uadfc\ud588\ub2e4_ \ub294 \uc810\uc774\ub2e4. \ud558\uc9c0\ub9cc\n\ubaa8\ub4e0 \ubc29\ubc95\ub4e4\uc740 **convolutional/recurrent model** \uc758 \ubb38\uc81c\uc810\uc744 \uadfc\ubcf8\uc801\uc73c\ub85c \ud574\uacb0\ud558\uc9c0 \ubabb\ud588\ub2e4\ub294 \uc810\uc774 \ud55c\uacc4\uc810\uc73c\ub85c\n\uc791\uc6a9\ud588\ub2e4.\n\nLinear State-Space Layer (LSSL)\uc740 \uc704\uc758 \uadf8\ub9bc\uc5d0\uc11c \ub098\uc624\ub294 \uac01\uac01\uc758 \uc7a5\uc810\uc744 \ud1b5\ud569\ud55c \uad6c\uc870\ub97c \uace0\uc548\ud558\ub294 \uac83\uc744 \uc8fc\ub41c \ubaa9\uc801\uc73c\ub85c\n\uc0bc\uc558\ub2e4. \uacb0\uad6d formulation\uc740 \uc774\uc804 approach\uc640 \ud070 \ucc28\uc774\ub294 \uc5c6\ub2e4. LSSL\uc740 1-dimensional function \ud639\uc740\nsequence $u(t) \\rightarrow y(t)$\ub97c implicit function $x(t)$\ub97c \ud1b5\ud574 mapping\ud558\uace0\uc790 \ud558\ub294\n\ubc29\ubc95\uc774\ub2e4.\n\n$A$\ub294 \uc55e\uc11c \ubd24\ub358 LMU\uc5d0\uc11c\uc640 \uac19\uc774 system\uc758 **implicit function** $x(t)$\uc758 **evolution** \uc744\n\uc870\uc815\ud558\ub294 matrix\uc774\uba70, $B, C, D$\ub294 **projection** \uc5d0 \uc0ac\uc6a9\ub41c\ub2e4.\n\n[ \\begin{aligned} \\dot{x}(t) =& Ax(t) + Bu(t) \\newline y(t) =& Cx(t) + Du(t)\n\\end{aligned} ]\n\n\ub9cc\uc57d $\\Delta t$\ub97c _discrete step-size_ \ub85c \uc815\ud55c\ub2e4\uba74, LSSL\uc740 \uc815\ud574\uc9c4 \uac2f\uc218\uc758 \uba54\ubaa8\ub9ac\uc640 \uc5f0\uc0b0\uc73c\ub85c \uac01 \uc2dc\uac04 \ucd95\uc5d0 \ub530\ub77c\nstate\ub97c \ubcc0\ud654\uc2dc\ud0a4\ub294 **recurrent model** \ub85c \ud574\uc11d\ud560 \uc218 \uc788\uc73c\uba70, LTI system\uc778 \uc704\uc758 \ub450 \uc218\uc2dd\uc740 \uacb0\uad6d\n**continous convolution** \uc73c\ub85c \ud45c\ud604\ub420 \uc218 \uc788\ub2e4. \uace0\ub85c discrete-time version\uc758 LTI system \ub610\ud55c\nconvolution\uc73c\ub85c \ubcd1\ub82c\ud654\uac00 \uac00\ub2a5\ud558\ub2e4. \ud559\uc2b5 \uc18d\ub3c4\uac00 \ube68\ub77c\uc9c8 \uc218 \uc788\ub2e4\ub294 \uac83\uc774\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c LSSL\uc740 LTI system\uc758 \ubaa8\ub378\ub9c1 \uc790\uccb4\uac00\ndifferential equation\uc774\uae30 \ub54c\ubb38\uc5d0 continous-time model\uc758 \ubaa8\ub4e0 \uc801\uc6a9 \uac00\ub2a5\ud55c \uc0c1\ud669\uc744 \uadf8\ub300\ub85c \ubaa8\ubc29\ud560 \uc218 \uc788\ub2e4.\n\n\uacb0\uad6d \uc774 \ub17c\ubb38\uc5d0\uc11c \ubc1d\ud788\uace0\uc790 \ud55c \ub0b4\uc6a9\uc740 \uc704\uc758 LSSL\uc774 \uace0\uc804\uc801\uc778 \uc81c\uc5b4 \uc774\ub860\uc73c\ub85c\ubd80\ud130 \uc775\ud788 \uc54c\ub824\uc838\uc788\ub294 \uc0ac\uc2e4\uacfc \uac19\uc774 \ubaa8\ub4e0 \ud615\ud0dc\uc758 1-D\nConvolution\uc744 \ud45c\ud604\ud560 \uc218 \uc788\uc744 \ubfd0\ub9cc \uc544\ub2c8\ub77c, \uc801\uc808\ud55c step size\uc778 $\\Delta t$ \uadf8\ub9ac\uace0 \uc801\uc808\ud55c state matrix\n$A$\ub97c \uac00\uc9c0\uace0 RNN \ubc0f ODE\uac00 \uac00\uc9c0\ub294 \ud2b9\uc131(\ud2b9\ud788 \uc7a5\uc810\uc5d0 \uc9d1\uc911)\uc744 \uadf8\ub300\ub85c \uac00\uc838\uc62c \uc218 \uc788\ub2e4\ub294 \uac83\uc774\ub2e4. $A$\ub294 \ub2e4\uc2dc \ub9d0\ud558\uc9c0\uba74 \uc2dc\uc2a4\ud15c\uc758\n\ubcc0\ud654\ub97c \uc8fc\ub3c4\ud558\ub294 \ud559\uc2b5 \ud589\ub82c\ub85c \uc0ac\uc6a9\ub418\ub294\ub370, HiPPO\uc640 \uac19\uc740 \uc774\uc804 \uc5f0\uad6c\uc5d0\uc11c \ub4dc\ub7ec\ub0ac\ub358 \uac83\ucc98\ub7fc \uc5f0\uc18d \uc2dc\uac04\uc5d0 \ub300\ud55c memory\ub97c \uace0\ub824\ud558\uba74\uc11c \ub3d9\uc2dc\uc5d0\nlong dependency\ub97c \uace0\ub824\ud574\uc57c\ud55c\ub2e4.\n\n* * *\n\n# Continuous-time memorization\n\nContinuous time memorization \uc5d0 \ub300\ud55c \uadfc\uc0ac\ud654(approximation)\ub294 HiPPO \uadf8\ub9ac\uace0 LSSL \ub17c\ubb38\uc5d0\uc11c\n\uacf5\ud1b5\uc801\uc73c\ub85c \uac00\uc9c0\ub294 \uc774\ub860\uc801/\uae30\uc220\uc801 \ubc30\uacbd\uc5d0 \ud574\ub2f9\ub41c\ub2e4.\n\n\ud544\uc5f0\uc801\uc73c\ub85c \uc5f0\uc18d \uc2dc\uac04 \ubaa8\ub378\ub9c1\uc744 \uadf8\ub300\ub85c \uc801\uc6a9\ud560 \uc218 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \uc774\ub97c \uc774\uc0b0 \uc2dc\uac04 \ubaa8\ub378\ub85c \uadfc\uc0ac\ud654 \ud639\uc740 \ub2e4\uc6b4 \uc0d8\ud50c\ub9c1\ud558\ub294 \uacfc\uc815\uc744 \uac70\uce58\uac8c \ub41c\ub2e4.", "start_char_idx": 7306, "end_char_idx": 8969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4": {"__data__": {"id_": "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f6d5450-7650-44c1-becc-83c00dc01ce6", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "da8525b6ac78561799ad83505c625031c0f1db6914a58335b747ba9161f70f01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "node_type": "1", "metadata": {}, "hash": "545625ea73e87e09c3f5c01f6f3102f969d0ee5474338b63c8b019b5eebb5f15", "class_name": "RelatedNodeInfo"}}, "text": "* * *\n\n# Continuous-time memorization\n\nContinuous time memorization \uc5d0 \ub300\ud55c \uadfc\uc0ac\ud654(approximation)\ub294 HiPPO \uadf8\ub9ac\uace0 LSSL \ub17c\ubb38\uc5d0\uc11c\n\uacf5\ud1b5\uc801\uc73c\ub85c \uac00\uc9c0\ub294 \uc774\ub860\uc801/\uae30\uc220\uc801 \ubc30\uacbd\uc5d0 \ud574\ub2f9\ub41c\ub2e4.\n\n\ud544\uc5f0\uc801\uc73c\ub85c \uc5f0\uc18d \uc2dc\uac04 \ubaa8\ub378\ub9c1\uc744 \uadf8\ub300\ub85c \uc801\uc6a9\ud560 \uc218 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \uc774\ub97c \uc774\uc0b0 \uc2dc\uac04 \ubaa8\ub378\ub85c \uadfc\uc0ac\ud654 \ud639\uc740 \ub2e4\uc6b4 \uc0d8\ud50c\ub9c1\ud558\ub294 \uacfc\uc815\uc744 \uac70\uce58\uac8c \ub41c\ub2e4.\n\n\ub514\ud4e8\uc804 \ubaa8\ub378\ub9c1\uc5d0\uc11c\ub3c4 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub358 \uac83\ucc98\ub7fc \uacb0\uad6d \uc5f0\uc18d \uc2dc\uac04 \ubbf8\ubd84 \ubc29\uc815\uc2dd\uc758 $dt$\ub97c \uc5bc\ub9c8\ub098 \uc870\uc815\ud558\ub0d0\uc5d0 \ub530\ub77c \uc0dd\uc131 \uc131\ub2a5\uc774 \ub2ec\ub77c\uc84c\uae30 \ub54c\ubb38\uc5d0,\n\uacb0\uad6d \uc5f0\uc18d \uc2dc\uac04 \ubaa8\ub378\ub9c1\uc744 \uc774\uc0b0\ud654\ud560 \ub54c\ub294 step size/time scale\uc778 $\\Delta t$\ub97c \uc870\uc808\ud558\ub294 \uac83\uc774 \uc911\uc694\ud558\ub2e4.\n\n\ud574\ub2f9 \uc139\uc158\uc5d0\uc11c\ub294 LSSL \ubaa8\ub378\ub9c1\uc73c\ub85c\ubd80\ud130 \uc5ec\ub7ec property\uc5d0 \ub300\ud55c insight\ub97c \uc5bb\uc744 \uc218 \uc788\ub294 **\uadfc\uac70** \ub77c\uace0 \ubcfc \uc218 \uc788\ub294 \uac1c\ub150\ub4e4\uc5d0\n\ub300\ud574\uc11c \uc815\ub9ac\ud558\ub3c4\ub85d \ud558\uaca0\ub2e4.\n\n### Approximations of differential equations\n\n\ubaa8\ub4e0 \ud615\ud0dc\uc758 differential equation $\\dot{x}(t) = f(t, x(t))$\ub294 integral equation\n$x(t) = x(t_0) + \\int_{t_0}^t f(s, x(s))ds$\uc744 \ub3d9\uce58\ub85c \uac00\uc9c0\uac8c \ub41c\ub2e4. \ud574\ub2f9 integral solution\uc740\n\ud568\uc218 $x$\uc758 \uadfc\uc0ac\uce58\ub97c $f(s, x(s))$\uc5d0 \ub123\uace0 \uacc4\uc18d \uc5f0\uc0b0\uc744 \ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \ud480\uc5b4\ub0bc \uc218 \uc788\ub2e4. \uc608\ucee8\ub370 $x_0(t) = x(t_0)$\ub77c\ub294\n\ud568\uc218 \ucd08\uae30 \uc870\uac74\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4\uba74,\n\n[ x_{i+1} (t) = x_0 (t) + \\int_{t_0}^t f(s, x_{i}(t))ds ]\n\n\uc704\uc640 \uac19\uc774 \uadfc\uc0ac\ud654\ud560 \uc218 \uc788\ub2e4. \uc774\ub97c _[Picard\niteration](https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem)_\n\uc774\ub77c\uace0 \ubd80\ub978\ub2e4.\n\n### Discretization\n\n\uadf8\ub9ac\uace0 \uc774\uc0b0\ud654 \uacfc\uc815\uc5d0\uc11c \ud568\uc218\ub97c \uc9c1\uc811 \uc801\ubd84\ud574\ub0bc \uc218 \uc5c6\uae30 \ub54c\ubb38\uc5d0 discrete times $t_i$\uc5d0 \ub300\ud574, $x(t_i)$\ub97c \ucabc\uac1c\uc11c\n\uc5bb\uc5b4\ub0b4\uc57c\ud55c\ub2e4. Integral equation\uc758 form\uc744 closed form\uc73c\ub85c \uc815\ud655\ud788 \uacc4\uc0b0\ud560 \uc218 \uc788\ub2e4\uba74 \ub2e8\uc21c\ud788\ndownsampling\ud558\ub294 \ubc29\ubc95\uc73c\ub85c \uac01 $x(t_0), x(t_1), \\cdots$ \ub97c \uc5bb\uc5b4\ub0b4\uac70\ub098, closed form\uc73c\ub85c \uc54c\uc9c0 \ubabb\ud558\ub354\ub77c\ub3c4\n_picard iteration_ \uc744 \uac01 \uad6c\uac04\ubcc4 integral equation\uc778\n\n[ x(t_{k+1}) = x(t_k) + \\int_{t_k}^{t_{k+1}} f(s, x(s)) ds ]\n\n\uc5d0 \uc801\uc6a9\ud558\uc5ec \uac01 $t_k$ \uc2dc\uc810\uc758 \ud568\uc22b\uac12\ub4e4\uc744 \uc0d8\ud50c\ub9c1\ud560 \uc218 \uc788\ub2e4. \ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c\ub294 **generalized bilinear transform\n(GBT)** \uac00 \uc788\ub294\ub370, \uc774\ub294 \ud604\uc7ac \uc6b0\ub9ac\uac00 \uad00\uc2ec\uc788\ub294 Linear ODE\uc5d0 \uc801\uc6a9\ub420 \uc218 \uc788\ub294 \ubc29\ubc95\uc774\ub2e4. \ud480\uace0\uc790\ud558\ub294 Linear ODE\uc758 \ud615\ud0dc\uac00\n\ub2e4\uc74c\uacfc \uac19\uc744\ub54c,\n\n[ \\begin{aligned} \\dot{x}(t) =& Ax(t) + Bu(t) \\newline y(t) =& Cx(t) + Du(t)\n\\end{aligned} ]\n\nGBT update\ub294 \ub2e4\uc74c\uc758 \uc218\uc2dd\uc73c\ub85c \uc9c4\ud589\ub41c\ub2e4. \uc218\uc2dd\uc5d0\uc11c\uc758 $\\Delta t$\ub294 step size\ub97c \uc758\ubbf8\ud55c\ub2e4.", "start_char_idx": 8753, "end_char_idx": 10358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5": {"__data__": {"id_": "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "23b3689a6d8c87d6817e077be4a5fa487c395d21feba17ceb51e9cf3cd7a13f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82f37326-55ac-4255-926e-9642e2a69cc4", "node_type": "1", "metadata": {}, "hash": "efd4b28dc7c97a2a84662daa833ab2861f008df6b07dff0397e08b2c39f89835", "class_name": "RelatedNodeInfo"}}, "text": "\ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c\ub294 **generalized bilinear transform\n(GBT)** \uac00 \uc788\ub294\ub370, \uc774\ub294 \ud604\uc7ac \uc6b0\ub9ac\uac00 \uad00\uc2ec\uc788\ub294 Linear ODE\uc5d0 \uc801\uc6a9\ub420 \uc218 \uc788\ub294 \ubc29\ubc95\uc774\ub2e4. \ud480\uace0\uc790\ud558\ub294 Linear ODE\uc758 \ud615\ud0dc\uac00\n\ub2e4\uc74c\uacfc \uac19\uc744\ub54c,\n\n[ \\begin{aligned} \\dot{x}(t) =& Ax(t) + Bu(t) \\newline y(t) =& Cx(t) + Du(t)\n\\end{aligned} ]\n\nGBT update\ub294 \ub2e4\uc74c\uc758 \uc218\uc2dd\uc73c\ub85c \uc9c4\ud589\ub41c\ub2e4. \uc218\uc2dd\uc5d0\uc11c\uc758 $\\Delta t$\ub294 step size\ub97c \uc758\ubbf8\ud55c\ub2e4.\n\n[ x(t+\\Delta t) = (I-\\alpha \\Delta t \\cdot A)^{-1}(I+(1-\\alpha)\\Delta t \\cdot\nA)x(t) +\\Delta t(I-\\alpha \\Delta t \\cdot A)^{-1}B \\cdot u(t) ]\n\n\uc218\uc2dd\uc774 \uc870\uae08 \ubcf5\uc7a1\ud574\uc11c \ud55c\ubc88\uc5d0 \uc798 \uc774\ud574\uac00 \ub418\uc9c8 \uc54a\uc9c0\ub9cc \ud2b9\ubcc4\ud55c \ucf00\uc774\uc2a4\ub97c \ubcf4\uba74 \uc774\ud574\ud558\uae30 \uc5b4\ub835\uc9c0 \uc54a\ub2e4. $\\alpha = 0$\uc744 \uc704 \uc218\uc2dd\uc5d0\n\ub300\uc785\ud558\uba74,\n\n[ \\begin{aligned} x(t+\\Delta t) =& x(t) + \\Delta t \\cdot (Ax(t) + Bu(t))\n\\newline =& x(t) + \\Delta t \\cdot \\dot{x}(t) \\end{aligned} ]\n\n\uc704\uc640 \uac19\uc774 \ud45c\ud604\ub418\uba70 \uc774\ub294 \uac00\uc7a5 \ub300\ud45c\uc801\uc778 \ubc29\ubc95\uc778 _Euler method_ \uc784\uc744 \uc54c \uc218 \uc788\ub2e4. \uacb0\uad6d $\\alpha$\ub294 \ub3d9\uc77c\ud558\uac8c \ud568\uc218\ub97c \uad6c\ud558\ub294\n\ubc29\uc2dd\uc5d0\uc11c \uc5b4\ub290 \uc704\uce58\uc5d0\uc11c\uc758 \ubbf8\ubd84\uac12\uc744 \uc0ac\uc6a9\ud558\ub0d0\uc5d0 \ub530\ub77c \ub2ec\ub824\uc788\ub2e4. $\\alpha=1$\uc774 \ub418\uba74 _backward Euler method_ \uac00\n\ub418\ub294\ub370, \uc774\ub294 \ub3d9\uc77c\ud558\uac8c \ud568\uc218\ub97c \uc608\uce21\ud560 \ub54c \ud2b9\uc815 \uc704\uce58\uc5d0\uc11c\uc758 \ub3c4\ud568\uc218\uc5d0 \uae30\ubc18\ud55c first order approximation\uc774\ub77c\ub294 \uc810\uc740 \uac19\uc9c0\ub9cc\n\ud2b9\uc815 \uc704\uce58\uac00 $t$ \uac00 \uc544\ub2cc $t + \\Delta t$ \ub77c\ub294 \uc810\uc5d0\uc11c \ucc28\uc774\uac00 \uc788\ub2e4.\n\n[ x(t+\\Delta t) = (I-\\Delta t A)^{-1}x(t) + \\Delta t (I - \\Delta t A)^{-1} B\n\\cdot \\dot{x}(t) ]\n\n\ub530\ub77c\uc11c $\\alpha = \\frac{1}{2}$\ub97c \uc0ac\uc6a9\ud558\uac8c \ub418\uba74 \uc11c\ub85c \ub2e4\ub978 \ub450 \uc704\uce58\uc758 \ub3c4\ud568\uc218 \ud3c9\uade0\uc744 \uc4f0\uac8c \ub418\ubbc0\ub85c, \ub9cc\uc57d \uace1\ub960\uc774 \ud070 \ubcf5\uc7a1\ub3c4\uac00\n\ub192\uc740 \ud568\uc218\uac00 \uc194\ub8e8\uc158\uc744 \uad6c\uc131\ud558\ub294 \uc0c1\ud669\uc5d0\uc11c\ub294 \uac19\uc740 $\\Delta t$\ub97c \uc0ac\uc6a9\ud558\ub354\ub77c\ub3c4 \ubcf4\ub2e4 \uc548\uc815\uc801\uc778 \ud568\uc218 \uc608\uce21\uc774 \uac00\ub2a5\ud574\uc9c4\ub2e4. \uc774\ub97c\n_bilinear_ \ubc29\ubc95\uc774\ub77c\uace0 \ubd80\ub978\ub2e4.\n\n[ x(t+\\Delta t) = (I-\\Delta t / 2A)^{-1}(I+\\Delta t / 2A) x(t) + \\Delta t (I -\n\\Delta t / 2A)^{-1} B\\cdot\\dot{x}(t) ]\n\n\uc774\ub807\uac8c _bilinear_ \ubc29\ubc95\uc5d0 \uc0ac\uc6a9\ub418\ub294 matrix A\uc640 B\ub97c $\\bar{A}, \\bar{B}$ \ub77c\uace0 \ud588\uc744 \ub54c, \uc774\ub97c \ud1b5\ud574 \uc704\uc758 \uc2dc\uc2a4\ud15c\uc744\ndiscretize\ud558\uac8c \ub418\uba74 \ub2e4\uc74c\uacfc \uac19\uc740 discrete-time state-space model\uc744 \uad6c\ud560 \uc218 \uc788\ub2e4.\n\n[ \\begin{aligned} x_t =& \\bar{A}x_{t-1} + \\bar{B}u_t \\newline y_t =& Cx_t +\nDu_t \\end{aligned} ]\n\n### Timescale factor\n\n\uc2dc\ud000\uc2a4 \uae38\uc774\uc5d0 \ub530\ub978 dependency\ub294 \uae38\uc774\uac00 \uae38\uc5b4\uc9c8\uc218\ub85d \uc904\uc5b4\ub4e0\ub2e4. \uc608\ucee8\ub370 $\\Delta t$ \ub9cc\ud07c\uc744 \uc2dc\uac04 \uac04\uaca9\uc73c\ub85c \uc7a1\ub294\ub2e4\uba74 \uc758\uc874\ub3c4\ub294 \uadf8\uc5d0\n\ubc18\ube44\ub840\ud558\uac8c \ub41c\ub2e4.", "start_char_idx": 10072, "end_char_idx": 11735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "82f37326-55ac-4255-926e-9642e2a69cc4": {"__data__": {"id_": "82f37326-55ac-4255-926e-9642e2a69cc4", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "6c2d10746f9a65d44c20f924b59571280441e1c5e68b4de89d22c1c9f2bc6839", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "node_type": "1", "metadata": {}, "hash": "95d4c7a2310287e125e94ceb50adac3539b2ee55e43c052884a6c428760f7f45", "class_name": "RelatedNodeInfo"}}, "text": "[ \\begin{aligned} x_t =& \\bar{A}x_{t-1} + \\bar{B}u_t \\newline y_t =& Cx_t +\nDu_t \\end{aligned} ]\n\n### Timescale factor\n\n\uc2dc\ud000\uc2a4 \uae38\uc774\uc5d0 \ub530\ub978 dependency\ub294 \uae38\uc774\uac00 \uae38\uc5b4\uc9c8\uc218\ub85d \uc904\uc5b4\ub4e0\ub2e4. \uc608\ucee8\ub370 $\\Delta t$ \ub9cc\ud07c\uc744 \uc2dc\uac04 \uac04\uaca9\uc73c\ub85c \uc7a1\ub294\ub2e4\uba74 \uc758\uc874\ub3c4\ub294 \uadf8\uc5d0\n\ubc18\ube44\ub840\ud558\uac8c \ub41c\ub2e4. \ub300\ubd80\ubd84\uc758 ODE \uae30\ubc18 RNN \uad6c\uc870\uc5d0\uc11c\ub294 $\\Delta t$\ub97c \uace0\uc815\uac12\uc73c\ub85c \uc0ac\uc6a9\ud558\uc600\uc9c0\ub9cc, classical RNN\uc758\ngating \uba54\ucee4\ub2c8\uc998\uc740 \uc774\ub97c \ud559\uc2b5\ud558\ub294 \uac83\uacfc \uac19\uc740 \ud6a8\uacfc\ub97c \uc9c0\ub2cc\ub2e4. \uadf8\ub9ac\uace0 CNN \uad00\uc810\uc5d0\uc11c\uc758 $\\Delta t$\ub294 convolution\nkernel\uc758 \ud06c\uae30\ub97c \uc870\uc808\ud558\ub294 \ud615\ud0dc\ub85c \ud574\uc11d\uc774 \uac00\ub2a5\ud558\ub2e4. \uc989, CNN\uc774\ub4e0 RNN\uc774\ub4e0 ODE \uae30\ubc18\uc73c\ub85c \ud574\uc11d\ud55c\ub2e4\uba74 \ubaa8\ub450 \uc2dc\uac04 \uac04\uaca9\uc778 $\\Delta\nt$\ub97c \uc5b4\ub5bb\uac8c\ud558\uba74 \ucd5c\uc801\ud654\ud560 \uc218 \uc788\uc744\uae4c\uc5d0 \ub300\ud55c \ubb38\uc81c\ub85c \ud574\uc11d\uc774 \uac00\ub2a5\ud558\ub2e4\ub294 \uac83\uc774\ub2e4.\n\n### Continuous-time memory\n\n\uc785\ub825\ub418\ub294 \ud568\uc218 $u(t)$\uc640 \uace0\uc815\ub41c probability measure(\uba54\ud2b8\ub9ad) $\\omega(t)$\uac00 \uc788\uc744 \ub54c, \ud568\uc218\uc758 \uae30\ubcf8\uaf34\uc774 \ub418\ub294\n$N$\uac1c\uc758 basis\uac00 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uac01 time step $t$\ub9c8\ub2e4 \uc774\uc804\uae4c\uc9c0\uc758 input\ub4e4\uc778 $u(\\tau)\\vert_{\\tau <\nt}$ \ub294 $N$\uac1c\uc758 basis\uc758 \uc870\ud569\uc73c\ub85c \ud45c\ud604\uc774 \uac00\ub2a5\ud558\uace0, \uc774\ub294 \uace7 \ud568\uc218\ub97c projection\ud558\uc5ec \ud68d\ub4dd\ud55c coefficient vector\n$x(t) \\in \\mathbb{R}^{N}$ \uc774\ub2e4. \uc774\ub54c \uac01 time step\ub9c8\ub2e4\uc758 \ucd5c\uc801\uc758 \uc194\ub8e8\uc158\uc740 \uac70\ub9ac \uba54\ud2b8\ub9ad $\\omega(t)$\uc5d0\n\uc758\uc874\ud558\uac8c \ub41c\ub2e4. \uc774\ub807\ub4ef \ud568\uc218 $u(t)$\ub97c coefficient $x(t)$\ub85c \ud45c\ud604\ud558\ub294 \uacfc\uc815\uc774 \uc55e\uc11c \uc18c\uac1c\ud588\ub358 HiPPO (High-Order\nPolynomial Projection Operator)\uac00 \ub41c\ub2e4.\n\nHiPPO\uc758 \uacbd\uc6b0\uc5d0\ub294 \ub450 \uac00\uc9c0 \uacbd\uc6b0(\ud574\ub2f9 \ub17c\ubb38\uc5d0\uc11c\ub294 LegT, LagT\ub77c\ub294 \uc774\ub984\uc73c\ub85c \uc81c\uc548\ub41c \uba54\ud2b8\ub9ad)\ub97c \uc81c\uc548\ud558\uc600\ub294\ub370, \ubaa8\ub4e0 time step\uc5d0\n\uac19\uc740 \uc911\uc694\ub3c4\ub97c \ub9e4\ud551\ud558\ub294 uniform measure $\\omega = \\mathbb{I}{[0, 1]}$ \uc640, \uac00\uae4c\uc6b4 time step\uc5d0\n\ubcf4\ub2e4 \ub192\uc740 \uc911\uc694\ub3c4\ub97c \ub9e4\ud551\ud558\ub294 exponential-decaying measure $\\omega(t) = \\exp(-t)$ \uac00 \uc788\ub2e4. \ub17c\uc678\uae34\n\ud558\uc9c0\ub9cc HiPPO\uc5d0\uc11c\ub294 \uc815\ud574\uc9c4 sliding window \ud06c\uae30\ub97c \uac00\uc9c0\ub294 translated Legendre (LegT) \ub300\uc2e0 long\ndependency \ubc0f forgetting \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uace0\uc790 scaled Legendre (LegS)\ub97c \uc0ac\uc6a9\ud558\uc600\ub2e4. \ub458\uc758 \uacf5\ud1b5\uc810\uc740\nwindow \uc548\uc5d0\uc11c \uade0\uc77c\ud55c measure weight\uc744 \uac00\uc9c4\ub2e4\ub294 \uc810\uc774\uc9c0\ub9cc, LegS\ub294 \uc2dc\uac04\uc774 \ud750\ub97c\uc218\ub85d window \ud06c\uae30\uac00 \ucee4\uc9c4\ub2e4\ub294 \ucc28\uc774\uc810\uc774\n\uc788\ub2e4. \uc544\ubb34\ud2bc \uc911\uc694\ud55c \uc810\uc740 measure \uc885\ub958\uc5d0 \ub530\ub77c matrix $A$\ub97c closed form\uc73c\ub85c \ud480\uc5b4\ub0bc \uc218 \uc788\uc73c\uba70, \uc774\ub97c \ud1a0\ub300\ub85c long\ndependency\uc5d0 \ub300\ud55c \ubaa8\ub378\ub9c1\uc774 \uac00\ub2a5\ud558\ub2e4.\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/86837dba-a674-4944-b703-726028884b54)\n\n\uac01 \uba54\ud2b8\ub9ad\uc5d0 \ub530\ub978 matrix $A$\ub97c \uc815\ud558\ub294 \uacfc\uc815\uc740 HiPPO \ub17c\ubb38\uc758 Appendix\ub97c \ucc38\uace0\ud558\uba74 \ub418\ub294\ub370, \uc774\ub97c \uc870\uae08 \uac04\ub2e8\ud558\uac8c \uc815\ub9ac\ud574\ubcf4\uace0\uc790\n\ud55c\ub2e4.", "start_char_idx": 11527, "end_char_idx": 13197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "32e6969d-b76c-41b3-a3a7-f7a74f51c11f": {"__data__": {"id_": "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82f37326-55ac-4255-926e-9642e2a69cc4", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "f296abbc43349ae13ffa65aea1c4bc517af27a8e859151ff96eba860e5f8e785", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22f9084a-e999-4632-adb4-f197179525bf", "node_type": "1", "metadata": {}, "hash": "6e43ebb43456c81453b702ca3a461f39e83f6cfa9a28d8ca9d80ca446a46259a", "class_name": "RelatedNodeInfo"}}, "text": "\uc544\ubb34\ud2bc \uc911\uc694\ud55c \uc810\uc740 measure \uc885\ub958\uc5d0 \ub530\ub77c matrix $A$\ub97c closed form\uc73c\ub85c \ud480\uc5b4\ub0bc \uc218 \uc788\uc73c\uba70, \uc774\ub97c \ud1a0\ub300\ub85c long\ndependency\uc5d0 \ub300\ud55c \ubaa8\ub378\ub9c1\uc774 \uac00\ub2a5\ud558\ub2e4.\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/86837dba-a674-4944-b703-726028884b54)\n\n\uac01 \uba54\ud2b8\ub9ad\uc5d0 \ub530\ub978 matrix $A$\ub97c \uc815\ud558\ub294 \uacfc\uc815\uc740 HiPPO \ub17c\ubb38\uc758 Appendix\ub97c \ucc38\uace0\ud558\uba74 \ub418\ub294\ub370, \uc774\ub97c \uc870\uae08 \uac04\ub2e8\ud558\uac8c \uc815\ub9ac\ud574\ubcf4\uace0\uc790\n\ud55c\ub2e4. \uad00\ub828 \ub0b4\uc6a9\uc744 \uc774\ud574\ud558\ub294\ub370 \ud544\uc694\ud55c \uc0ac\uc804 \uc9c0\uc2dd\uc774 \ub108\ubb34 \ubc29\ub300\ud558\uc5ec \uc644\ubcbd\ud55c \uc99d\uba85 \uacfc\uc815\uc744 \ub2f4\uae30\uc5d0\ub294 \ubb34\ub9ac\uac00 \uc788\uc9c0\ub9cc \uadf8\ub7fc\uc5d0\ub3c4 HiPPO \uc804\ubc18\uc801\uc778\n\ub0b4\uc6a9\uc744 \uc774\ud574\ud574\uc57c LSSL \ubaa8\ub378\ub9c1\uc744 \ud574\uc11d\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4.\n\n### Orthogonal Polynomials\n\nOrthogonal polynomials (\uc11c\ub85c \uc218\uc9c1 \uad00\uacc4\uc5d0 \uc788\ub294 \ub2e4\ud56d\uc2dd)\uc740 \ud568\uc218\ub97c \ud574\uc11d\ud558\ub294\ub370 \uc0ac\uc6a9\ub418\ub294 \uae30\ubcf8\uc801\uc778 \ud234\uc774\ub2e4. \ubaa8\ub4e0 measure\n$\\mu$ \uc0c1\uc5d0\uc11c \ud574\ub2f9 OP\uc5d0 \ub300\uc751\ub418\ub294 unique\ud55c \ud568\uc218 \uc2dc\ud000\uc2a4\uac00 \ub098\uc624\uac8c \ub41c\ub2e4. \uc5ec\uae30\uc11c measure metric\uc740 \uc801\ubd84\uc774 \uc774\ub8e8\uc5b4\uc9c0\ub294 \uc11c\ube0c\n\uacf5\uac04\uc73c\ub85c \uc774\ud574\ud558\uba74 \ub41c\ub2e4. OP\uc758 \ud2b9\uc9d5\uc740, \uc11c\ub85c \ub2e4\ub978 OP\ub4e4\uc744 measure \uc0c1\uc5d0\uc11c \uc801\ubd84\ud588\uc744\ub54c 0\uc774 \ub098\uc640\uc57c\ud55c\ub2e4\ub294 \uac83\uc774\ub2e4. \uadf8\ub9ac\uace0 $i$\ubc88\uc9f8\nPolynomial\uc740 \ucc28\uc218\uac00 $i$\ub77c\ub294 constraints\ub3c4 \ud3ec\ud568\ub41c\ub2e4.\n\n[ \\langle P_i, P_j \\rangle_\\mu = \\int P_i(x) P_j(x) d\\mu (x) = 0~~(i \\neq\nj),~\\deg (P_i) = i ]\n\n\uc774\ub7ec\ud55c \uc870\uac74\uc5d0\uc11c $f$\ub77c\ub294 \uc774\uc0c1\uc801\uc778 \ud568\uc218\uc5d0 \uadfc\uc0ac\ud558\ub294 \ucd5c\uc801\uc758 \uc194\ub8e8\uc158\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \uacc4\uc0b0\ub41c\ub2e4.\n\n[ \\sum_{i=0}^{N-1} c_i P_i(x) / \\parallel P_i \\parallel_\\mu^2,~\\text{where\n}c_i = \\langle f,P_i \\rangle_\\mu = \\int f(x)P_i (x) d\\mu(x) ]\n\n\uac00\uc7a5 \ub300\ud45c\uc801\uc73c\ub85c \uc720\uba85\ud55c OP\uc5d0\ub294 Fourier series basis\ub97c \uc0dd\uac01\ud574\ubcfc \uc218 \uc788\uace0, Jacobi, Laguerre \ud639\uc740 Hermite\nPolynomial\ub3c4 \uc774\uc5d0 \ud3ec\ud568\ub41c\ub2e4. \uc5ec\uae30\uc5d0\uc11c \uc18c\uac1c\ud560 OP\ub294 Jacobi Polynomial\uc5d0 \uc18d\ud558\ub294 \ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd\uc774\ub2e4.\n\n### Legendre Polynomials\n\n\ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd\uc740 \ud754\ud788 \uad6c\uba74 \uc88c\ud45c\uacc4\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ud55c\ub2e4. \uacf5\ud559 \uc218\ud559\uc744 \ubc30\uc6b8 \ub54c\uc758 \uc545\ubabd\uc774 \ub5a0\uc624\ub974\ub294 \uae30\ubd84\uc774\ub2e4. \uc554\ud2bc orthogonal \uad00\uacc4\ub294\n\uc775\ud788 \uc54c\ub824\uc9c4\ub300\ub85c \uad6c\uac04 $[-1, 1]$ \ub0b4\uc5d0\uc11c $L^2$ \ub0b4\uc801\uc744 \ucde8\ud588\uc744 \ub54c $\\frac{2}{2n+1}$ \ub9cc\ud07c \uc2a4\ucf00\uc77c\ub9c1\ub41c \ud06c\ub85c\ub124\ucee4 \ub378\ud0c0\ub97c\n\ud68d\ub4dd\ud560 \uc218 \uc788\ub2e4. \uadf8\ub9ac\uace0 \uc720\uba85\ud55c \uc131\uc9c8 \uc911 \ud558\ub098\uac00 $P_n(1) = 1, P_n(-1) = (-1)^n$ \ub77c\ub294 \uacbd\uacc4\uc870\uac74\uc744 \uac00\uc9c4\ub2e4\ub294 \uac83.\n\n\uc5ec\uae30\uc11c \uc77c\uc885\uc758 \uc120\ud615\uc131\uc744 \ud1b5\ud574 \ub2e4\uc591\ud55c time-scale \ucd95\uc5d0 \ub300\ud55c Polynomial \ub610\ud55c \uad6c\ud560 \uc218 \uc788\ub2e4. \uacb0\uad6d \ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd\uc774 \uc131\ub9bd\ud558\ub294\nmeasure \uacf5\uac04 \uc790\uccb4\ub3c4 \uade0\uc77c \ud655\ub960 \ubd84\ud3ec\uc600\uae30 \ub54c\ubb38\uc5d0 \uac00\ub2a5\ud55c \uc77c\uc774\ub2e4.", "start_char_idx": 12916, "end_char_idx": 14417, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "22f9084a-e999-4632-adb4-f197179525bf": {"__data__": {"id_": "22f9084a-e999-4632-adb4-f197179525bf", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "6de3820e389ea15927e6ea439482ecf450186b47514b0a84eeaeab7f69a8fc9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "node_type": "1", "metadata": {}, "hash": "dfbbb011151c48588d7d5202708b1091d1e2ba13c05da6fd56785df774ca012c", "class_name": "RelatedNodeInfo"}}, "text": "\uadf8\ub9ac\uace0 \uc720\uba85\ud55c \uc131\uc9c8 \uc911 \ud558\ub098\uac00 $P_n(1) = 1, P_n(-1) = (-1)^n$ \ub77c\ub294 \uacbd\uacc4\uc870\uac74\uc744 \uac00\uc9c4\ub2e4\ub294 \uac83.\n\n\uc5ec\uae30\uc11c \uc77c\uc885\uc758 \uc120\ud615\uc131\uc744 \ud1b5\ud574 \ub2e4\uc591\ud55c time-scale \ucd95\uc5d0 \ub300\ud55c Polynomial \ub610\ud55c \uad6c\ud560 \uc218 \uc788\ub2e4. \uacb0\uad6d \ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd\uc774 \uc131\ub9bd\ud558\ub294\nmeasure \uacf5\uac04 \uc790\uccb4\ub3c4 \uade0\uc77c \ud655\ub960 \ubd84\ud3ec\uc600\uae30 \ub54c\ubb38\uc5d0 \uac00\ub2a5\ud55c \uc77c\uc774\ub2e4.\n\n\uc6d0\ub798\uc758 orthogonality\ub294 $[-1, 1]$\uc5d0\uc11c \uc131\ub9bd\ud588\uace0, \uc774\ub97c $[0, t]$ \uad6c\uac04\uc5d0\uc11c \uc131\ub9bd\ud558\uac8c \ud558\uae30 \uc704\ud574 \ud568\uc218 \uad6c\uac04\uc744 \ub9de\ucdb0\uc8fc\uac8c\n\ub418\uba74 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n[ \\begin{aligned} (2n+1)\\int_0^t P_n \\left( \\frac{2x}{t} - 1 \\right) P_m\n\\left( \\frac{2x}{t}-1 \\right) \\frac{1}{t} dx = \\frac{2n+1}{2}\\int P_n P_m\n\\omega_\\text{leg} dx \\end{aligned} ]\n\n\uc801\ubd84 \uad6c\uac04\ub9cc \ub9de\ucdb0\uc92c\ub294\ub370 \ub2e4\uc2dc \ud06c\ub85c\ub124\ucee4 \ub378\ud0c0\ub97c \ud68d\ub4dd\ud560 \uc218 \uc788\ub2e4. \uace0\ub85c measure\uac00 \uc2a4\ucf00\uc77c\ub9c1\ub41c \uacbd\uc6b0 \ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd\uc740 \uc6d0\ub798\uc758 \ub2e4\ud56d\uc2dd\uc744\n\uc2a4\ucf00\uc77c\ub9c1 \ud574\uc8fc\uba74 \uc27d\uac8c \uc5bb\uc744 \uc218 \uc788\ub2e4.\n\n[ (2n+1)^{1/2} P_n \\left(\\frac{2x}{t} - 1\\right) ]\n\n### Translated Legendre\n\nTranslated Legendre\ub294 \uc708\ub3c4\uc6b0 \ud06c\uae30\uac00 $\\theta$\uc774\uace0, \ud604\uc7ac \uc9c0\uc810\uc774 $t$\uc778 \uacbd\uc6b0\uc758 Legendre measure\ub97c\n\uc758\ubbf8\ud55c\ub2e4.\n\n[ \\begin{aligned} \\omega(t, x) =& \\frac{1}{\\theta} \\mathbb{I}_{[t-\\theta, t]}\n\\newline p_n(t, x) =& (2n+1)^{1/2}P_n\\left(\\frac{2(x-t)}{\\theta} + 1\\right)\n\\newline g_n(t, x) =& \\lambda_n p_n (t, x) \\end{aligned} ]\n\n\uadf8\ub9ac\uace0 \uc6d0\ub798 \uc5ec\uae30\uc5d0\uc11c tilting \uac1c\ub150\uc774 \ub4f1\uc7a5\ud558\uba74\uc11c \uad73\uc774 OP\ub97c \uc4f0\uc9c0 \uc54a\uc744 \ub54c \uc0ac\uc6a9\ud558\ub294 \ud568\uc218\uac00 \ub4f1\uc7a5\ud55c\ub2e4. \uc774\ub97c $\\chi$\ub77c\uace0 \ud558\ub294\ub370,\n\ub9cc\uc57d $p_n(t, x)$ \ub300\uc2e0 \uc870\ud569\ub41c \ud568\uc218 \ud615\ud0dc\uc778 $p_n(x)\\chi(x)$\ub97c \uc4f4\ub2e4\uace0 \uac00\uc815\ud55c\ub2e4\uba74 \uac01 time step\uc5d0\uc11c \uc774\ubc88\uc5d0\ub294\n$\\omega/\\chi^2$\uc5d0 orthogonal\ud574\uc9c0\uac8c \ub41c\ub2e4 (OP \uacf1\ud558\uae30 OP \uacf1\ud558\uae30 $\\chi^2$\uc774 \ub418\ubbc0\ub85c). \ub9cc\uc57d\nnormalized\ub41c measure\uc640 orthonormal basis\ub97c \uad6c\ud55c\ub2e4\uce58\uba74,\n\n[ \\zeta(t) = \\int \\frac{\\omega}{\\chi^2} = \\int\n\\frac{\\omega^{(t)}(x)}{(\\chi^{(t)}(x))^2}dx ]\n\n\ud574\ub2f9 \ud568\uc218\uac00 \uace7 normalization constant\uac00 \ub41c\ub2e4. \uadf8\ub807\uae30\uc5d0 normalized\ub41c measure\uc778 $\\nu^{(t)}$\ub294\n$\\frac{\\omega^{(t)}(x)}{\\zeta(t)\\cdot(\\chi^{(t)}(x))^2}$\ub97c density\ub85c \uac00\uc9c4\ub2e4. \uc774\ub807\uac8c\uae4c\uc9c0\n\ud558\ub294 \uc774\uc720\ub294 \uacb0\uad6d tilted OP\ub97c orthonormal\ud558\uac8c \ub9de\ucdb0\uc8fc\uae30 \uc704\ud568\uc774\ub2e4. \uc704\uc758 \uc218\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec orthogonality\ub97c \ud655\uc778\ud558\uba74\n\ub974\uc7a5\ub4dc\ub974\uc5d0\uc11c\uc758 orthogonality\uac00 \uc6d0\ub798\uc758 measure $\\omega$\uc5d0 \ub300\ud574 \uc815\uaddc\ud654\uac00 \ub428\uc744 \uc54c \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uc774\uac74 \ud2b9\uc218\ud55c \uacbd\uc6b0\uc5d0\nformulation\uc744 \uc704\ud574 \uc0ac\uc6a9\ud558\uac8c \ub418\uc9c0\ub9cc, \ub974\uc7a5\ub4dc\ub974\uc5d0 \uc758\ud55c projection\uc5d0\ub294 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\ub294\ub2e4.", "start_char_idx": 14235, "end_char_idx": 15893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53": {"__data__": {"id_": "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22f9084a-e999-4632-adb4-f197179525bf", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "a3d13b5d2b8df4ad63607862d53404e79e346afbd406e78ac8acf3f401c6166a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "node_type": "1", "metadata": {}, "hash": "5afd11dd025e9f6cf2153318b9f7b4f7e2a13005dc447503f75bd95c521d4153", "class_name": "RelatedNodeInfo"}}, "text": "\uadf8\ub807\uae30\uc5d0 normalized\ub41c measure\uc778 $\\nu^{(t)}$\ub294\n$\\frac{\\omega^{(t)}(x)}{\\zeta(t)\\cdot(\\chi^{(t)}(x))^2}$\ub97c density\ub85c \uac00\uc9c4\ub2e4. \uc774\ub807\uac8c\uae4c\uc9c0\n\ud558\ub294 \uc774\uc720\ub294 \uacb0\uad6d tilted OP\ub97c orthonormal\ud558\uac8c \ub9de\ucdb0\uc8fc\uae30 \uc704\ud568\uc774\ub2e4. \uc704\uc758 \uc218\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec orthogonality\ub97c \ud655\uc778\ud558\uba74\n\ub974\uc7a5\ub4dc\ub974\uc5d0\uc11c\uc758 orthogonality\uac00 \uc6d0\ub798\uc758 measure $\\omega$\uc5d0 \ub300\ud574 \uc815\uaddc\ud654\uac00 \ub428\uc744 \uc54c \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uc774\uac74 \ud2b9\uc218\ud55c \uacbd\uc6b0\uc5d0\nformulation\uc744 \uc704\ud574 \uc0ac\uc6a9\ud558\uac8c \ub418\uc9c0\ub9cc, \ub974\uc7a5\ub4dc\ub974\uc5d0 \uc758\ud55c projection\uc5d0\ub294 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\ub294\ub2e4. \ub530\ub77c\uc11c \uadf8\ub0e5 \uc77c\ubc18\uc801\uc778 \uc218\uc2dd\uc744\n\uc0dd\uac01\ud574\uc8fc\uba74 \ub41c\ub2e4. \uc55e\uc11c \ucd94\uac00\ub85c \uc5b8\uae09\ud588\ub358 \ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd\uc758 \ud2b9\uc131\uc744 \ud65c\uc6a9\ud558\uba74 \ub9c8\ucc2c\uac00\uc9c0\ub85c shifted and scaled Legendre\uc5d0\n\ub300\ud574,\n\n[ \\begin{aligned} g_n(t, t) =& \\lambda_n (2n+1)^{1/2} \\newline g_n(t,t-\\theta)\n=& \\lambda_n (-1)^n (2n+1)^{1/2} \\end{aligned} ]\n\n\uc704\uc758 \uacbd\uacc4\uc870\uac74\uc744 \uac00\uc9c4\ub2e4.\n\n### Projection and Coefficients\n\n$A$ \ud558\ub098 \uc720\ub3c4\ud558\ub294\ub370 \ub108\ubb34 \ub3cc\uc544\uac00\ub294 \ub4ef \ud558\uc9c0\ub9cc HiPPO\ub97c \uc644\uc804\ud788 \uc815\ubcf5\ud558\uae30 \uc704\ud574\uc120 \ud544\uc218\uc801\uc778 \uc218\uc2dd\ub4e4\uc774\ub2e4. \uc55e\uc11c tilting\uc744 \uace0\ub824\ud55c\nmeasure\ub97c \uc720\ub3c4\ud588\uc5c8\ub294\ub370, \uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec coefficient\ub97c \uacc4\uc0b0\ud558\uae30 \uc704\ud574 measure\uc5d0 projection\ud55c \uacb0\uacfc\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n[ c_n(t) = \\zeta(t)^{-1/2} \\lambda_n \\int fp_n^{(t)}\n\\frac{\\omega^{(t)}}{\\chi^{(t)}} ]\n\n\ud574\ub2f9 \uc218\uc2dd\uc744 \ud1a0\ub300\ub85c end-to-end model\uc744 \uad6c\uc131\ud558\uace0, \ud574\ub2f9 \ub124\ud2b8\uc6cc\ud06c\uac00 online prediction\uc5d0 \uae30\ubc18\uc5d0\uc11c \uc774\uc804\uc758 \ud568\uc22b\uac12\n$f$ \uadf8\ub9ac\uace0 \ud604\uc7ac\uc758 \ud568\uc218\ub97c \uc81c\ub300\ub85c \ub300\ubcc0\ud558\uac8c \ud558\uae30 \uc704\ud574\uc11c\ub294 $c(t)$\ub97c \ubca1\ud130\ub85c \ud45c\ud604\ud574\uc57c\ud558\uace0, \uc774\ub294 \uace7 coefficient\uc758 \ubca1\ud130 \ud615\ud0dc\ub85c\n\uc5bb\uace0\uc790 \ud558\ub294 \ubaa9\uc801\uc5d0 \ubd80\ud569\ud55c\ub2e4.\n\nCoefficient\ub294 \ud56d\uc0c1 \ud604\uc7ac\uc758 \uc608\uce21\uc5d0 \uae30\ubc18\ud558\uc5ec \uc5c5\ub370\uc774\ud2b8\ub418\uc5b4\uc57c\ud55c\ub2e4. \uc989 coefficient\ub294 \uace0\uc815\ub418\uc5b4\uc788\uc9c0 \uc54a\uace0 \uc9c0\uc18d\uc801\uc73c\ub85c \ubcc0\ud558\ub294 \ud568\uc218\ub85c\n\uace0\ub824\ud574\uc57c\ud558\uba70, \uc774\uc5d0 \ub9de\ub294 \ubbf8\ubd84 \ubc29\uc815\uc2dd\uc744 \uc0dd\uac01\ud574\ubcfc \uc218 \uc788\ub2e4.\n\n[ \\begin{aligned} \\frac{d}{dt} c_n(t) &= \\zeta(t)^{-1/2} \\lambda_n \\int f(x)\n\\left(\\frac{\\partial}{\\partial t}p_n (t, x) \\right) \\frac{\\omega}{\\chi} (t, x)\ndx \\newline &+\\int f(x) \\left( \\zeta^{-1/2}\\lambda_n p_n(t, x)\n\\right)\\left(\\frac{\\partial}{\\partial t} \\frac{\\omega}{\\chi} (t, x)\\right) dx\n\\end{aligned} ]\n\n### Coefficient dynamics with Translated Legendre\n\n\ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd\uc758 projection\uc744 \uad6c\ud560 \ub54c tilting\uc744 \ubb34\uc2dc\ud55c\ub2e4\uace0 \ud588\ub2e4. \uadf8\ub7ec\uba74 \uc704\uc758 \uc218\uc2dd\uc744 \ud480\uc5b4\ub0bc \ub54c \ud544\uc694\ud55c \uac83\uc740 OP\uc758 \ud3b8\ubbf8\ubd84\uacfc\nmeasure\uc758 \ud3b8\ubbf8\ubd84\uc774\ub2e4. OP\uc758 \ud3b8\ubbf8\ubd84\uc740 \uc790\uc138\ud55c \uacfc\uc815\uc740 \uc0dd\ub7b5\ud558\uace0 \uacb0\uacfc\ub9cc \uc5b8\uae09\ud558\uc790\uba74 $n$\ubc88\uc9f8 \ub974\uc7a5\ub4dc\ub974\uc758 \ubbf8\ubd84\uc740 $n-1$\ubc88\uc9f8\uc758\n\ub974\uc7a5\ub4dc\ub974\uae4c\uc9c0\uc758 linear combination\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \ub180\ub77c\uc6b4 \ub974\uc7a5\ub4dc\ub974\uc758 \uc138\uacc4.", "start_char_idx": 15562, "end_char_idx": 17265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4": {"__data__": {"id_": "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "15849e3d9f23b0511ee058239851eb29ebf62be07837946412a5f4a4b7a66fdc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "node_type": "1", "metadata": {}, "hash": "923e70329c7fa94c0906827cc9d1077339e8c85ebcfec57e4cbf973b15c1f1e3", "class_name": "RelatedNodeInfo"}}, "text": "\uadf8\ub7ec\uba74 \uc704\uc758 \uc218\uc2dd\uc744 \ud480\uc5b4\ub0bc \ub54c \ud544\uc694\ud55c \uac83\uc740 OP\uc758 \ud3b8\ubbf8\ubd84\uacfc\nmeasure\uc758 \ud3b8\ubbf8\ubd84\uc774\ub2e4. OP\uc758 \ud3b8\ubbf8\ubd84\uc740 \uc790\uc138\ud55c \uacfc\uc815\uc740 \uc0dd\ub7b5\ud558\uace0 \uacb0\uacfc\ub9cc \uc5b8\uae09\ud558\uc790\uba74 $n$\ubc88\uc9f8 \ub974\uc7a5\ub4dc\ub974\uc758 \ubbf8\ubd84\uc740 $n-1$\ubc88\uc9f8\uc758\n\ub974\uc7a5\ub4dc\ub974\uae4c\uc9c0\uc758 linear combination\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \ub180\ub77c\uc6b4 \ub974\uc7a5\ub4dc\ub974\uc758 \uc138\uacc4.\n\n\uadf8\ub798\uc11c \uc815\ub9d0 \ub2e4\ud589\uc774\uc9c0\ub9cc $\\lambda_n p_n(t, x)$\uc758 \ubbf8\ubd84\uc740 \uc218\ub9ce\uc740 $g$\ub4e4\ub85c \uac04\ub2e8\ud558\uac8c \ud45c\ud604 \uac00\ub2a5\ud558\ub2e4.\n\n[ \\frac{\\partial}{\\partial t} g_n (t, x) = -\\lambda_n (2n+1)^{1/2}\n\\frac{2}{\\theta} \\left( \\lambda_{n-1}^{-1} (2n-1)^{1/2}g_{n-1} +\n\\lambda_{n-3}^{-1} (2n-1)^{1/2} g_{n-3} + \\cdots \\right) ]\n\n\uadf8\ub9ac\uace0 measure\uc5d0 \ub300\ud55c \ud3b8\ubbf8\ubd84\uc740 rectangular function\uc5d0 \ub300\ud55c \ubbf8\ubd84\uacfc \uac19\ub2e4.\n\n[ \\frac{\\partial}{\\partial t} \\omega (t, x) = \\frac{1}{\\theta}\\delta_t -\n\\frac{1}{\\theta} \\delta_{t-\\theta} ]\n\n\uc900\ube44\ubb3c\uc774 \ubaa8\ub450 \uc644\ub8cc\ub418\uc5c8\uae30 \ub54c\ubb38\uc5d0 \uc774\ub97c \ud1b5\ud574 \uc55e\uc11c \uad6c\ud588\ub358 coefficient dynamics\ub97c \ud45c\ud604\ud55c \ubbf8\ubd84 \ubc29\uc815\uc2dd\uc5d0 \ub300\uc785\uc774 \uac00\ub2a5\ud558\ub2e4.\n\n[ \\frac{d}{dt}c_n(t) = -\\frac{\\lambda_n}{\\theta} (2n+1)^{1/2} \\sum_{k=0}^{N-1}\nM_{nk} (2k+1)^{1/2} \\frac{c_k(t)}{\\lambda_k} + (2n+1)^{1/2}\n\\frac{\\lambda_n}{\\theta} f(t) ]\n\n\uc774\uba70 \uc774 \ub54c $M_{nk}$\ub294 $k$\uac00 $n$\ubcf4\ub2e4 \uc791\uac70\ub098 \uac19\uc73c\uba74 \ubb34\uc870\uac74 $1$\uc774\uace0 $k$\uac00 $n$\ubcf4\ub2e4 \ud06c\uba74 $(-1)^{n-k}$\uc758 \uac12\uc744\n\uac00\uc9c0\ub294 value\uc774\ub2e4. \uc774\uc81c \uc784\uc758\ub85c \uc815\ud574\uc904 \uc218 \uc788\ub294 $\\lambda_n = (2n+1)^{1/2}(-1)^n$\ub97c \uc801\uc6a9\ud558\uba74\n\n[ \\frac{d}{dt} c(t) = -\\frac{1}{\\theta} Ac(t) + \\frac{1}{\\theta} B f(t) ]\n\n\uc758 \uc218\uc2dd\uc5d0\uc11c\n\n[ A_{nk} = (2n+1)\\begin{cases} (-1)^{n-k}& \\text{if }k < n \\newline 1 &\n\\text{if }k \\ge n \\end{cases},~~B_n = (2n+1)(-1)^n ]\n\n\uc55e\uc11c \uc18c\uac1c\ud588\ub358 LMU\uac00 \uadf8\ub300\ub85c \ub098\uc624\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.\n\n* * *\n\n# LSSL \ud574\uc11d\ud574\ubcf4\uae30\n\n\ub2e4\uc2dc LSSL\ub85c \ub3cc\uc544\uc640\uc11c Fixed state space representation $A, B, C, D$\uac00 \uc8fc\uc5b4\uc9c4 \uc0c1\ud669\uc744 \uac00\uc815\ud574\ubcf4\uc790.\n\uac04\ub2e8\ud558\uac8c\ub3c4 LSSL\uc740 input sequence\ub97c output sequence\ub85c \ub9e4\ud551\ud558\ub294 \uacfc\uc815\uc774 \ub41c\ub2e4. LSSL\ub294 \uc774\ub7ec\ud55c \ub9e4\ud551 \uacfc\uc815\uc5d0\uc11c\n\ud30c\ub77c\ubbf8\ud130 \ud589\ub82c $A, B, C, D$ \uadf8\ub9ac\uace0 discretize\uc5d0 \ud544\uc218\uc801\uc778 $\\Delta t$\ub85c \uc815\uc758\ub41c\ub2e4. \uc774\uc81c \uc774\ub7ec\ud55c LSSL\uc774 \ub300\uccb4\n\uc5b4\ub5bb\uac8c RNN, CNN \uadf8\ub9ac\uace0 Neural ODE\uc758 \ubaa8\ub4e0 \ud2b9\uc9d5\uc744 \uac00\uc9c8 \uc218 \uc788\ub294\uc9c0 \ud574\uc11d\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\ub2e4.\n\n### LSSL to RNN\n\nLSSL\uc5d0\uc11c\uc758 recurrent state\ub294 \uac01 time step$t$$x_{t-1}$\uc5d0 \ud574\ub2f9\ud55c\ub2e4.", "start_char_idx": 17107, "end_char_idx": 18707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9": {"__data__": {"id_": "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "81d1c5f77aa206a07b835813dec565a172bd99baad3e3ba910cfd8bf28a53e77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62484605-dde2-4655-90f8-4ed35db10f1b", "node_type": "1", "metadata": {}, "hash": "db0b555ea671cbe2dcbc95998a6a08aca7553b84f99d2918e5c4eaee7423db0b", "class_name": "RelatedNodeInfo"}}, "text": "\uac04\ub2e8\ud558\uac8c\ub3c4 LSSL\uc740 input sequence\ub97c output sequence\ub85c \ub9e4\ud551\ud558\ub294 \uacfc\uc815\uc774 \ub41c\ub2e4. LSSL\ub294 \uc774\ub7ec\ud55c \ub9e4\ud551 \uacfc\uc815\uc5d0\uc11c\n\ud30c\ub77c\ubbf8\ud130 \ud589\ub82c $A, B, C, D$ \uadf8\ub9ac\uace0 discretize\uc5d0 \ud544\uc218\uc801\uc778 $\\Delta t$\ub85c \uc815\uc758\ub41c\ub2e4. \uc774\uc81c \uc774\ub7ec\ud55c LSSL\uc774 \ub300\uccb4\n\uc5b4\ub5bb\uac8c RNN, CNN \uadf8\ub9ac\uace0 Neural ODE\uc758 \ubaa8\ub4e0 \ud2b9\uc9d5\uc744 \uac00\uc9c8 \uc218 \uc788\ub294\uc9c0 \ud574\uc11d\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\ub2e4.\n\n### LSSL to RNN\n\nLSSL\uc5d0\uc11c\uc758 recurrent state\ub294 \uac01 time step$t$$x_{t-1}$\uc5d0 \ud574\ub2f9\ud55c\ub2e4. \ud604\uc7ac state $x_t$ \uadf8\ub9ac\uace0\noutput $y_t$\ub294 \uc774\uc0b0\ud654\ub41c LSSL formulation\uc5d0 \uc758\ud574 \uacc4\uc0b0\ub41c\ub2e4.\n\n[ \\begin{aligned} x_t =& \\bar{A}x_{t-1} + \\bar{B}u_t \\newline y_t =& Cx_t +\nDu_t \\end{aligned} ]\n\n\ub530\ub77c\uc11c RNN \uad6c\uc870\uc640 \uac19\uc774 \ub3d9\uc791\ud558\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \uc2ec\uc9c0\uc5b4 RNN \uad6c\uc870\uc5d0\uc11c\uc758 gated recurrence \ub3c4 \ub9cc\uc871\ud55c\ub2e4. \uc608\ucee8\ub370 1\ucc28\uc6d0\uc758\ngated recurrence \uad6c\uc870 $(1-\\sigma (z))x_{t-1} + \\sigma(z) u_t$\ub294 backward-Euler\nmethod\ub85c $\\dot{x}(t) = -x(t) + u(t)$\ub97c \uc774\uc0b0\ud654\ud55c \uac83\uacfc \ub3d9\uc77c\ud558\ub2e4. $z$\ub294 \uc784\uc758\uc758 expression\uc774 \ubaa8\ub450\n\uac00\ub2a5\ud55c\ub370, sigmoid function \ud2b9\uc131\uacfc \uc55e\uc11c \uc18c\uac1c\ud55c GBT\ub97c \uc0dd\uac01\ud558\uba74 $\\Delta t = \\exp (z)$\ub85c \ud45c\ud604\ud588\uc744\ub54c gated\nrecurrence\uac00 $A = -1, B = 1$\uc778 backward-Euler method\uc784\uc744 \uc99d\uba85\ud560 \uc218 \uc788\ub2e4. \uadf8\ub7f0\ub370 \uc5ec\uae30\uc11c \uc758\ubb38\uc774 \uc0dd\uae38\n\uc218 \uc788\ub294 \uc810\uc740, Linear system\uc5d0\uc11c \uad6c\ucd95\ud55c state layer\uac00 \uacfc\uc5f0 \uc77c\ubc18\uc801\uc778 deep RNN\uc774 \uac00\uc9c0\ub294 non-linearity\n\ubc0f \ubcf5\uc7a1\ub3c4\ub97c \ud45c\ud604\ud560 \uc218 \uc788\ub294\uac00\uc5d0 \ub300\ud55c \ubb38\uc81c\uc774\ub2e4.\n\n\uc55e\uc11c \ub2e8\uc21c\ud788 $\\dot{x}(t) = -x(t) + u(t)$\uc758 \uc774\uc0b0\ud654\uc5d0 \ub300\ud574 \uc5b8\uae09\ud588\uc5c8\ub294\ub370, \uc774\ub97c \ub2e4\ub974\uac8c \ud574\uc11d\ud574\uc11c _Picard\niteration_ \uc744 \uc0ac\uc6a9\ud55c\ub2e4\uace0 \uc0dd\uac01\ud558\uba74, \uacb0\uad6d deep RNN\uc740 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c _Picard iteration_ \uc744 \uac70\uce58\uba74\uc11c \ud568\uc218\ub97c\n\ucc3e\uc544\uac04\ub2e4\uace0 \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4. \uc989, \ub9cc\uc57d linear recurrence\uac00 \uc544\ub2cc non-linear recurrence\ub97c \uc0ac\uc6a9\ud55c\ub2e4\uba74 LSSL\n\ub610\ud55c non-linearity\ub97c \ud559\uc2b5\ud560 \uc218 \uc788\uac8c \ub41c\ub2e4. \uc774\ub97c \ud1b5\ud574 RNN \uad6c\uc870\uc640 LSSL\ub294 \ud544\uc694\ucda9\ubd84 \uad00\uacc4\uc5d0 \ub193\uc5ec\uc788\ub2e4\uace0 \ubcfc \uc218 \uc788\ub2e4.\n\n### LSSL to CNN\n\n\uac04\ub2e8\ud55c \uc0c1\ud669\uc744 \uac00\uc815\ud558\uae30 \uc704\ud574 initial state\ub97c $0$\uc774\ub77c \uac00\uc815\ud574\ubcf4\uc790. \uadf8\ub807\uac8c \ub418\uba74 Linear state system\uc744 \ud480\uc5b4\ub0b8\noutput\uc744\n\n[ y_k = C(\\bar{A})^k\\bar{B}u_0 + C(\\bar{A})^{k-1}\\bar{B}u_1 + \\cdots +\nC\\bar{A} \\bar{B}u_{k-1} + \\bar{B}u_k + Du_k ]\n\n\uc774\ucc98\ub7fc \uc815\ub9ac\ud560 \uc218 \uc788\uc73c\uba70, \uc774\ub294 \uace7 discrete-time convolution\uc73c\ub85c \ud45c\ud604 \uac00\ub2a5\ud558\ub2e4.", "start_char_idx": 18425, "end_char_idx": 19942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "62484605-dde2-4655-90f8-4ed35db10f1b": {"__data__": {"id_": "62484605-dde2-4655-90f8-4ed35db10f1b", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "53e212a7a82cb16a3bbe9278d3b3d3c294e85243cd3b72f4b88f90aa346286a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "048a2342-c722-4cdd-a21d-2dc72b6ff821", "node_type": "1", "metadata": {}, "hash": "4b4ef84d72e8ec8aff2384f3f5d1ffdcc499400d52911fb5ddc41a66bb8a32a0", "class_name": "RelatedNodeInfo"}}, "text": "\uc774\ub97c \ud1b5\ud574 RNN \uad6c\uc870\uc640 LSSL\ub294 \ud544\uc694\ucda9\ubd84 \uad00\uacc4\uc5d0 \ub193\uc5ec\uc788\ub2e4\uace0 \ubcfc \uc218 \uc788\ub2e4.\n\n### LSSL to CNN\n\n\uac04\ub2e8\ud55c \uc0c1\ud669\uc744 \uac00\uc815\ud558\uae30 \uc704\ud574 initial state\ub97c $0$\uc774\ub77c \uac00\uc815\ud574\ubcf4\uc790. \uadf8\ub807\uac8c \ub418\uba74 Linear state system\uc744 \ud480\uc5b4\ub0b8\noutput\uc744\n\n[ y_k = C(\\bar{A})^k\\bar{B}u_0 + C(\\bar{A})^{k-1}\\bar{B}u_1 + \\cdots +\nC\\bar{A} \\bar{B}u_{k-1} + \\bar{B}u_k + Du_k ]\n\n\uc774\ucc98\ub7fc \uc815\ub9ac\ud560 \uc218 \uc788\uc73c\uba70, \uc774\ub294 \uace7 discrete-time convolution\uc73c\ub85c \ud45c\ud604 \uac00\ub2a5\ud558\ub2e4.\n\n[ \\begin{aligned} &y = \\mathcal{K}_L (\\bar{A}, \\bar{B}, C) \\ast u + Du\n\\newline &\\mathcal{K}_L (\\bar{A}, \\bar{B}, C) = (CA^iB)_{i \\in [L]} \\in\n\\mathbb{R}^L \\end{aligned} ]\n\n\ub530\ub77c\uc11c LSSL\uc740 output\uc774 convolution\uc5d0 \uc758\ud574 \uc5f0\uc0b0\ub418\ub294 \ubaa8\ub378\ub85c \ud574\uc11d \uac00\ub2a5\ud558\uba70, \ucf58\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc740 FFT\ub85c \uac00\uc18d\ud654\uac00 \uac00\ub2a5\ud558\ub2e4.\n\n\uc77c\ubc18\uc801\uc778 continous state-space system\uc758 \uad00\uc810\uc5d0\uc11c output $y$\ub294 input $u$\uc5d0 \ub300\ud574 \uc2dc\uc2a4\ud15c\uc758 impulse\nresponse function $h$\uc640\uc758 \ucf58\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \ud45c\ud604\ub41c\ub2e4.\n\n[ y(t) = \\int h(\\tau)u(t-\\tau) d\\tau ]\n\n\uc774\uc640\ub294 \uc870\uae08 \ub2e4\ub974\uac8c, convolutional filter\uac00 \ub9cc\uc57d rational functional degree ($N$)\ub97c \uac00\uc9c0\ub294 \uacbd\uc6b0,\n\ud06c\uae30\uac00 $N$\uc778 state-space model\ub85c \ud544\ud130\ub97c \ub098\ud0c0\ub0bc \uc218 \uc788\ub2e4. \uae30\uc874 \uc5f0\uad6c\ub4e4\uc5d0\uc11c \ubc1d\ud614\ub358 \uc810\uc744 \ud1a0\ub300\ub85c \uc784\uc758\uc758\nconvolutional filter $h$\ub294 \uc720\ud55c\ud55c degree \uac12\uc744 \uac00\uc9c0\ub294 rational function\uc73c\ub85c \ud45c\ud604\uc774 \uac00\ub2a5\ud558\ub2e4. \uc55e\uc11c\n\ubd24\ub358 HiPPO matrix\uc758 \ucf00\uc774\uc2a4\ub97c \uc608\ub85c \ub4e4\uc5b4\ubcf4\ub3c4\ub85d \ud558\uc790. \ud544\uc694\ud55c \uc0ac\uc804\uc9c0\uc2dd\uc744 \uc815\ub9ac\ud560 \ub54c Translated Legendre\uc758 \uacbd\uc6b0\ub97c \ubcf4\uac8c\n\ub418\uba74, $A$\ub294 \ud2b9\uc815 \uad6c\uac04($\\theta$) \ub0b4\uc5d0\uc11c \ub3d9\uc77c\ud55c \ud655\ub960 \ubd84\ud3ec\ub97c \uac00\uc9c0\ub294 measure\uc5d0\uc11c \uc815\uc758\ub418\uc5c8\ub2e4. \uc77c\ubc18\uc801\uc778 LSSL\uc5d0\uc11c $dt$\ub97c\n\uace0\uc815\uc2dc\ucf1c\uc11c \uc0dd\uac01\ud588\uc744 \ub54c, \uccab\ubc88\uc9f8 \uc2dd\uc778\n\n[ \\dot{x}(t) = Ax(t) + Bu(t) ]\n\n\uc740 history element\ub97c \uae30\uc5b5\ud558\ub294 \uacfc\uc815\uc5d0 \ud574\ub2f9\ub418\uace0 \ub450\ubc88\uc9f8 \uc2dd\uc778\n\n[ y(t) = Cx(t) + Du(t) ]\n\n\uc740 \ud574\ub2f9 \uc708\ub3c4\uc6b0 \ub0b4\uc5d0\uc11c \uc720\uc758\ubbf8\ud55c feature\ub97c \ubf51\ub294 \uc791\uc5c5\uc774\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 LSSL\uc740 \uacb0\uad6d width\uac00 \ud559\uc2b5 \uac00\ub2a5\ud55c\nconvolutional kernel filter\ub97c \ud559\uc2b5\ud558\ub294 \uacfc\uc815\uacfc \ub3d9\uce58\ub77c\uace0 \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4.\n\n* * *\n\n# Deep Linear State-System Layers\n\n\uc77c\ubc18\uc801\uc778 LSSL\uc740 \uac04\ub2e8\ud558\uac8c \uc694\uc57d\ud558\uba74 \uc785\ub825 \uc2dc\ud000\uc2a4\ub97c \ucd9c\ub825 \uc2dc\ud000\uc2a4\ub85c \ub9e4\ud551\ud558\ub294 \uc2dc\uc2a4\ud15c\uc774\uc5c8\ub2e4. \uc608\ucee8\ub370 \uae38\uc774\uac00 $L$\uc778 \uc2e0\ud638\uac00 \uc788\ub2e4\uba74, LSSL\uc740\n$\\mathbb{R}^L \\rightarrow \\mathbb{R}^L$\uc744 \uc218\ud589\ud558\ub294 \ud558\ub098\uc758 vec to vec \ud568\uc218 \uad6c\uc870\uc774\uba70 \uc774\ub54c \ud568\uc218 \uc790\uccb4\ub294\nparameterized \ub418\uc5b4\uc788\ub2e4.", "start_char_idx": 19622, "end_char_idx": 21195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "048a2342-c722-4cdd-a21d-2dc72b6ff821": {"__data__": {"id_": "048a2342-c722-4cdd-a21d-2dc72b6ff821", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62484605-dde2-4655-90f8-4ed35db10f1b", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "1f2f99fb17816bd5f9aa694ffd4b79e95714cb71b51d83b8c690c7f9fa854898", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "node_type": "1", "metadata": {}, "hash": "aa29bbf459126ac5555737304aaf275ee05ccb5e8eb9ccaf188765d1a81bbdf7", "class_name": "RelatedNodeInfo"}}, "text": "\uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 LSSL\uc740 \uacb0\uad6d width\uac00 \ud559\uc2b5 \uac00\ub2a5\ud55c\nconvolutional kernel filter\ub97c \ud559\uc2b5\ud558\ub294 \uacfc\uc815\uacfc \ub3d9\uce58\ub77c\uace0 \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4.\n\n* * *\n\n# Deep Linear State-System Layers\n\n\uc77c\ubc18\uc801\uc778 LSSL\uc740 \uac04\ub2e8\ud558\uac8c \uc694\uc57d\ud558\uba74 \uc785\ub825 \uc2dc\ud000\uc2a4\ub97c \ucd9c\ub825 \uc2dc\ud000\uc2a4\ub85c \ub9e4\ud551\ud558\ub294 \uc2dc\uc2a4\ud15c\uc774\uc5c8\ub2e4. \uc608\ucee8\ub370 \uae38\uc774\uac00 $L$\uc778 \uc2e0\ud638\uac00 \uc788\ub2e4\uba74, LSSL\uc740\n$\\mathbb{R}^L \\rightarrow \\mathbb{R}^L$\uc744 \uc218\ud589\ud558\ub294 \ud558\ub098\uc758 vec to vec \ud568\uc218 \uad6c\uc870\uc774\uba70 \uc774\ub54c \ud568\uc218 \uc790\uccb4\ub294\nparameterized \ub418\uc5b4\uc788\ub2e4. \ub9cc\uc57d LSSL\uc744 $\\psi$\ub77c\uace0 \ud55c\ub2e4\uba74,\n\n[ \\psi(\\cdot \\vert A, B, C, D, \\Delta t),~A \\in \\mathbb{R}^{N \\times N},~B \\in\n\\mathbb{R}^{N \\times 1},~C \\in \\mathbb{R}^{1 \\times N},~D \\in \\mathbb{R}^{1\n\\times 1} ]\n\n\uc774\ucc98\ub7fc \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \uc55e\uc11c \uc5b8\uae09\ud588\ub358 \uac83\ucc98\ub7fc \ub2e8\uc77c LSSL\uc740 Recurrence, Convolution\uc758 \ud2b9\uc9d5\uc744 \ubaa8\ub450 \uac00\uc9c0\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0\nRNN\uacfc CNN\uc758 \ub300\ud45c\uc801\uc778 \ub808\uc774\uc5b4\uc778 recurent unit\uc774\ub098 convolution kernel\ucc98\ub7fc \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4.\n\n\ub610\ud55c \uc785\ub825 \uc2dc\ud000\uc2a4\uac00 transformer\uc758 input\ucc98\ub7fc $H$\uc758 hidden dimension\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4\uace0 \ud558\uba74($L \\times\nH$), LSSL\uc740 $H$\ub9cc\ud07c\uc758 LSSL\uc744 \ub3c5\ub9bd\uc801\uc73c\ub85c \ud559\uc2b5\ud558\uac8c \ub418\uace0, Transformer\uc758 multi-head \ud6a8\uacfc \ub610\ud55c \uadf8\ub300\ub85c \uc801\uc6a9\ud560 \uc218\n\uc788\ub2e4.\n\n\ub9d0\ud558\uace0\uc790 \ud588\ub358 \uac83\uc740 LSSL\ub97c stacking\ud558\ub294 \uacfc\uc815\uc73c\ub85c \uae30\uc874 DNN \ubc29\ubc95\ub860\uacfc \uac19\uc774 \ub2e4\uc591\ud55c \ud568\uc218\ub97c \ubaa8\uc0ac\ud560 \uc218 \uc788\uc73c\uba70 \ub3d9\uc2dc\uc5d0\nnormalization, residual connection\uacfc \uac19\uc740 \ubc29\ubc95\ub860\uacfc \ud568\uaed8 \ubaa8\ub378\ub9c1\ub420 \uc218 \uc788\ub2e4\ub294 \uc0ac\uc2e4\uc774\ub2e4.\n\n* * *\n\n# LSSL\uacfc Continuous-time Memorization\n\nLSSL\uc774 \uae30\uc874 DNN \ubaa8\ub378\ub9c1\uc758 \ud2b9\uc9d5\uc744 \uc0b4\ub9ac\uba74\uc11c \uc0ac\uc6a9\ub420 \uc218 \uc788\ub2e4\uace0 \ud574\uc11c \ubb34\uc791\uc815 \uc0ac\uc6a9\ud560 \uc218\ub294 \uc5c6\ub294 \ub178\ub987\uc774\uace0, LSSL\uc774 \uc7a5\uc810\uc744 \ubcf4\uc77c \uc218 \uc788\uc5b4\uc57c\n\ud55c\ub2e4.\n\n### Long dependency into LSSLs\n\nDiscretized Linear system ODE\uc5d0\uc11c, \uc2dc\uc2a4\ud15c\uc740 \uc774\uc0b0\ud654\ub41c parameter $\\bar{A}$\uac00 \uacc4\uc18d \uacf1\ud574\uc9c0\uba70 \ubc1c\uc804\ud574\uac04\ub2e4.\n\n[ x_t = \\bar{A}x_{t-1} + \\bar{B}u_t ]\n\n\uadf8 \ub9d0\uc740 gradient descent\ub85c \ud559\uc2b5\ud558\uac8c \ub418\uba74vanishing gradient \ubb38\uc81c\ub97c \ud53c\ud560 \uc218 \uc5c6\ub2e4\ub294 \uac83\uc774\ub2e4. \uc774\ucc98\ub7fc \ub9cc\uc57d $A$\ub97c\n\ub79c\ub364\ud558\uac8c \ucd08\uae30\ud654\ud55c \ud6c4 \ud559\uc2b5\ud558\ub294 \ud615\ud0dc\ub97c \uc0ac\uc6a9\ud558\uba74, \uae30\ub300\ud558\ub294 \uc131\ub2a5\uc774 \ub098\uc624\uc9c0 \uc54a\uc744 \uac83\uc774\ub77c\ub294 \ub9d0\uc774 \ub41c\ub2e4.\n\n\ud558\uc9c0\ub9cc HiPPO\uc640 \uac19\uc740 framework\uc5d0\uc11c\ub294 measure $\\omega$\uc5d0 \ub530\ub77c \uc5b4\ub5a4 \ubc29\uc2dd\uc73c\ub85c \uc774\uc804 function\uc744 \uae30\uc5b5\ud560 \uc9c0\uc5d0 \ub300\ud55c\n\ubb38\uc81c\ub97c \uc5b8\uae09\ud588\uc5c8\ub2e4 (projection/coefficient\ud654 \uacfc\uc815\uc744 \ud1b5\ud574). \uadf8\ub7ec\ub098 HiPPO\uc758 \ubb38\uc81c\uc810\uc774\ub77c\uace0 \ud55c\ub2e4\uba74 \uc774\ub807\uac8c \ub9e4\ub274\uc5bc\ud558\uac8c \uc815\ud55c\nhippo matrix\ub97c \ud559\uc2b5\ud558\uc9c0 \ubabb\ud558\uace0 \uadf8\ub300\ub85c \uc0ac\uc6a9\ud574\uc57c\ud55c\ub2e4\ub294 \uc810\uc774\ub2e4.", "start_char_idx": 20892, "end_char_idx": 22441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "30b8ceed-c561-4ca1-b92c-bb404d6a63fe": {"__data__": {"id_": "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "048a2342-c722-4cdd-a21d-2dc72b6ff821", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "964ee75cff2d1eb637fa39e53e3423fadb5e3479f2f90f021847031d251a2a85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f61e087-53ee-4370-9d00-16c6584e5618", "node_type": "1", "metadata": {}, "hash": "4e9f1d5123d555f92826812c284dcfd139f53959302e897d0ee2a17dd2b840da", "class_name": "RelatedNodeInfo"}}, "text": "\uc774\ucc98\ub7fc \ub9cc\uc57d $A$\ub97c\n\ub79c\ub364\ud558\uac8c \ucd08\uae30\ud654\ud55c \ud6c4 \ud559\uc2b5\ud558\ub294 \ud615\ud0dc\ub97c \uc0ac\uc6a9\ud558\uba74, \uae30\ub300\ud558\ub294 \uc131\ub2a5\uc774 \ub098\uc624\uc9c0 \uc54a\uc744 \uac83\uc774\ub77c\ub294 \ub9d0\uc774 \ub41c\ub2e4.\n\n\ud558\uc9c0\ub9cc HiPPO\uc640 \uac19\uc740 framework\uc5d0\uc11c\ub294 measure $\\omega$\uc5d0 \ub530\ub77c \uc5b4\ub5a4 \ubc29\uc2dd\uc73c\ub85c \uc774\uc804 function\uc744 \uae30\uc5b5\ud560 \uc9c0\uc5d0 \ub300\ud55c\n\ubb38\uc81c\ub97c \uc5b8\uae09\ud588\uc5c8\ub2e4 (projection/coefficient\ud654 \uacfc\uc815\uc744 \ud1b5\ud574). \uadf8\ub7ec\ub098 HiPPO\uc758 \ubb38\uc81c\uc810\uc774\ub77c\uace0 \ud55c\ub2e4\uba74 \uc774\ub807\uac8c \ub9e4\ub274\uc5bc\ud558\uac8c \uc815\ud55c\nhippo matrix\ub97c \ud559\uc2b5\ud558\uc9c0 \ubabb\ud558\uace0 \uadf8\ub300\ub85c \uc0ac\uc6a9\ud574\uc57c\ud55c\ub2e4\ub294 \uc810\uc774\ub2e4. \uc65c\ub0d0\ud558\uba74 HiPPO\uc5d0\uc11c\ub294 \ub974\uc7a5\ub4dc\ub974\ub97c \ud3ec\ud568\ud55c \uc77c\ubd80 measure\uc5d0\n\ub300\ud574\uc11c\ub9cc \uc774\ub97c \ud480\uc5b4\ub0bc \uc218 \uc788\ub294 structured solution matrix $A$\uac00 \uc874\uc7ac\ud588\uace0, \ubaa8\ub4e0 \uc77c\ubc18\uc801\uc778measure\uc5d0\ub3c4 \ub2e4\ub978 \ud615\ud0dc\uc758\n$A$\uac00 \uc874\uc7ac\ud560 \uc218 \uc788\ub2e4\ub294 \uc0ac\uc2e4\uc744 \ubc1d\ud788\uc9c0 \ubabb\ud588\uae30 \ub54c\ubb38\uc774\ub2e4.\n\n\ub530\ub77c\uc11c LSSL\uc5d0\uc11c\ub294 \uc774\ub97c arbitrary measure $\\omega$\ub85c \ud655\uc7a5\uc2dc\ud0a4\uace0, \uc774\ub54c Low-recurrence width $A$\uc5d0\n\ub300\ud55c \ubbf8\ubd84 \ubc29\uc815\uc2dd\uc744 \ucc3e\uc744 \uc218 \uc788\ub2e4\uace0 \uc99d\uba85\ud558\uc600\ub2e4.\n\n### Efficient Algorithms for LSSLs\n\n\uadf8\ub7ec\ub098 A\uc640 $\\Delta t$\uac00 \uc0c1\ub2f9\ud788 \uc911\uc694\ud55c parameter\uc784\uc774 \ub4dc\ub7ec\ub0ac\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, naive LSSL\uc5d0\uc11c\ub294 \ud559\uc2b5\ud558\uae30 \uc5b4\ub835\ub2e4\ub294\n\ubb38\uc81c\uac00 \ubc1c\uc0dd\ud55c\ub2e4. LSSL\uc740 MVM(Matrix Vector Multiplication) \uadf8\ub9ac\uace0 Krylov function\uc744 \uc5f0\uc0b0\ud560 \ub54c\n(\uac01\uac01 convolution/recurrence\uc5d0 \ud574\ub2f9) \uc804\uc790\uc758 \uacbd\uc6b0\uc5d0\ub294 matrix inversion\uc774 \ud544\uc694\ud558\ub2e4\ub294 \uc5b4\ub824\uc6c0\uacfc,\n\n[ x(t+\\Delta t) = (I-\\alpha \\Delta t \\cdot A)^{-1}(I+(1-\\alpha)\\Delta t \\cdot\nA)x(t) +\\Delta t(I-\\alpha \\Delta t \\cdot A)^{-1}B \\cdot u(t) ]\n\n\ud6c4\uc790\ub294 $\\bar{A}$\ub97c feautre\uc758 \uae38\uc774\uc778 $L$\ub9cc\ud07c \uacf1\ud574\uc57c \ud55c\ub2e4\ub294 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud55c\ub2e4.\n\n[ \\mathcal{K}_L (\\bar{A}, \\bar{B}, C) = (CB, CAB, \\ldots, CA^{L-1}B) ]\n\n\uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 $A$\ub97c \ud559\uc2b5\ud560 \ub54c\uc758 \ud6a8\uc728\uc131\uc744 \uc99d\ub300\ud558\uae30 \uc704\ud55c \uc870\uac74\uc774 \ud558\ub098 \ub354 \ubc1c\uc0dd\ud55c\ub2e4. \ubaa8\ub4e0 \uae30\uc874\uc758 fixed LSSL\uc758 $A$\ub294\n_3-quasiseparable_ \ud568\uc774 \uc99d\uba85\ub418\uc5c8\ub2e4. \ub9cc\uc57d \ud559\uc2b5\ub418\ub294 $A$ \ub610\ud55c _quasiseparable_ \ud2b9\uc131\uc744 \uc720\uc9c0\ud560 \uacbd\uc6b0, MVM\uacfc\nkrylov function \uc5f0\uc0b0\uc774 \ubcf4\ub2e4 \uc801\uc740 \uc5f0\uc0b0\ub7c9\uc73c\ub85c \ucc98\ub9ac\ub420 \uc218 \uc788\ub2e4.\n\n* * *\n\n# Evaluations and Demonstrations\n\n\uc2e4\uc81c\ub85c \ud2b9\uc815 \uc870\uac74\uc744 \uac00\uc9c0\ub294 $A$\ub97c \ud559\uc2b5\ud560 \uc218 \uc788\uc73c\uba74, \uc774\ub294 \uc774\uc804 HiPPO system $A$\ubcf4\ub2e4 \ub354 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778\ud558\uc600\ub2e4.\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/8eecb1e1-bac6-4e12-bbde-2a5f655a4a65)\n\n\ub610\ud55c \uae38\uc774\uac00 \uae34 \uc74c\uc131 \uc2e0\ud638\uc758 classification \uc131\ub2a5\uc744 \ud1b5\ud574 long time dependency \ub610\ud55c \uc785\uc99d\ud558\uc600\ub2e4.\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/451dfa18-8b16-4813-97a2-b55b5ffa5b84)\n\n\ub610\ud55c \uae30\uc874 SoTA\uc5d0 \ud544\uc801\ud558\ub294 \uc131\ub2a5\uc744 \ubcf4\uc774\uae30\uae4c\uc9c0 \ud559\uc2b5 epoch\uac00 \ud6e8\uc52c \uc801\uc5b4\uc9c8 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.\n\n!", "start_char_idx": 22176, "end_char_idx": 23865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9f61e087-53ee-4370-9d00-16c6584e5618": {"__data__": {"id_": "9f61e087-53ee-4370-9d00-16c6584e5618", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9607249a-1341-472a-81f8-e8c861654e2c", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7af060dac4442737d99b434b43d853f031736895c65aced4b99da1441b3b0d67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0e5ad01e7533878bb1d6d800fb2ef1358d268875f51957430b5c28aab8b8ab91", "class_name": "RelatedNodeInfo"}}, "text": "![](https://github.com/junia3/LayerwiseTTA/assets/79881119/8eecb1e1-bac6-4e12-bbde-2a5f655a4a65)\n\n\ub610\ud55c \uae38\uc774\uac00 \uae34 \uc74c\uc131 \uc2e0\ud638\uc758 classification \uc131\ub2a5\uc744 \ud1b5\ud574 long time dependency \ub610\ud55c \uc785\uc99d\ud558\uc600\ub2e4.\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/451dfa18-8b16-4813-97a2-b55b5ffa5b84)\n\n\ub610\ud55c \uae30\uc874 SoTA\uc5d0 \ud544\uc801\ud558\ub294 \uc131\ub2a5\uc744 \ubcf4\uc774\uae30\uae4c\uc9c0 \ud559\uc2b5 epoch\uac00 \ud6e8\uc52c \uc801\uc5b4\uc9c8 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.\n\n![](https://github.com/junia3/LayerwiseTTA/assets/79881119/1babd539-fdea-42ec-8790-7e9ff95d7526)\n\n* * *\n\n# Conclusion\n\nMamba modeling\uc758 \uac00\uc7a5 \uae30\ucd08\uac00 \ub418\ub294 LSSL\uc744 \uc0b4\ud3b4\ubcf4\uc558\uc73c\uba70, LSSL\uc758 \uc774\ud574\uc5d0\ub294 HiPPO\uc758 \uc774\ud574\uac00 \ud544\uc218\uc801\uc774\uae30 \ub54c\ubb38\uc5d0 \ud574\ub2f9 \ub17c\ubb38\ub3c4\n\ud568\uaed8 \ub2e4\ub8e8\uc5c8\ub2e4. \uc55e\uc73c\ub85c \uba87\uac1c\uc758 \ud3ec\uc2a4\ud305\uc744 \ud1b5\ud574 Mamba\ub97c \ub9ac\ubdf0\ud558\uac8c \ub420\uc9c0\ub294 \ubaa8\ub974\uaca0\uc9c0\ub9cc State Modeling\uc5d0 \ub300\ud574\uc11c\ub294 \uc544\ubb34\ub3c4 \uc81c\ub300\ub85c\n\uc815\ub9ac\ub97c \uc548\ud574\ub193\uc744 \uac83 \uac19\uc544\uc11c..\n\nA n o t h e r p o s t i n c a t e g o r y\n\n[ \u276e\u276e DINOv2(Learning Robust Visual Features without Supervision) \ub17c\ubb38 \ub9ac\ubdf0\n\n](/blog/dino2)\n\n[ \u276f\u276f InfoBatch - Lossless Training Speech-Up By Unbiased Dynamic Data Pruning\n\ub17c\ubb38 \ub9ac\ubdf0\n\n](/blog/infobatch)\n\nBACK TO TOP\n\n[ ![](https://avatars.githubusercontent.com/u/79881119?v=4)\n\n##### JunYoung\n\nYonsei University, Electrical electronic engineering, YAI, Artificial in...\n\n](https://junia3.github.io/#about)\n\n### More Links\n\n* [DEVELOPMENT](https://junia3.github.io/category/development)\n* [GITHUB BLOG](https://junia3.github.io/category/github%20blog)\n* [PAPER REVIEW](https://junia3.github.io/category/paper%20review)\n* [Projects](https://junia3.github.io/project)\n\n### Recent Posts\n\n* [InfoBatch - Lossless Training Speech-Up By Unbiased Dynamic Data Pruning \ub17c\ubb38 \ub9ac\ubdf0](https://junia3.github.io/blog/infobatch)\n* [Mamba modeling\uc758 \uae30\ucd08 (1) - Linear State-Space Layer (LSSL)\uc5d0 \ub300\ud558\uc5ec](https://junia3.github.io/blog/lssl)\n* [DINOv2(Learning Robust Visual Features without Supervision) \ub17c\ubb38 \ub9ac\ubdf0](https://junia3.github.io/blog/dino2)", "start_char_idx": 23540, "end_char_idx": 25204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3f13e06e-f447-4e0e-8452-71d61930bb4e": {"__data__": {"id_": "3f13e06e-f447-4e0e-8452-71d61930bb4e", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4267f7ab-2ce7-48a5-9d2f-aeefc6afe469", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "3fb2c27bc883fda4e89b1055f28e25e9a02a96b5edc851253cd7d2a4c31581a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "865ff251-d290-411e-8d16-5b981da7115c", "node_type": "1", "metadata": {}, "hash": "9853e73e0d600802f27fe14a2c1e7b02de1e5fb73bee1b412da57fe10305d776", "class_name": "RelatedNodeInfo"}}, "text": "[ \ud30c\uc774\ud1a0\uce58 \ud55c\uad6d \uc0ac\uc6a9\uc790 \ubaa8\uc784 ](/)\n\n#  [Mamba: \uc120\ud0dd\uc801 \uc0c1\ud0dc \uacf5\uac04\uc744 \ud65c\uc6a9\ud55c \uc120\ud615 \uc2dc\uac04 \uc2dc\ud000\uc2a4 \ubaa8\ub378\ub9c1 (Linear-Time Sequence Modeling with\nSelective State Spaces)](/t/mamba-linear-time-sequence-modeling-with-\nselective-state-spaces/3043)\n\n[ \uc77d\uc744\uac70\ub9ac&\uc815\ubcf4\uacf5\uc720 ](/c/news/14)\n\n[rnn](https://discuss.pytorch.kr/tag/rnn), [selective-state-space-\nmodels](https://discuss.pytorch.kr/tag/selective-state-space-models),\n[cnn](https://discuss.pytorch.kr/tag/cnn), [long-\nsequence](https://discuss.pytorch.kr/tag/long-sequence), [sequence-\nmodel](https://discuss.pytorch.kr/tag/sequence-model),\n[mamba](https://discuss.pytorch.kr/tag/mamba),\n[paper](https://discuss.pytorch.kr/tag/paper)\n\n[9bow](https://discuss.pytorch.kr/u/9bow) (\ubc15\uc815\ud658)  12\uc6d4 14, 2023, 8:20\uc624\uc804  1\n\n  * _\uc774 \uae00\uc740 GPT \ubaa8\ub378\ub85c \uc790\ub3d9 \uc694\uc57d\ud55c \uc124\uba85\uc73c\ub85c, \uc798\ubabb\ub41c \ub0b4\uc6a9\uc774 \uc788\uc744 \uc218 \uc788\uc73c\ub2c8 \uc6d0\ubb38\uc744 \ucc38\uace0\ud574\uc8fc\uc138\uc694!![:smile:](https://discuss.pytorch.kr/images/emoji/apple/smile.png?v=12)_\n  * _\uc77d\uc73c\uc2dc\uba74\uc11c \uc5b4\uc0c9\ud558\uac70\ub098 \uc798\ubabb\ub41c \ub0b4\uc6a9\uc744 \ubc1c\uacac\ud558\uc2dc\uba74 \ub367\uae00\ub85c \uc54c\ub824\uc8fc\uc2dc\uae30\ub97c \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4!![:bowing_man:](https://discuss.pytorch.kr/images/emoji/apple/bowing_man.png?v=12)_\n\n* * *\n\n# Mamba: \uc120\ud0dd\uc801 \uc0c1\ud0dc \uacf5\uac04\uc744 \ud65c\uc6a9\ud55c \uc120\ud615 \uc2dc\uac04 \uc2dc\ud000\uc2a4 \ubaa8\ub378\ub9c1 (Linear-Time Sequence Modeling with\nSelective State Spaces)\n\n![Mamba: \uc120\ud0dd\uc801 \uc0c1\ud0dc \uacf5\uac04\uc744 \ud65c\uc6a9\ud55c \uc120\ud615 \uc2dc\uac04 \uc2dc\ud000\uc2a4 \ubaa8\ub378\ub9c1 \\(Linear-Time Sequence Modeling with\nSelective State\nSpaces\\)](https://discuss.pytorch.kr/uploads/default/original/2X/0/0e44b2dc7743afd7b3e780d4b358bb81df76d8f1.jpeg)\n\n## \uac1c\uc694\n\nMamba(\ub9d8\ubc14)\ub294 \uc5b8\uc5b4 \ubaa8\ub378\ub9c1\uacfc \uac19\uc774 \uc815\ubcf4 \ubc00\ub3c4\uac00 \ub192\uc740 \ub370\uc774\ud130\uc5d0 \ub300\ud574 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uc0c8\ub85c\uc6b4 \uc0c1\ud0dc \uacf5\uac04 \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\uc785\ub2c8\ub2e4. \uc774\ub294\n\uad6c\uc870\ud654\ub41c \uc0c1\ud0dc \uacf5\uac04 \ubaa8\ub378\uc744 \ubc14\ud0d5\uc73c\ub85c \ud558\uba70, \ud6a8\uc728\uc801\uc778 \ud558\ub4dc\uc6e8\uc5b4 \uc778\uc2dd \uc124\uacc4\uc640 \uad6c\ud604\uc744 \ud2b9\uc9d5\uc73c\ub85c \ud569\ub2c8\ub2e4\n\n\bMamba\ub294 \uae34 \ub370\uc774\ud130 \uc2dc\ud000\uc2a4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \ubaa8\ub378\ub9c1\ud558\ub294 \uc0c8\ub85c\uc6b4 \uc2e0\uacbd\ub9dd \ubaa8\ub378\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc740 \uae30\uc874 \uc2dc\ud000\uc2a4 \ubaa8\ub378, \ud2b9\ud788\n\ud2b8\ub79c\uc2a4\ud3ec\uba38(Transformer)\uc758 \ud55c\uacc4\ub97c \uadf9\ubcf5\ud558\uae30 \uc704\ud574 \uc124\uacc4\ub41c \uc0c8\ub85c\uc6b4 \uc120\ud0dd\uc801 \uc0c1\ud0dc \uacf5\uac04 \ubaa8\ub378(Selective State Space\nModels, SSMs)\uc785\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc740 \uc21c\ud658 \uc2e0\uacbd\ub9dd(Recurrent Neural Networks, RNNs)\uacfc \ud569\uc131\uacf1\n\uc2e0\uacbd\ub9dd(Convolutional Neural Networks, CNNs)\uc758 \uc870\ud569\uc73c\ub85c, \uace0\uc804\uc801\uc778 \uc0c1\ud0dc \uacf5\uac04 \ubaa8\ub378\uc5d0\uc11c \uc601\uac10\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4\u200b\u200b\u200b\u200b.\n\n## Mamba(\ub9d8\ubc14) \ubaa8\ub378 \uc18c\uac1c\n\n[!", "start_char_idx": 0, "end_char_idx": 1738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "865ff251-d290-411e-8d16-5b981da7115c": {"__data__": {"id_": "865ff251-d290-411e-8d16-5b981da7115c", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4267f7ab-2ce7-48a5-9d2f-aeefc6afe469", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "3fb2c27bc883fda4e89b1055f28e25e9a02a96b5edc851253cd7d2a4c31581a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f13e06e-f447-4e0e-8452-71d61930bb4e", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e3133525acaa0c1e827f17f82ef485ca89ca8a15c2bcf3979f6f2bf39d2adfb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f96fcd1e-edb5-439d-9826-2eb7c0c9ddd4", "node_type": "1", "metadata": {}, "hash": "ef836c70d604bc7cec4f93a59fd3152a5ad3c71eeb4df2b1e56a12833bc7d994", "class_name": "RelatedNodeInfo"}}, "text": "\uc774 \ubaa8\ub378\uc740 \uae30\uc874 \uc2dc\ud000\uc2a4 \ubaa8\ub378, \ud2b9\ud788\n\ud2b8\ub79c\uc2a4\ud3ec\uba38(Transformer)\uc758 \ud55c\uacc4\ub97c \uadf9\ubcf5\ud558\uae30 \uc704\ud574 \uc124\uacc4\ub41c \uc0c8\ub85c\uc6b4 \uc120\ud0dd\uc801 \uc0c1\ud0dc \uacf5\uac04 \ubaa8\ub378(Selective State Space\nModels, SSMs)\uc785\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc740 \uc21c\ud658 \uc2e0\uacbd\ub9dd(Recurrent Neural Networks, RNNs)\uacfc \ud569\uc131\uacf1\n\uc2e0\uacbd\ub9dd(Convolutional Neural Networks, CNNs)\uc758 \uc870\ud569\uc73c\ub85c, \uace0\uc804\uc801\uc778 \uc0c1\ud0dc \uacf5\uac04 \ubaa8\ub378\uc5d0\uc11c \uc601\uac10\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4\u200b\u200b\u200b\u200b.\n\n## Mamba(\ub9d8\ubc14) \ubaa8\ub378 \uc18c\uac1c\n\n[![\ub9d8\ubc14 \ubaa8\ub378 \uad6c\uc870 / Mamba Model\nArchitecture](https://discuss.pytorch.kr/uploads/default/optimized/2X/e/ea19838cee47d85c37798231a9d25cac81589833_2_1028x513.png)\ub9d8\ubc14\n\ubaa8\ub378 \uad6c\uc870 / Mamba Model Architecture1413\u00d7706 91.4\nKB](https://discuss.pytorch.kr/uploads/default/original/2X/e/ea19838cee47d85c37798231a9d25cac81589833.png\n\"\ub9d8\ubc14 \ubaa8\ub378 \uad6c\uc870 / Mamba Model Architecture\")\n\nMamba(\ub9d8\ubc14)\ub294 \uc120\ud0dd\uc801 \uc9d1\uc911\uc744 \ud1b5\ud574 \uc785\ub825\uc5d0 \ub530\ub77c \ud2b9\uc815 \uc815\ubcf4\uc5d0 \uc9d1\uc911\ud558\uac70\ub098 \ubb34\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc785\ub825\uc5d0 \uae30\ubc18\ud558\uc5ec\nSSM(Selective State Space Model) \uac00\uc911\uce58\ub97c \ub9e4\uac1c\ubcc0\uc218\ud654\ud568\uc73c\ub85c\uc368, \ubaa8\ub378\uc774 \uad00\ub828 \uc5c6\ub294 \uc815\ubcf4\ub97c \uac78\ub7ec\ub0b4\uace0 \uad00\ub828 \uc788\ub294 \ub370\uc774\ud130\ub97c\n\ubb34\uae30\ud55c \uc720\uc9c0\ud560 \uc218 \uc788\uac8c \ud569\ub2c8\ub2e4\u200b\u200b.\n\n\ub610\ud55c, Mamba\ub294 \ud558\ub4dc\uc6e8\uc5b4 \uc778\uc2dd \uc54c\uace0\ub9ac\uc998(Hardware-aware Algorithm)\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ud569\uc131\uacf1 \ub300\uc2e0 \uc7ac\uadc0\uc801\uc73c\ub85c\n\uacc4\uc0b0\ud569\ub2c8\ub2e4. \uc774 \uc811\uadfc \ubc29\uc2dd\uc740 \ud655\uc7a5\ub41c \uc0c1\ud0dc\ub97c \uc2e4\uccb4\ud654\ud558\uc9c0 \uc54a\uace0, GPU \uba54\ubaa8\ub9ac \uacc4\uce35 \uac04\uc758 I/O \uc811\uadfc\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc5b4 \uae30\uc874 \ubc29\ubc95\ubcf4\ub2e4 \ube60\ub974\uace0\n\ud6a8\uc728\uc801\uc785\ub2c8\ub2e4\u200b\u200b.\n\n### **\uae34 \uc2dc\ud000\uc2a4 \ucc98\ub9ac \ub2a5\ub825**\n\n\uae30\uc874\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc740 \uc2dc\ud000\uc2a4 \uae38\uc774\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uacc4\uc0b0 \ubcf5\uc7a1\ub3c4\uac00 \uc81c\uacf1\uc73c\ub85c \uc99d\uac00\ud558\ub294 \ubb38\uc81c\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uae34 \uc2dc\ud000\uc2a4\ub97c \ucc98\ub9ac\ud560 \ub54c\n\ube44\ud6a8\uc728\uc801\uc774\uace0, \uc790\uc6d0\uc744 \ub9ce\uc774 \uc18c\ubaa8\ud569\ub2c8\ub2e4. \ub9d8\ubc14\ub294 \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uba70, \uc2dc\ud000\uc2a4 \uae38\uc774\uc5d0 \ub300\ud574 \uc120\ud615\uc801\uc73c\ub85c \uc2a4\ucf00\uc77c\ub9c1\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \ub9d8\ubc14\ub294 \uae34\n\uc2dc\ud000\uc2a4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \ucc98\ub9ac\ud560 \uc218 \uc788\uc73c\uba70, \ud2b9\ud788 \uc5b8\uc5b4, \uc624\ub514\uc624, \uc720\uc804\uccb4\ud559\uacfc \uac19\uc740 \ubd84\uc57c\uc5d0\uc11c \uc911\uc694\ud55c \uc751\uc6a9 \uac00\ub2a5\uc131\uc744 \uac00\uc9d1\ub2c8\ub2e4\u200b\u200b\u200b\u200b.\n\n### **\uacc4\uc0b0 \ud6a8\uc728\uc131 \ubc0f \uc18d\ub3c4**\n\n\ub9d8\ubc14\ub294 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ubcf4\ub2e4 \ube60\ub978 \ucd94\ub860 \uc18d\ub3c4\uc640 \ub354 \uc801\uc740 \uba54\ubaa8\ub9ac \uc694\uad6c\ub7c9\uc744 \uac00\uc9d1\ub2c8\ub2e4. \uc774\ub294 \ub9d8\ubc14\uac00 \uc2e4\uc81c \uc751\uc6a9\uc5d0\uc11c \ub354 \ud6a8\uc728\uc801\uc774\uace0, \ub300\uaddc\ubaa8 \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\uace0\n\ucd94\ub860\ud558\ub294 \ub370 \ud544\uc694\ud55c \ucef4\ud4e8\ud305 \uc790\uc6d0\uc744 \uc808\uc57d\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4\u200b\u200b.\n\n## \uc8fc\uc694 \ud2b9\uc9d5\n\n!", "start_char_idx": 1472, "end_char_idx": 2804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f96fcd1e-edb5-439d-9826-2eb7c0c9ddd4": {"__data__": {"id_": "f96fcd1e-edb5-439d-9826-2eb7c0c9ddd4", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4267f7ab-2ce7-48a5-9d2f-aeefc6afe469", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "3fb2c27bc883fda4e89b1055f28e25e9a02a96b5edc851253cd7d2a4c31581a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "865ff251-d290-411e-8d16-5b981da7115c", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "a1ebd2cc6ed1c84595cb3aba4c704940fb9f84f8aeb77ee9794e63fd98948224", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "805a1371-c5b9-4499-a35a-bce0084e3581", "node_type": "1", "metadata": {}, "hash": "78d02cd80148466ca4d285d3d0107acdc4ed53d056224309e949b604312d4867", "class_name": "RelatedNodeInfo"}}, "text": "### **\uacc4\uc0b0 \ud6a8\uc728\uc131 \ubc0f \uc18d\ub3c4**\n\n\ub9d8\ubc14\ub294 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ubcf4\ub2e4 \ube60\ub978 \ucd94\ub860 \uc18d\ub3c4\uc640 \ub354 \uc801\uc740 \uba54\ubaa8\ub9ac \uc694\uad6c\ub7c9\uc744 \uac00\uc9d1\ub2c8\ub2e4. \uc774\ub294 \ub9d8\ubc14\uac00 \uc2e4\uc81c \uc751\uc6a9\uc5d0\uc11c \ub354 \ud6a8\uc728\uc801\uc774\uace0, \ub300\uaddc\ubaa8 \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\uace0\n\ucd94\ub860\ud558\ub294 \ub370 \ud544\uc694\ud55c \ucef4\ud4e8\ud305 \uc790\uc6d0\uc744 \uc808\uc57d\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4\u200b\u200b.\n\n## \uc8fc\uc694 \ud2b9\uc9d5\n\n![Mamba Selective\nCopying](https://discuss.pytorch.kr/uploads/default/original/2X/1/1c88f99e36ee8edbcfd920a50738a8c148ffed32.png)\n\n### **\uc120\ud0dd\uc801 \uc0c1\ud0dc \uacf5\uac04(Selective State Spaces)**\n\n\ub9d8\ubc14\ub294 \uc785\ub825\uc5d0 \uae30\ubc18\ud558\uc5ec SSM \ub9e4\uac1c\ubcc0\uc218\ub97c \ub9e4\uac1c\ubcc0\uc218\ud654\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378\uc740 \uad00\ub828 \uc5c6\ub294 \uc815\ubcf4\ub97c \uac78\ub7ec\ub0b4\uace0, \ud544\uc694\ud55c \uc815\ubcf4\ub97c \ubb34\uae30\ud55c \uc720\uc9c0\ud560 \uc218\n\uc788\uc2b5\ub2c8\ub2e4. \uc774 \uc120\ud0dd \uba54\ucee4\ub2c8\uc998\uc740 \ub9d8\ubc14\uac00 \uad00\ub828 \uc788\ub294 \ub370\uc774\ud130\uc5d0\ub9cc \uc9d1\uc911\ud560 \uc218 \uc788\uac8c \ud558\uc5ec, \ub370\uc774\ud130 \ucc98\ub9ac \ud6a8\uc728\uc131\uc744 \ub192\uc785\ub2c8\ub2e4\u200b\u200b.\n\n### **\ud558\ub4dc\uc6e8\uc5b4 \uc778\uc2dd \uc54c\uace0\ub9ac\uc998(Hardware-aware Algorithm)**\n\n\ub9d8\ubc14\ub294 \ud569\uc131\uacf1 \ub300\uc2e0 \uc7ac\uadc0\uc801\uc73c\ub85c \uacc4\uc0b0\ud558\ub294 \ud558\ub4dc\uc6e8\uc5b4 \uc778\uc2dd \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774\ub294 GPU \uba54\ubaa8\ub9ac \uacc4\uce35 \uac04\uc758 IO \uc811\uadfc\uc744 \ubc29\uc9c0\ud558\uace0, \ud655\uc7a5\ub41c\n\uc0c1\ud0dc\ub97c \uc2e4\uccb4\ud654\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c, \uc774 \uad6c\ud604\uc740 \uc774\ub860\uc801\uc73c\ub85c(\uc2dc\ud000\uc2a4 \uae38\uc774\uc5d0 \ub530\ub77c \uc120\ud615\uc801\uc73c\ub85c \uc2a4\ucf00\uc77c\ub9c1) \ubc0f \ud604\ub300 \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c(\uc608: A100\nGPU\uc5d0\uc11c \ucd5c\ub300 3\ubc30 \ube60\ub984) \uc774\uc804 \ubc29\ubc95\ubcf4\ub2e4 \ube60\ub985\ub2c8\ub2e4\u200b\u200b.\n\n### **\ub2e8\uc21c\ud654\ub41c \uc544\ud0a4\ud14d\ucc98**\n\n\ub9d8\ubc14\ub294 \uc774\uc804 SSM \uc544\ud0a4\ud14d\ucc98\uc640 \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc758 MLP \ube14\ub85d\uc744 \ub2e8\uc77c \ube14\ub85d\uc73c\ub85c \uacb0\ud569\ud558\uc5ec, \ub354 \ub2e8\uc21c\ud558\uace0 \ud6a8\uc728\uc801\uc778 \uc544\ud0a4\ud14d\ucc98\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub294 \ub9d8\ubc14\ub97c\n\ub354 \uc27d\uac8c \uad6c\ud604\ud558\uace0 \ud655\uc7a5\ud560 \uc218 \uc788\uac8c \ud574\uc8fc\uba70, \ub2e4\uc591\ud55c \uc751\uc6a9 \ubd84\uc57c\uc5d0 \uc801\uc6a9\ud558\uae30\uc5d0 \uc801\ud569\ud569\ub2c8\ub2e4\u200b\u200b.\n\n## SSM\uc758 \uae30\ubcf8 \uac1c\ub150\n\nSSM\uc740 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130(\uc608: \uc2dc\uac04\uc5d0 \ub530\ub77c \ubcc0\ud654\ud558\ub294 \ub370\uc774\ud130)\ub97c \ubaa8\ub378\ub9c1\ud558\uae30 \uc704\ud574 \uc124\uacc4\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4. \uc774 \ubaa8\ub378\ub4e4\uc740 \uc804\ud1b5\uc801\uc778 \uc21c\ud658 \uc2e0\uacbd\ub9dd(RNN)\uacfc\n\ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd(CNN)\uc758 \ud2b9\uc9d5\uc744 \uacb0\ud569\ud558\uba74\uc11c, \uace0\uc804\uc801\uc778 \uc0c1\ud0dc \uacf5\uac04 \ubaa8\ub378\uc5d0\uc11c \uc601\uac10\uc744 \ubc1b\uc544 \uac1c\ubc1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\nSSM\uc740 \uae34 \uc2dc\ud000\uc2a4 \ucc98\ub9ac\uac00 \uac00\ub2a5\ud558\uba70 \ub2e4\uc591\ud55c \ud615\ud0dc\uc758 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ud2b9\uc9d5\uc73c\ub85c \ub2e4\uc591\ud55c \uc544\ud0a4\ud14d\ucc98\uc640 \uacb0\ud569\ud558\uc5ec \uc0c8\ub85c\uc6b4\n\ud615\ud0dc\uc758 \uc2dc\ud000\uc2a4 \ubaa8\ub378\ub9c1 \uc791\uc5c5\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n## \ub354 \uc77d\uc5b4\ubcf4\uae30\n\n### Mamba \ub17c\ubb38\n\n[arxiv.org](https://arxiv.org/pdf/2312.00752.pdf)\n[](https://arxiv.org/pdf/2312.00752.pdf)\n\n### [2312.00752.pdf](https://arxiv.org/pdf/2312.00752.pdf)\n\n1263.54 KB\n\n### GitHub \uc800\uc7a5\uc18c\n\n!", "start_char_idx": 2654, "end_char_idx": 3969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "805a1371-c5b9-4499-a35a-bce0084e3581": {"__data__": {"id_": "805a1371-c5b9-4499-a35a-bce0084e3581", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4267f7ab-2ce7-48a5-9d2f-aeefc6afe469", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "3fb2c27bc883fda4e89b1055f28e25e9a02a96b5edc851253cd7d2a4c31581a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f96fcd1e-edb5-439d-9826-2eb7c0c9ddd4", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "133af860089e9ef445beb2bc4790d995f882e1ee77ca00bd133912bd48bc36aa", "class_name": "RelatedNodeInfo"}}, "text": "SSM\uc740 \uae34 \uc2dc\ud000\uc2a4 \ucc98\ub9ac\uac00 \uac00\ub2a5\ud558\uba70 \ub2e4\uc591\ud55c \ud615\ud0dc\uc758 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ud2b9\uc9d5\uc73c\ub85c \ub2e4\uc591\ud55c \uc544\ud0a4\ud14d\ucc98\uc640 \uacb0\ud569\ud558\uc5ec \uc0c8\ub85c\uc6b4\n\ud615\ud0dc\uc758 \uc2dc\ud000\uc2a4 \ubaa8\ub378\ub9c1 \uc791\uc5c5\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n## \ub354 \uc77d\uc5b4\ubcf4\uae30\n\n### Mamba \ub17c\ubb38\n\n[arxiv.org](https://arxiv.org/pdf/2312.00752.pdf)\n[](https://arxiv.org/pdf/2312.00752.pdf)\n\n### [2312.00752.pdf](https://arxiv.org/pdf/2312.00752.pdf)\n\n1263.54 KB\n\n### GitHub \uc800\uc7a5\uc18c\n\n![](https://github.githubassets.com/favicons/favicon.svg)\n[GitHub](https://github.com/state-spaces/mamba)\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/0/06b9b3d23f9f5ee0f225a00fa7b3bb6357fc7f64.png)\n\n### [GitHub - state-spaces/mamba](https://github.com/state-spaces/mamba)\n\nContribute to state-spaces/mamba development by creating an account on GitHub.\n\n### \ud559\uc2b5\ub41c \ubaa8\ub378 \uac00\uc911\uce58 \ub2e4\uc6b4\ub85c\ub4dc at\n![:hugs:](https://discuss.pytorch.kr/images/emoji/apple/hugs.png?v=12)HuggingFace\n\n[huggingface.co](https://huggingface.co/state-spaces)\n\n![](https://discuss.pytorch.kr/uploads/default/original/2X/4/48be4a21f31983ef1644ee17c8a6e5222ea0ad9a.png)\n\n### [state-spaces (State Space Models)](https://huggingface.co/state-spaces)\n\nOrg profile for State Space Models on Hugging Face, the AI community building\nthe future.\n\n1\uac1c\uc758 \uc88b\uc544\uc694\n\n[Vision Mamba(Vim): \uc591\ubc29\ud5a5 \uc0c1\ud0dc \uacf5\uac04 \ubaa8\ub378(SSM)\uc744 \ud65c\uc6a9\ud55c \ud6a8\uc728\uc801 \uc2dc\uac01 \ud45c\ud604\n\ud559\uc2b5](https://discuss.pytorch.kr/t/vision-mamba-vim-ssm/3423)\n\n[[GN\u207a] Mamba: \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc5d0 \ub3c4\uc804\ud558\ub294 \uc0c1\ud0dc-\uacf5\uac04 \ubaa8\ub378(SSM)](https://discuss.pytorch.kr/t/gn-\nmamba-ssm/3640)\n\n[[GN] 2023\ub144\uc740 \uc624\ud508 LLM\uc758 \ud574](https://discuss.pytorch.kr/t/gn-2023-llm/3087)\n\n  * [\ud648 ](/)\n  * [\uce74\ud14c\uace0\ub9ac ](/categories)\n  * [FAQ/\uac00\uc774\ub4dc\ub77c\uc778 ](/guidelines)\n  * [\uc774\uc6a9\uc57d\uad00 ](/tos)\n  * [\uac1c\uc778\uc815\ubcf4 \ucde8\uae09\ubc29\uce68 ](/privacy)\n\n[Discourse](https://www.discourse.org)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. JavaScript\uac00 \ud65c\uc131\ud654\ub41c \uc0c1\ud0dc\uc5d0\uc11c \uac00\uc7a5 \uc798\n\ubcf4\uc785\ub2c8\ub2e4.", "start_char_idx": 3657, "end_char_idx": 5253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ee8e54f9-7071-442a-8c77-854a7ae27118": {"__data__": {"id_": "ee8e54f9-7071-442a-8c77-854a7ae27118", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0800fd72-b783-4cda-882b-6e66085ad6b9", "node_type": "1", "metadata": {}, "hash": "7e3fdfa77dbe5361584714ecada520ed6bbb08c63cb7b29162a80a042ae353a1", "class_name": "RelatedNodeInfo"}}, "text": "| [![](y18.svg)](https://news.ycombinator.com) | **[Hacker News](news)**\n[new](newest) | [past](front) | [comments](newcomments) | [ask](ask) |\n[show](show) | [jobs](jobs) | [submit](submit) |\n[login](login?goto=item%3Fid%3D39501982)  \n---|---|---  \n|  | [](vote?id=39501982&how=up&goto=item%3Fid%3D39501982)| [Mamba Explained:\nThe State Space Model Taking On\nTransformers](https://www.kolaayonrinde.com/blog/2024/02/11/mamba.html)\n([kolaayonrinde.com](from?site=kolaayonrinde.com))  \n---|---|---  \n|  270 points by [koayon](user?id=koayon) [77 days ago](item?id=39501982) |\n[hide](hide?id=39501982&goto=item%3Fid%3D39501982) |\n[past](https://hn.algolia.com/?query=Mamba%20Explained%3A%20The%20State%20Space%20Model%20Taking%20On%20Transformers&type=story&dateRange=all&sort=byDate&storyText=false&prefix&page=0)\n| [favorite](fave?id=39501982&auth=28acf4ce27055c1fa6eb93adf2ec7db5dc5838f4) |\n[93 comments](item?id=39501982)  \n  \n  \n| ![](s.gif)|  [](vote?id=39504332&how=up&goto=item%3Fid%3D39501982) |\n\n[Straw](user?id=Straw) [77 days ago](item?id=39504332) | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nThe SSMs papers and blogs always have unnecessarily complicated explanations.\nAt this point I almost wonder if its to hide how simple the underlying\nalgorithms are, or to make them seem fancy.\n\nSSMs are doing exponentially weighted moving averages (EMA). That's it- to\nsummarize the past, you take an EMA of a variable output at each time step.\nMamba changes one key thing- instead of decaying the past by a fixed amount\neach step as in a constant-time EMA, we have another output which decides how\nmuch to forget, or equivalently, how much 'time' has passed since the last\nobservation in our EMA.\n\nAll of the matrix equations, continuous time, discretization, etc, will end up\nwith a dynamic-forgetting EMA as I describe above. This also makes the\nbenefits and limitations clear- finite state size, has to decide at a given\nlayer what to forget before it sees the past at that layer.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504440&how=up&goto=item%3Fid%3D39501982) |\n\n[logicchains](user?id=logicchains) [77 days ago](item?id=39504440) | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nAre there any fundamental differences between Mamba, Retnet and RWKV, or are\nthey all variants of this same architecture?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39507834&how=up&goto=item%3Fid%3D39501982) |\n\n[Straw](user?id=Straw) [77 days ago](item?id=39507834) | root | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nNo, all of these use the same fundamental architecture with minor tweaks, such\nas the dynamic gate for mamba or an outer product paramterization of the\nvalues for RWKV-v5  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39508177&how=up&goto=item%3Fid%3D39501982) |\n\n[pama](user?id=pama) [77 days ago](item?id=39508177) | root | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nA dynamic gate is a pretty distinct feature from previous SSM architectures in\nmy opinion. In a sense, the overall fundamental architecture of mamba is still\nthat of the transformer but with attention replaced by an SSM with dynamic\ngating.", "start_char_idx": 0, "end_char_idx": 3126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0800fd72-b783-4cda-882b-6e66085ad6b9": {"__data__": {"id_": "0800fd72-b783-4cda-882b-6e66085ad6b9", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee8e54f9-7071-442a-8c77-854a7ae27118", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "0a34b2ad0fe5024b7858c67e5cefa0fa033491ba27b4231d6a3274cb6ba8249c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d684716-d0c5-414c-9280-b94a0eb99c1c", "node_type": "1", "metadata": {}, "hash": "8938efee9824abad11fbc40044ef920e25b0a2503f6cd3bec21db0a7e229ff38", "class_name": "RelatedNodeInfo"}}, "text": "[](s.gif)|  [](vote?id=39508177&how=up&goto=item%3Fid%3D39501982) |\n\n[pama](user?id=pama) [77 days ago](item?id=39508177) | root | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nA dynamic gate is a pretty distinct feature from previous SSM architectures in\nmy opinion. In a sense, the overall fundamental architecture of mamba is still\nthat of the transformer but with attention replaced by an SSM with dynamic\ngating. All of deep learning uses closely related ideas, but the SSM class of\nmodels took advantage of stability guarantees from integrators in control\ntheory and created a class of RNN that don\u2019t have to worry about exploding\ngradients. Mamba is one of the ways to make these SSM models much more\nexpressive.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39545657&how=up&goto=item%3Fid%3D39501982) |\n\n[Straw](user?id=Straw) [74 days ago](item?id=39545657) | root | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nIts distinct, but not very- its an EMA without assuming uniform time. The\nstability of EMA has nothing to do with integrators in control theory and\nneither do these models.\n\nThese models aren't really RNNs- they have only a linear gate which cannot\ndepend on previous tokens at this layer, so they cant update their state in a\nway which depends on the current state very much.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504567&how=up&goto=item%3Fid%3D39501982) |\n\n[ogogmad](user?id=ogogmad) [77 days ago](item?id=39504567) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nThat might explain the motivation for why the \u0394 variable is used and varied;\nbut not the \"Selectivity\", which the article says is expressed by how the\nmatrices B and C vary while consuming input.\n\nSomething I've noticed is that B, C and \u0394 depend only on the current token.\nSee this:\n[https://www.kolaayonrinde.com/blog/images/mamba/ssm_algorith...](https://www.kolaayonrinde.com/blog/images/mamba/ssm_algorithm.png)\n\\-- Another thing is that I've noticed that the definition of \"SSM\" in the\nimage I've linked to is apparently recursive. This is also in the Arxiv paper.\nStrange.\n\n+1 though for making me go back to the article and read it more carefully! +1\nalso to the article.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505758&how=up&goto=item%3Fid%3D39501982) |\n\n[ogogmad](user?id=ogogmad) [77 days ago](item?id=39505758) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nOK, I've noticed that the pseudo-code above is vectorised, and so there's no\nrecursion. The SSM function is actually described at the start of the paper,\nand an efficient hardware-aware implementation is suggested in section 3:\n[https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504561&how=up&goto=item%3Fid%3D39501982) |\n\n[binarymax](user?id=binarymax) [77 days ago](item?id=39504561) | parent | prev\n| next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI hadn\u2019t heard of Mamba before reading this article, and I was wondering if\nanyone has tried setting importance of a token as a TF-IDF or BM25 lookup.\nRequires a first pass to construct the token index but otherwise it seems like\nit would address the big issue that all these architectures have - they don\u2019t\nknow how \u201cimportant\u201d a token is. Interestingly this seems to be the crux of\nMamba - deciding what tokens to forget! EMA other treats all tokens equally at\nsequence time. What if the tokens were weighted beforehand and the weights\nwere passed as an attention mechanism?", "start_char_idx": 2708, "end_char_idx": 6226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8d684716-d0c5-414c-9280-b94a0eb99c1c": {"__data__": {"id_": "8d684716-d0c5-414c-9280-b94a0eb99c1c", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0800fd72-b783-4cda-882b-6e66085ad6b9", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "9050292991656076707f306090efaea4d269837ef376d0789988d18e636617c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "node_type": "1", "metadata": {}, "hash": "1433f90486019ad671b05b68dc6f9a1a164d8e9f8ddfbeb7fc18ca74c1f28f38", "class_name": "RelatedNodeInfo"}}, "text": "[](s.gif)|  [](vote?id=39504561&how=up&goto=item%3Fid%3D39501982) |\n\n[binarymax](user?id=binarymax) [77 days ago](item?id=39504561) | parent | prev\n| next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI hadn\u2019t heard of Mamba before reading this article, and I was wondering if\nanyone has tried setting importance of a token as a TF-IDF or BM25 lookup.\nRequires a first pass to construct the token index but otherwise it seems like\nit would address the big issue that all these architectures have - they don\u2019t\nknow how \u201cimportant\u201d a token is. Interestingly this seems to be the crux of\nMamba - deciding what tokens to forget! EMA other treats all tokens equally at\nsequence time. What if the tokens were weighted beforehand and the weights\nwere passed as an attention mechanism? I wonder if anyone has tried something\nlike this.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504840&how=up&goto=item%3Fid%3D39501982) |\n\n[halflings](user?id=halflings) [77 days ago](item?id=39504840) | root | parent\n| next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nThe importance (e.g. attention) needs to be dynamic, e.g. one token will be\nimportant to some other tokens but not others.\n\ntf-idf and similar heuristics are what we were using before attention came\nalong, e.g. tf-idf weighted bag-of-words representation of word2vec\nembeddings. That approaches fails in so many cases.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505673&how=up&goto=item%3Fid%3D39501982) |\n\n[binarymax](user?id=binarymax) [77 days ago](item?id=39505673) | root | parent\n| next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nAttention in transformers works because over time the model learns token\nimportance based on frequency and context.\n\nIf you don\u2019t have attention and need a fast substitute for \u201cforgetting\u201d non\nimportant tokens, then BM25 is an intuitive hypothesis.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505926&how=up&goto=item%3Fid%3D39501982) |\n\n[curious_cat_163](user?id=curious_cat_163) [77 days ago](item?id=39505926) |\nroot | parent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nTo use your metaphor, TF-IDF will result in \u2018fixed\u2019 weights.\n\nAttention makes it so that the weights of each token can be different in each\nsequence of tokens. Same token gets different weights depending on who its\n\u2018neighbors\u2019 in the sequence end up being.\n\nThis property allows the models to solve a variety of natural language\nproblems and gets \u2018used\u2019 by the model to express context-aware dependencies.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39508316&how=up&goto=item%3Fid%3D39501982) |\n\n[littlestymaar](user?id=littlestymaar) [77 days ago](item?id=39508316) | root\n| parent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nGiven that GP explicitly said \u201cif you don't have attention\u201d, and we're in a\nthread about a language model whose main characteristics is not to use\nattention, I don't understand why you insist in talking about attention \u2026  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39508611&how=up&goto=item%3Fid%3D39501982) |\n\n[curious_cat_163](user?id=curious_cat_163) [77 days ago](item?id=39508611) |\nroot | parent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI mean, if we are going to get past attention (very much on board with the\nidea!), then it might help to know what it is really contributing to a model.\n\nMy response was trying to clarify some confusion.\n\nI am all for alternatives to attention. I don\u2019t think BM25 cuts it. I don\u2019t\nthink anything that samples tokens based on BM25 weights (the idea in this\nsubthread) would cut it.  \n  \n---|---|---  \n| !", "start_char_idx": 5462, "end_char_idx": 8957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab": {"__data__": {"id_": "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d684716-d0c5-414c-9280-b94a0eb99c1c", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "708a792c80aa7bb9639bf1497e28a1d6d758adb5217170fe710a6ef92d305af6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "308ad536-8781-4dd3-b5dc-f739aaf6202a", "node_type": "1", "metadata": {}, "hash": "8abf12c29180a5ef579e2a54a95da558baafd36a91049f86b0996ffae19f9b31", "class_name": "RelatedNodeInfo"}}, "text": "[](s.gif)|  [](vote?id=39508611&how=up&goto=item%3Fid%3D39501982) |\n\n[curious_cat_163](user?id=curious_cat_163) [77 days ago](item?id=39508611) |\nroot | parent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI mean, if we are going to get past attention (very much on board with the\nidea!), then it might help to know what it is really contributing to a model.\n\nMy response was trying to clarify some confusion.\n\nI am all for alternatives to attention. I don\u2019t think BM25 cuts it. I don\u2019t\nthink anything that samples tokens based on BM25 weights (the idea in this\nsubthread) would cut it.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39510230&how=up&goto=item%3Fid%3D39501982) |\n\n[binarymax](user?id=binarymax) [77 days ago](item?id=39510230) | root | parent\n| next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nWhat confusion? I know exactly how BM25 works and how Transformers work. I\nstated a hypothesis and asked if anyone has tried it. You say it won\u2019t work.\nThat\u2019s just your opinion. Do you have proof or evidence? This is science.\nDismissal of ideas without evidence goes against scientific principles.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39574625&how=up&goto=item%3Fid%3D39501982) |\n\n[curious_cat_163](user?id=curious_cat_163) [71 days ago](item?id=39574625) |\nroot | parent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nJust catching up to this thread again. You had said:\n\n\"I was wondering if anyone has tried setting importance of a token as a TF-IDF\nor BM25 lookup.\"\n\nSo, I take it back. This is not a confusion. You are right to call it out. :)\n\nI like this idea directionally. A lot of energy (literally) would be saved if\nwe could get to the model accuracy outcomes with static weights like this.\n\nHowever, I do think that this (as stated in your original message) would not\nwork as well as transformer or SSM and I explained my reasoning as to why,\nalready. I don't have an empirical proof (not having run the experiment) but\nif you believe in it, you should try it and share your findings.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39507690&how=up&goto=item%3Fid%3D39501982) |\n\n[nelsondev](user?id=nelsondev) [77 days ago](item?id=39507690) | root | parent\n| prev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nNot exactly related, but in the same vein - Deep Impact - deep learning to\nfind term impacts in the context of their document.\n\n[https://arxiv.org/abs/2104.12016](https://arxiv.org/abs/2104.12016)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39509121&how=up&goto=item%3Fid%3D39501982) |\n\n[torginus](user?id=torginus) [77 days ago](item?id=39509121) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nIs this analogous to digital filters, where Transformers are the FIR filters\nthat operate on the history of input, and IIR filters, which take past inputs\ninto account with an exponentially decaying importance?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503026&how=up&goto=item%3Fid%3D39501982) |\n\n[CrypticShift](user?id=CrypticShift) [77 days ago](item?id=39503026) | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\n> In other words, you can drag and drop downloaded states into your model,\n> like literal plug-in cartridges\n\nThe same could be said of \"control vectors\" [1]. Both ideas are still\nexperimental, but is seems to me IINM that they could replace \"system prompts\"\nand \"RAG\" respectively.", "start_char_idx": 8354, "end_char_idx": 11665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "308ad536-8781-4dd3-b5dc-f739aaf6202a": {"__data__": {"id_": "308ad536-8781-4dd3-b5dc-f739aaf6202a", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a90ae5f32bc1d6c8483761ece0235a7ab4c26adf699043402cfda7f57d07a08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "node_type": "1", "metadata": {}, "hash": "7c8889c413f0b56e9bcace8a3b0f49bc6be7cfcbda483449150ecd91c97c4c2b", "class_name": "RelatedNodeInfo"}}, "text": "---|---|---  \n| ![](s.gif)|  [](vote?id=39503026&how=up&goto=item%3Fid%3D39501982) |\n\n[CrypticShift](user?id=CrypticShift) [77 days ago](item?id=39503026) | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\n> In other words, you can drag and drop downloaded states into your model,\n> like literal plug-in cartridges\n\nThe same could be said of \"control vectors\" [1]. Both ideas are still\nexperimental, but is seems to me IINM that they could replace \"system prompts\"\nand \"RAG\" respectively.\n\n[1]\n[https://news.ycombinator.com/item?id=39414532](https://news.ycombinator.com/item?id=39414532)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503076&how=up&goto=item%3Fid%3D39501982) |\n\n[refulgentis](user?id=refulgentis) [77 days ago](item?id=39503076) | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nCan control vectors replace RAG?\n\ni.e. if I want the model to give me a summary of the news today, and the model\nwas trained before today, can control vectors help?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503186&how=up&goto=item%3Fid%3D39501982) |\n\n[p1esk](user?id=p1esk) [77 days ago](item?id=39503186) | root | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nNo technique can get you the news other than actually searching for and then\nparsing the published news.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503351&how=up&goto=item%3Fid%3D39501982) |\n\n[refulgentis](user?id=refulgentis) [77 days ago](item?id=39503351) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nCan a control vector replace system prompts?\n\ni.e. can it do in-context learning without the context?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504340&how=up&goto=item%3Fid%3D39501982) |\n\n[jncfhnb](user?id=jncfhnb) [77 days ago](item?id=39504340) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nIt more or less is the same as a system prompt  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505051&how=up&goto=item%3Fid%3D39501982) |\n\n[refulgentis](user?id=refulgentis) [77 days ago](item?id=39505051) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nSo, no  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39508856&how=up&goto=item%3Fid%3D39501982) |\n\n[jncfhnb](user?id=jncfhnb) [77 days ago](item?id=39508856) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nSo, yes, but not in a meaningfully different form  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503295&how=up&goto=item%3Fid%3D39501982) |\n\n[Der_Einzige](user?id=Der_Einzige) [77 days ago](item?id=39503295) | parent |\nprev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nWhoever is downvoting this post needs to stop.\n\nThe concepts behind control vectors, i.e. \"representation engineering\" are not\nespecially new and have been highly effective in the diffusion space. I always\nfind it entertaining when LLM folks act like they're discovering stuff that\nwaifu stable diffusion folks knew for 6 months + about - like \"concept slider\nloras\".  \n  \n---|---|---  \n| !", "start_char_idx": 11181, "end_char_idx": 14095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "31d95e9c-1baa-4000-a9f6-123a33a9cfd7": {"__data__": {"id_": "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "308ad536-8781-4dd3-b5dc-f739aaf6202a", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e78067d4343627e698db76548c785be62e7c712e404111a31265bb76c82762b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38da4efd-d4bb-4902-82d2-20a078368d8b", "node_type": "1", "metadata": {}, "hash": "a4ed57e33df4082b03d34381eeec5a8da307bfc52d4ed877fabf7fbb8e19909b", "class_name": "RelatedNodeInfo"}}, "text": "[](s.gif)|  [](vote?id=39503295&how=up&goto=item%3Fid%3D39501982) |\n\n[Der_Einzige](user?id=Der_Einzige) [77 days ago](item?id=39503295) | parent |\nprev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nWhoever is downvoting this post needs to stop.\n\nThe concepts behind control vectors, i.e. \"representation engineering\" are not\nespecially new and have been highly effective in the diffusion space. I always\nfind it entertaining when LLM folks act like they're discovering stuff that\nwaifu stable diffusion folks knew for 6 months + about - like \"concept slider\nloras\".  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503707&how=up&goto=item%3Fid%3D39501982) |\n\n[CuriouslyC](user?id=CuriouslyC) [77 days ago](item?id=39503707) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nYou are right that playing with AI image generation models is really good for\nbuilding intuition about AI models in general, even if they seem superficially\ndifferent. It's kind of like surveying a battlefield from the air.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503343&how=up&goto=item%3Fid%3D39501982) |\n\n[refulgentis](user?id=refulgentis) [77 days ago](item?id=39503343) | root |\nparent | prev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI don't know what you mean, can you help me?\n\nI'm familiar with our intrepid stable diffusion sailors.\n\nI don't know why you think the post is being downvoted.\n\nI don't know why it would be verboten to downvote it, or indicative of the\ndownvoter being an LLM fanatic who thinks they discovered everything.\n\nI am puzzled by the post because it claims RAG can be replaced by control\nvectors.\n\nI'm also puzzled because it claims prompts can be replaced by control vectors.\n\nI get that if system prompts were only to shift output tone, control vectors\ncould replace that case, but that seems narrow compared to the full set of\nthings prompt input enables (inter alia, the in-context learning)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504461&how=up&goto=item%3Fid%3D39501982) |\n\n[jncfhnb](user?id=jncfhnb) [77 days ago](item?id=39504461) | root | parent |\nprev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nMost of these things aren\u2019t much better than a single weighted token though  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39502432&how=up&goto=item%3Fid%3D39501982) |\n\n[Der_Einzige](user?id=Der_Einzige) [77 days ago](item?id=39502432) | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nFirst it was longformer, and linear attention models. Then it was RWKV and now\nit's Mamba. So many bombastic claims of improved architectural performance -\nand no open source models that beat the thing they purport to beat. The proof\nis always in the pudding, and these models will remain a curiosity for most\nuntil their weights are being benchmarked favorably on LLM leaderboards.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39502575&how=up&goto=item%3Fid%3D39501982) |\n\n[digdugdirk](user?id=digdugdirk) [77 days ago](item?id=39502575) | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nYes, that's technically accurate. But I prefer to think of the entire LLM\nspace as a new scientific field that started when OpenAI released ChatGPT.\n\nIn that context, all new research directions are valuable simply for the fact\nthat they're expanding the foundation of the field. 5 years from now, who\nknows what the most effective models will use under the hood, but the more we\ncan learn about them in general, the better.  \n  \n---|---|---  \n| !", "start_char_idx": 13513, "end_char_idx": 16948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "38da4efd-d4bb-4902-82d2-20a078368d8b": {"__data__": {"id_": "38da4efd-d4bb-4902-82d2-20a078368d8b", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "b70d42e20689174e1b483f0216b15b4f80f8b59bd54647ffe3ea52a1831203f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72b6de12-23a7-461b-b9bc-ad45c10e81da", "node_type": "1", "metadata": {}, "hash": "c5f3c97d80db20bd4064208b71b1de20787932a143f957b24b8aa27c8eddc135", "class_name": "RelatedNodeInfo"}}, "text": "---|---|---  \n| ![](s.gif)|  [](vote?id=39502575&how=up&goto=item%3Fid%3D39501982) |\n\n[digdugdirk](user?id=digdugdirk) [77 days ago](item?id=39502575) | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nYes, that's technically accurate. But I prefer to think of the entire LLM\nspace as a new scientific field that started when OpenAI released ChatGPT.\n\nIn that context, all new research directions are valuable simply for the fact\nthat they're expanding the foundation of the field. 5 years from now, who\nknows what the most effective models will use under the hood, but the more we\ncan learn about them in general, the better.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503389&how=up&goto=item%3Fid%3D39501982) |\n\n[lettergram](user?id=lettergram) [77 days ago](item?id=39503389) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nlol I think in general, LLM research traces its origins back to all the\nstandard deep learning techniques: NNs, CNNs, LSTMs, RNNs, etc.\n\nIn 2018, with the release of transformers (via google) it enabled much more\nrapid training of models and more generalization with less data. 100% of the\nLLMs (as you\u2019d probably thing of them)trace their origins to BERT.\n\nThat said, my team was working with hundred million to low billions of\nparameter LSTMs & CNNs back in 2016-2017 that were comparable to some lighter\nweight LLMs today.\n\nIn my opinion, the greatest strides in the space has less to do with the\nunderlying architecture, and more to do with improved data formatting,\naccessibility and compute improvements.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39502909&how=up&goto=item%3Fid%3D39501982) |\n\n[CityOfThrowaway](user?id=CityOfThrowaway) [77 days ago](item?id=39502909) |\nroot | parent | prev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nThe field of research here is far older than ChatGPT's release. Neural network\nresearch has been going on for at least 50 years.\n\nMost of the research that enabled ChatGPT was also already known. \"Attention\nis all you need\" was a 2017 paper.\n\nIt still is a fast evolving field, but not one that just kicked off.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39502764&how=up&goto=item%3Fid%3D39501982) |\n\n[sigmoid10](user?id=sigmoid10) [77 days ago](item?id=39502764) | parent | prev\n| next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nTrue, but bear in mind the Mamba preprint is less than three months old. A lot\nof people are probably experimenting with these ideas right now and training a\ncompletely new, large foundation model with a different architecture will take\na significant amount of time.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39514943&how=up&goto=item%3Fid%3D39501982) |\n\n[nickpsecurity](user?id=nickpsecurity) [76 days ago](item?id=39514943) |\nparent | prev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nGPT3-176B cost $30 million dollars in compute plus millions in design,\npreprocessing, and operations. Then, it was able to perform as much better\nthan prior architectures as it does today. You might want to include that in\nyour challenge for competing models.\n\nLet\u2019s rephrase it. If their architecture is superior, and they have $30\nmillion dollars, and similar preparation for training, and similar operational\nteams during training, then we can see if they can beat the model they\u2019re\ncomparing themselves to. Except, the alternatives don\u2019t have tens of millions\nof dollars with the best support teams. So, the proof you seek hasn\u2019t had a\nchance to happen due to severe lack of resources.\n\nHence, comparisons to GPT2 and small versions of GPT3. Even that might not be\nfair given the money and teams behind even small GPT3\u2019s.", "start_char_idx": 16301, "end_char_idx": 19910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "72b6de12-23a7-461b-b9bc-ad45c10e81da": {"__data__": {"id_": "72b6de12-23a7-461b-b9bc-ad45c10e81da", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38da4efd-d4bb-4902-82d2-20a078368d8b", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "c0db6cc47ebcfdd241845f8af97ebe1391daa0afb85843b747fb99690d0aa1ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "038cfb51-44d6-4872-953f-c57b4b388f11", "node_type": "1", "metadata": {}, "hash": "a67c71a1ee7117c241c4e87c307e05b6b361baebbd7c3f7534e90c1a2f4a5fa5", "class_name": "RelatedNodeInfo"}}, "text": "Then, it was able to perform as much better\nthan prior architectures as it does today. You might want to include that in\nyour challenge for competing models.\n\nLet\u2019s rephrase it. If their architecture is superior, and they have $30\nmillion dollars, and similar preparation for training, and similar operational\nteams during training, then we can see if they can beat the model they\u2019re\ncomparing themselves to. Except, the alternatives don\u2019t have tens of millions\nof dollars with the best support teams. So, the proof you seek hasn\u2019t had a\nchance to happen due to severe lack of resources.\n\nHence, comparisons to GPT2 and small versions of GPT3. Even that might not be\nfair given the money and teams behind even small GPT3\u2019s. Execution of the\nproject is as critical for success as the model architecture.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503444&how=up&goto=item%3Fid%3D39501982) |\n\n[imjonse](user?id=imjonse) [77 days ago](item?id=39503444) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nMost (all?) open-ish 7B+ models today are finetunes of proprietary/semi-\nclosed/bigbudget LLMs. There is no such foundation model for Mamba yet.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503466&how=up&goto=item%3Fid%3D39501982) |\n\n[imjonse](user?id=imjonse) [77 days ago](item?id=39503466) | prev | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nExplaining Mamba is a rite of passage, like the monad tutorials of yore.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503502&how=up&goto=item%3Fid%3D39501982) |\n\n[SkyMarshal](user?id=SkyMarshal) [77 days ago](item?id=39503502) | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nMamba is like a burrito...  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504188&how=up&goto=item%3Fid%3D39501982) |\n\n[kekebo](user?id=kekebo) [77 days ago](item?id=39504188) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nIt gets soggy and disintegrates when not consumed swiftly?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505651&how=up&goto=item%3Fid%3D39501982) |\n\n[sja](user?id=sja) [77 days ago](item?id=39505651) | parent | prev | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nOr Balks[0]:\n\nBALK RULES! IMPORTANT! 1\\. You can\u2019t just be up there and just doin\u2019 a balk\nlike that.\n\n1a. A balk is when you\n\n1b. Okay well listen. A balk is when you balk the\n\n1c. Let me start over\n\n1c-a. The pitcher is not allowed to do a motion to the, uh, batter, that\nprohibits the batter from doing, you know, just trying to hit the ball. You\ncan\u2019t do that.\n\n1c-b. Once the pitcher is in the stretch, he can\u2019t be over here and say to the\nrunner, like, \u201cI\u2019m gonna get ya! I\u2019m gonna tag you out! You better watch your\nbutt!\u201d and then just be like he didn\u2019t even do that.\n\n1c-b(1). Like, if you\u2019re about to pitch and then don\u2019t pitch, you have to\nstill pitch. You cannot not pitch. Does that make any sense?\n\n1c-b(2). You gotta be, throwing motion of the ball, and then, until you just\nthrow it.\n\n1c-b(2)-a. Okay, well, you can have the ball up here, like this, but then\nthere\u2019s the balk you gotta think about.\n\n1c-b(2)-b. Fairuza Balk hasn\u2019t been in any movies in forever. I hope she\nwasn\u2019t typecast as that racist lady in American History X.\n\n1c-b(2)-b(i).", "start_char_idx": 19187, "end_char_idx": 22365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "038cfb51-44d6-4872-953f-c57b4b388f11": {"__data__": {"id_": "038cfb51-44d6-4872-953f-c57b4b388f11", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72b6de12-23a7-461b-b9bc-ad45c10e81da", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "6d4fed5cb4ab216c3c1f5fdf8911ab40070c7bc2e9758ec6e1812aebcaa11500", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06559292-7485-444b-b545-bd2404895948", "node_type": "1", "metadata": {}, "hash": "2fd5ce9a2f62b777fd1b9c57c9008c26223cbf9fbb2332924ead990cf9aeb278", "class_name": "RelatedNodeInfo"}}, "text": "I\u2019m gonna tag you out! You better watch your\nbutt!\u201d and then just be like he didn\u2019t even do that.\n\n1c-b(1). Like, if you\u2019re about to pitch and then don\u2019t pitch, you have to\nstill pitch. You cannot not pitch. Does that make any sense?\n\n1c-b(2). You gotta be, throwing motion of the ball, and then, until you just\nthrow it.\n\n1c-b(2)-a. Okay, well, you can have the ball up here, like this, but then\nthere\u2019s the balk you gotta think about.\n\n1c-b(2)-b. Fairuza Balk hasn\u2019t been in any movies in forever. I hope she\nwasn\u2019t typecast as that racist lady in American History X.\n\n1c-b(2)-b(i). Oh wait, she was in The Waterboy too! That would be even worse.\n\n1c-b(2)-b(ii). \u201cget in mah bellah\u201d \u2013 Adam Water, \u201cThe Waterboy.\u201d Haha,\nclassic\u2026\n\n1c-b(3). Okay seriously though. A balk is when the pitcher makes a movement\nthat, as determined by, when you do a move involving the baseball and field of\n\n2\\. Do not do a balk please.\n\n[0]: [https://justinbee.tumblr.com/post/15309101943/best-\nexplanati...](https://justinbee.tumblr.com/post/15309101943/best-explanation-\nof-a-balk-ive-ever-seen/amp)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505249&how=up&goto=item%3Fid%3D39501982) |\n\n[hyperbovine](user?id=hyperbovine) [77 days ago](item?id=39505249) | parent |\nprev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nSimilar market share too.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503153&how=up&goto=item%3Fid%3D39501982) |\n\n[behnamoh](user?id=behnamoh) [77 days ago](item?id=39503153) | prev | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nCan the low adoption of Mamba be attributed to what is being discussed today\non HN\n([https://news.ycombinator.com/item?id=39491863](https://news.ycombinator.com/item?id=39491863))?\n\nBasically, Nvidia et al. don't want the AI research to move in a direction\nthat requires less GPU compute, less training data, and less inference\ncompute.\n\nSomeone on HN (I don't remember the name) mentioned that the idea of deep\nlearning is backed by big tech because it benefits them the most as they are\nthe only players in town with huge amounts of data. If the AI community would\nfind entirely different approaches to AGI (maybe not even learning), who do\nyou think would suffer the most from the implications?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503230&how=up&goto=item%3Fid%3D39501982) |\n\n[p1esk](user?id=p1esk) [77 days ago](item?id=39503230) | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nThis doesn\u2019t make sense - there are literally thousands of academic AI\nresearch labs who are severely limited by compute resources. If anything could\nwork better than transformers and require less compute they would be all over\nthat.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503263&how=up&goto=item%3Fid%3D39501982) |\n\n[behnamoh](user?id=behnamoh) [77 days ago](item?id=39503263) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI guess the argument is that most AI research is supported by the big tech,\nand they have heavily invested in the deep learning approach.\n\nIf the fundings were funneled to research groups working on alternative\napproaches, maybe we'd see the same amount of progress in AI only using\nanother approach.  \n  \n---|---|---  \n| !", "start_char_idx": 21781, "end_char_idx": 24960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "06559292-7485-444b-b545-bd2404895948": {"__data__": {"id_": "06559292-7485-444b-b545-bd2404895948", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "038cfb51-44d6-4872-953f-c57b4b388f11", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "ed3e5dea9de293cd49151aaba12cd8e3f99be4ed1e81158600f6319d0022318d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84fbca06-688c-4ca1-9194-17209850a235", "node_type": "1", "metadata": {}, "hash": "08e3b9b82fd720c300ed749adf60cf509ba08f46b473d3e517b36fdbfe10ab5f", "class_name": "RelatedNodeInfo"}}, "text": "If anything could\nwork better than transformers and require less compute they would be all over\nthat.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503263&how=up&goto=item%3Fid%3D39501982) |\n\n[behnamoh](user?id=behnamoh) [77 days ago](item?id=39503263) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI guess the argument is that most AI research is supported by the big tech,\nand they have heavily invested in the deep learning approach.\n\nIf the fundings were funneled to research groups working on alternative\napproaches, maybe we'd see the same amount of progress in AI only using\nanother approach.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503314&how=up&goto=item%3Fid%3D39501982) |\n\n[kettleballroll](user?id=kettleballroll) [77 days ago](item?id=39503314) |\nroot | parent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nAs a member of the research community: that's nonsense. Like already pointed\nout: academic groups (who by no means are dependent on big tech) would jump\nall over that. Mamba has been out long enough that you'd already see tons of\npapers at arxiv showing mamba dominating transformers in all sorts of\napplications. But that's not happening, despite the ton of hype. That doesn't\nmean that mamba is nonsense. Just that it isn't the immediate transformer\nkiller. It remains to be seen if something comes from it, eventually.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503681&how=up&goto=item%3Fid%3D39501982) |\n\n[godelski](user?id=godelski) [77 days ago](item?id=39503681) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nAs a member of the research community: that's nonsense. Publishing is an\nextremely noisy process in ML and is getting increasingly difficult for\nsmaller non big tech collaborating labs. Reviewers' go to are: more datasets,\nscale, not novel. The easiest way to approach this is to work off of\npretrained models. This is probably more obvious in the NLP world.\n\nI agree that Mamba doesn't solve everything and it still needs work. But I\ndisagree with the logic that there isn't an issue of railroading.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503919&how=up&goto=item%3Fid%3D39501982) |\n\n[p1esk](user?id=p1esk) [77 days ago](item?id=39503919) | root | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nWhat\u2019s the main difference between an ape\u2019s brain and a human brain? Scale. So\nthat\u2019s the train we\u2019re riding at the moment. No roadblocks yet, aside from\ncost.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504248&how=up&goto=item%3Fid%3D39501982) |\n\n[godelski](user?id=godelski) [77 days ago](item?id=39504248) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\n> What\u2019s the main difference between an ape\u2019s brain and a human brain? Scale.\n\nThis is incredibly naive with absolutely no scientific basis. There is no\nevidence that this is in scale of data nor scale of architecture.\n\nThere are a number of animals with larger brains in terms of both mass and\ntotal number of neurons. An African Elephant has roughly 3x the number of\nneurons humans have. Dolphins beat humans in total surface area. Neanderthals\nare estimated to have had larger brains too! It isn't mass, neurons, neuron\ndensity, surface area. We aren't just scaled up chimps.  \n  \n---|---|---  \n| !", "start_char_idx": 24327, "end_char_idx": 27560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "84fbca06-688c-4ca1-9194-17209850a235": {"__data__": {"id_": "84fbca06-688c-4ca1-9194-17209850a235", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06559292-7485-444b-b545-bd2404895948", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "48e899353ce347ae11297d5317116fdf4518d93fbf56b78296e088dafe25d57d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a913046-7b31-424a-9254-89d6ebf914e0", "node_type": "1", "metadata": {}, "hash": "a6750de32eac96f2489943d6732bc44a501e437b4ca50f3ea738ee91f45f76b6", "class_name": "RelatedNodeInfo"}}, "text": "Scale.\n\nThis is incredibly naive with absolutely no scientific basis. There is no\nevidence that this is in scale of data nor scale of architecture.\n\nThere are a number of animals with larger brains in terms of both mass and\ntotal number of neurons. An African Elephant has roughly 3x the number of\nneurons humans have. Dolphins beat humans in total surface area. Neanderthals\nare estimated to have had larger brains too! It isn't mass, neurons, neuron\ndensity, surface area. We aren't just scaled up chimps.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504404&how=up&goto=item%3Fid%3D39501982) |\n\n[p1esk](user?id=p1esk) [77 days ago](item?id=39504404) | root | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nOther animals with larger brains might have other bottlenecks preventing them\nfrom reaching full potential of their intelligence. Neanderthals might have\nbeen smarter than us, but went extinct for reasons not related to\nintelligence.\n\nBut my point stands - our brains have evolved directly from apes brains and\nthe main difference between them and us is brain size.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504744&how=up&goto=item%3Fid%3D39501982) |\n\n[godelski](user?id=godelski) [77 days ago](item?id=39504744) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\n> Other animals with larger brains might have other bottlenecks\n\n>>> What\u2019s the main difference between an ape\u2019s brain and a human brain?\nScale.\n\nYour argument is inconsistent. Very clearly everything isn't scale or we'd use\nother things besides transformers. Different architectures scale in different\nways and everything has different inductive biases. No one doubts scale is\nimportant, but there's a lot more.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504876&how=up&goto=item%3Fid%3D39501982) |\n\n[p1esk](user?id=p1esk) [77 days ago](item?id=39504876) | root | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nScale is all we need for transformers (so far). It might also be all we need\nfor ape brains. It\u2019s not all we need for whatever elephant or dolphin brains\nevolved from.\n\nWhen this stops being the case for transformers, we will need something else.\nI\u2019m just pointing out it\u2019s not the case yet.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504949&how=up&goto=item%3Fid%3D39501982) |\n\n[godelski](user?id=godelski) [77 days ago](item?id=39504949) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI see no evidence of this in biology nor in ML. I've read those scale papers.\nI've worked on scale myself. I'll bet the farm that scale isn't all you need.\nBut I won't be surprised if people say that it is all scale.\n\nIf you really think it is all scale, train a 7T ResNet MLP based model for\nNLP. If scale is all you need, make a LLM without DPO or RLHF. If scale is all\nyou need, make SD3 with a GAN. Or what about a VAE, Normalizing Flow, HMM? Do\nit with different optimizers. Do it with gradient free methods. Do it with\ndifferent loss functions.\n\nThe bitter lesson wasn't \"scale is all you need.\" That's just a\nmisinterpretation.\n\nEdit: It's fine to disagree. We can compete on the ideas and methods. That's\ngood for our community. So continue down yours, and I'll continue down mine.\nAll I ask is that since your camp is more popular, you don't block those on my\nside. If you're right, we'll get to AGI soon. If we're right, we still might.\nBut if we're right and you all block us, we'll get another AI winter in\nbetween. If we're right and you all don't block us, we can progress without\nskipping a beat. Just don't put all your eggs in one basket. It isn't good for\nanyone.", "start_char_idx": 27030, "end_char_idx": 30610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9a913046-7b31-424a-9254-89d6ebf914e0": {"__data__": {"id_": "9a913046-7b31-424a-9254-89d6ebf914e0", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84fbca06-688c-4ca1-9194-17209850a235", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "e9592decc6d88f1de7814d421a86776db39478fa7f7834614e3a9a0e5a3377f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "node_type": "1", "metadata": {}, "hash": "205160b236e23bdcee2d02bbba20e4a44fc956b2192c81e706f18062713ef036", "class_name": "RelatedNodeInfo"}}, "text": "Do\nit with different optimizers. Do it with gradient free methods. Do it with\ndifferent loss functions.\n\nThe bitter lesson wasn't \"scale is all you need.\" That's just a\nmisinterpretation.\n\nEdit: It's fine to disagree. We can compete on the ideas and methods. That's\ngood for our community. So continue down yours, and I'll continue down mine.\nAll I ask is that since your camp is more popular, you don't block those on my\nside. If you're right, we'll get to AGI soon. If we're right, we still might.\nBut if we're right and you all block us, we'll get another AI winter in\nbetween. If we're right and you all don't block us, we can progress without\nskipping a beat. Just don't put all your eggs in one basket. It isn't good for\nanyone.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505085&how=up&goto=item%3Fid%3D39501982) |\n\n[p1esk](user?id=p1esk) [77 days ago](item?id=39505085) | root | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nI said \u201cscale is all you need _for transformers_ \u201d. That has been true since\nGPT1. The best way to improve our best model today still seems to be \u201cmake it\nlarger and train it on more data\u201d.\n\nIf you disagree please suggest a better way, or at least provide evidence that\nscaling up no longer works for transformers.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505846&how=up&goto=item%3Fid%3D39501982) |\n\n[algo_trader](user?id=algo_trader) [77 days ago](item?id=39505846) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\n> at least provide evidence that scaling up no longer works for transformers.\n\nIsnt the Mixture-of-Experts trend (GPT4 is MoE?) kinda of a proof ?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39506848&how=up&goto=item%3Fid%3D39501982) |\n\n[godelski](user?id=godelski) [77 days ago](item?id=39506848) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nOf scale? I would think not. I would say they are evidence against scale\nbecause they are more an argument for multi agent systems. Scale is about a\nsingular framework. What that means is debatable though (I mean anything we\ncall a singular network can be decomposed into sub networks. It's messy),\nhence the other part of my comment about not scale solutions being claimed as\nscale.\n\n> I said \u201cscale is all you need for transformers\u201d\n\nNo you didn't. What kicked this all off was\n\n> What\u2019s the main difference between an ape\u2019s brain and a human brain? Scale.\n\nDon't retcon.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505987&how=up&goto=item%3Fid%3D39501982) |\n\n[p1esk](user?id=p1esk) [77 days ago](item?id=39505987) | root | parent | prev\n| next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI think they went MoE purely because straight up scaling from 175B to 1.8T is\njust too expensive. But it\u2019s still 10x scaling, right?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39507683&how=up&goto=item%3Fid%3D39501982) |\n\n[frozenseven](user?id=frozenseven) [77 days ago](item?id=39507683) | root |\nparent | prev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nScary how someone can be so confident in their wrong information.\n\n> An African Elephant has roughly 3x the number of neurons humans have.\n\nAn African elephant's brain is _not_ a scaled up chimp brain in any way.\nAfrican elephants have less cortical neurons than a chimp, and roughly a third\nof the amount that humans have.\n\n> Dolphins beat humans in total surface area.", "start_char_idx": 29876, "end_char_idx": 33204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e9233990-d9c9-41e0-8cc7-d9d89e29cad2": {"__data__": {"id_": "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a913046-7b31-424a-9254-89d6ebf914e0", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "4b9e6c79053baf23f662ac8dc41c15ba4b11a8cf0aed15dfc32f4a6fdfcce837", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74e7593d-86c3-4e71-8614-8b8970cd57ef", "node_type": "1", "metadata": {}, "hash": "e464c92091e096681828f55505c265a955a44cc367b40b07986adb7976ab5598", "class_name": "RelatedNodeInfo"}}, "text": "But it\u2019s still 10x scaling, right?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39507683&how=up&goto=item%3Fid%3D39501982) |\n\n[frozenseven](user?id=frozenseven) [77 days ago](item?id=39507683) | root |\nparent | prev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nScary how someone can be so confident in their wrong information.\n\n> An African Elephant has roughly 3x the number of neurons humans have.\n\nAn African elephant's brain is _not_ a scaled up chimp brain in any way.\nAfrican elephants have less cortical neurons than a chimp, and roughly a third\nof the amount that humans have.\n\n> Dolphins beat humans in total surface area.\n\nAnimals even less related to humans and chimps, with no prehensile appendages,\nliving in an environment where building stuff is exceedingly difficult. And of\ncourse their brains are obviously different from any great ape.\n\n> Neanderthals are estimated to have had larger brains too!\n\nAnd were just as smart as us and also had a scaled up chimp brain.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39508546&how=up&goto=item%3Fid%3D39501982) |\n\n[godelski](user?id=godelski) [77 days ago](item?id=39508546) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nanimal :: cortical neurons (b) :: total neurons (b)\n\nHuman :: 16 :: 86\n\nGorilla :: 9.1 :: 33\n\nChimp :: 6 :: 22\n\nAfrican Elephant :: 5.6 :: 251\n\nChimps are generally considered more intelligent than gorillas.\n\nBottlenose Dolphins have 11-15b cortical neurons while humans are in the range\n14-18 (range is measurement uncertainty). It's also worth noting these\ndolphins have a larger brain mass (1.6 kg) and larger cortical surface (3700\ncm2) than humans (1.3 kg and 2400 cm2, respectively).\n\n> with no prehensile appendages, living in...\n\nSo more than scale. Glad we agree. Seems you also agree that architecture\nmatters too.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39508765&how=up&goto=item%3Fid%3D39501982) |\n\n[frozenseven](user?id=frozenseven) [77 days ago](item?id=39508765) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\n> Chimps are generally considered more intelligent than gorillas.\n\nAnd chimps are genetically more similar to humans than gorillas. A chimp brain\nis more similar to a human brain than a gorilla brain.\n\n> So more than scale. Glad we agree.\n\nWe absolutely do not agree. Notice how nobody suggested that a human brain is\na scaled up version of an axolotl's brain? Yeah, that didn't happen. I do\nwonder why?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505729&how=up&goto=item%3Fid%3D39501982) |\n\n[algo_trader](user?id=algo_trader) [77 days ago](item?id=39505729) | root |\nparent | prev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\n> Just that it isn't the immediate transformer killer.\n\nWhat is the best/stable-ish linear alternative for transformer right now?\nEspecially for text generation and summarization.\n\nWe have domain specific ways of over sampling and search, so we much prefer\nless expensive models.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39507407&how=up&goto=item%3Fid%3D39501982) |\n\n[anon291](user?id=anon291) [77 days ago](item?id=39507407) | root | parent |\nprev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nAs someone who's worked at several NVIDIA competitors, including Groq, I can\nguarantee you that, based on my knowledge, of existing products, they would be\nable to make much more money should they have lower memory footprint models.\nGiven the amount of VC capital deployed for this (on the order of 100s of\nmillions), I don't believe this is a reasonable take.", "start_char_idx": 32578, "end_char_idx": 36082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "74e7593d-86c3-4e71-8614-8b8970cd57ef": {"__data__": {"id_": "74e7593d-86c3-4e71-8614-8b8970cd57ef", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "81017e65075dc008f74cb32ab4e5d2c9de01148fa88916e4fe2e0fb68d06d263", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77b729dc-066f-457d-b665-9b982766edeb", "node_type": "1", "metadata": {}, "hash": "7ecf8f13324fe55cd67fe8f7da08444c641e4fe5bfd3f717b9cd1ee3121f5b3e", "class_name": "RelatedNodeInfo"}}, "text": "Especially for text generation and summarization.\n\nWe have domain specific ways of over sampling and search, so we much prefer\nless expensive models.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39507407&how=up&goto=item%3Fid%3D39501982) |\n\n[anon291](user?id=anon291) [77 days ago](item?id=39507407) | root | parent |\nprev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nAs someone who's worked at several NVIDIA competitors, including Groq, I can\nguarantee you that, based on my knowledge, of existing products, they would be\nable to make much more money should they have lower memory footprint models.\nGiven the amount of VC capital deployed for this (on the order of 100s of\nmillions), I don't believe this is a reasonable take.\n\nSure, NVIDIA et al may not want that (although, again I don't see why... they\ntoo can't produce chips fast enough so being able to provide models for\ncustomers now ought to be good), but there's so much money out there that\ndoes...  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503327&how=up&goto=item%3Fid%3D39501982) |\n\n[landryraccoon](user?id=landryraccoon) [77 days ago](item?id=39503327) | root\n| parent | prev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nWhy would Meta, Microsoft, Amazon and Google want Nvidia to remain dominant in\nhardware? Are you treating \u201cbig tech\u201d like they all have one hive mind?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503626&how=up&goto=item%3Fid%3D39501982) |\n\n[behnamoh](user?id=behnamoh) [77 days ago](item?id=39503626) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nFor MSFT, AMZN, GOOG, the competitive advantage comes from having huge\ndatasets (that Nvidia doesn't have). It's a symbiosis that benefits the data-\nrich and GPU-rich players.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504121&how=up&goto=item%3Fid%3D39501982) |\n\n[pixl97](user?id=pixl97) [77 days ago](item?id=39504121) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nThis still makes little sense as that scale will always matter. If you can\ndrop the compute cost of a model by 10x it means you can increase model\nintegrity/intelligence/speed etc beyond what your compute bound competitors\nhave.\n\nSimply put, for the time being huge datasets are going to be needed and those\nwith bigger (cleaner?) datasets will have a better behaving model.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504232&how=up&goto=item%3Fid%3D39501982) |\n\n[fauigerzigerk](user?id=fauigerzigerk) [77 days ago](item?id=39504232) | root\n| parent | prev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nWhere is the symbiosis? If data is the differentiator, how do the data owners\nbenefit from Nvidia eating into their margins?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39506845&how=up&goto=item%3Fid%3D39501982) |\n\n[coldtea](user?id=coldtea) [77 days ago](item?id=39506845) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nIt's sumbiosis based on a common factor they both appreciate:\n\n\\- the data/processing having to be large means the data-owners have a benefit\n\n\\- the data/processing having to be large means NVIDIA also has a benefit\n(sells more GPUs to handle all that load)  \n  \n---|---|---  \n| !", "start_char_idx": 35359, "end_char_idx": 38492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "77b729dc-066f-457d-b665-9b982766edeb": {"__data__": {"id_": "77b729dc-066f-457d-b665-9b982766edeb", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74e7593d-86c3-4e71-8614-8b8970cd57ef", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "2fae57e431c15acd8e7ab5d0bdced3ae818feac099056b06a0b8705ae1a522f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "574edc77-b679-4f4b-811a-cf5ec3565d88", "node_type": "1", "metadata": {}, "hash": "50d26b93377d7e3ecc5dc604f9dccf960a6846d626b6e37970da2dad01d81972", "class_name": "RelatedNodeInfo"}}, "text": "If data is the differentiator, how do the data owners\nbenefit from Nvidia eating into their margins?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39506845&how=up&goto=item%3Fid%3D39501982) |\n\n[coldtea](user?id=coldtea) [77 days ago](item?id=39506845) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nIt's sumbiosis based on a common factor they both appreciate:\n\n\\- the data/processing having to be large means the data-owners have a benefit\n\n\\- the data/processing having to be large means NVIDIA also has a benefit\n(sells more GPUs to handle all that load)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39508690&how=up&goto=item%3Fid%3D39501982) |\n\n[fauigerzigerk](user?id=fauigerzigerk) [77 days ago](item?id=39508690) | root\n| parent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nData owners are benefitting from having access to data while others don't.\n\nHigh processing cost is not a benefit to them at all. It's just a cost eating\ninto their margins.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503657&how=up&goto=item%3Fid%3D39501982) |\n\n[nyrikki](user?id=nyrikki) [77 days ago](item?id=39503657) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nThe fact that 'removing the \u201cquadratic bottleneck\u201d' involves either reduced\nexpressability compared to self attention or disproving SETH is another\nreason.\n\nThe quadratic bottleneck is due to the lower bounds of exhaustive search.\n\nThe papers on this only ever seem to reference perplexity.\n\nThe fact it can append a word to \"I'm going to the beach\" that sounds good\ndoesn't mean it is useful.\n\nThere is no free lunch, and this project hasn't shown that the costs are\nacceptable.\n\n\"I'm going to the beach\" \\+ house\n\nDoesn't help if what you needed was\n\n\"I'm going to the beach\" \\+ tomorrow\n\nI do hope that there is more information on the costs, or that they have\ndisproven SETH soon.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503739&how=up&goto=item%3Fid%3D39501982) |\n\n[soVeryTired](user?id=soVeryTired) [77 days ago](item?id=39503739) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nWhat's SETH in this context? I googled to no avail.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503924&how=up&goto=item%3Fid%3D39501982) |\n\n[nyrikki](user?id=nyrikki) [77 days ago](item?id=39503924) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nStrong Exponential Time Hypothesis\n\nHere is how it relates to attention.\n\n[https://arxiv.org/abs/2209.04881](https://arxiv.org/abs/2209.04881)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503539&how=up&goto=item%3Fid%3D39501982) |\n\n[fbdab103](user?id=fbdab103) [77 days ago](item?id=39503539) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nIt is a really recent development. Even if this architecture is technically\nsuperior, it could take time before a model using it becomes competitive.\n\nOr maybe it does not pan out at all. We are still at the stage where people\nare throwing everything at the wall to see what sticks. Some promising ideas\nwhich work at small scale do not work at bigger.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503667&how=up&goto=item%3Fid%3D39501982) |\n\n[CuriouslyC](user?id=CuriouslyC) [77 days ago](item?id=39503667) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nThis.", "start_char_idx": 37904, "end_char_idx": 41146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "574edc77-b679-4f4b-811a-cf5ec3565d88": {"__data__": {"id_": "574edc77-b679-4f4b-811a-cf5ec3565d88", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77b729dc-066f-457d-b665-9b982766edeb", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "178ca7c51bbec269ed0c8f902c35c704c53f7bd1a9f0d19397dc6c3f284abf43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "node_type": "1", "metadata": {}, "hash": "df39754a7b9ee2bc063011efd1efd41334cdd613678fe98d233793c4accd8415", "class_name": "RelatedNodeInfo"}}, "text": "Even if this architecture is technically\nsuperior, it could take time before a model using it becomes competitive.\n\nOr maybe it does not pan out at all. We are still at the stage where people\nare throwing everything at the wall to see what sticks. Some promising ideas\nwhich work at small scale do not work at bigger.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503667&how=up&goto=item%3Fid%3D39501982) |\n\n[CuriouslyC](user?id=CuriouslyC) [77 days ago](item?id=39503667) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nThis. Hyperparameter tuning and training include a lot of model specific black\nmagic. Transformers have had time to mature, it'll take a while for other\nstuff to catch up even if they have a higher potential ceiling.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504083&how=up&goto=item%3Fid%3D39501982) |\n\n[koayon](user?id=koayon) [77 days ago](item?id=39504083) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nDefinitely agree that a lot of work going into hyperparameter tuning and\nmaturing the ecosystem will be key here!\n\nI'm seeing the Mamba paper as the `Attention Is All You Need` of Mamba - it\nmight take a little while before we get everything optimised to the point of a\nGPT-4 (it took 6 years for transformers but should be faster than that now\nwith all the attention on ML)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504087&how=up&goto=item%3Fid%3D39501982) |\n\n[koayon](user?id=koayon) [77 days ago](item?id=39504087) | root | parent |\nprev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nAnother interesting one is that the hardware isn't really optimised for Mamba\nyet either - ideally we'd want more of the fast SRAM so that we can store more\nlarger hidden states efficiently  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503640&how=up&goto=item%3Fid%3D39501982) |\n\n[godelski](user?id=godelski) [77 days ago](item?id=39503640) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nYes and no.\n\nThe thing is that Mamba is not perfect. There's no neural architecture to rule\nthem all, if you will. I think the bigger issue is that we more act like there\nis and get on bandwagons. Let me give a clearer example from the past so we\ncan see. The predecessor to DDPM (the work that kicked off the diffusion model\nera) was published in 2015[0], only a year after GANs[1]. Diffusion then\nshowed promise but why did DDPM come out in 2020[2]? Because everyone was\nworking on GANs. GANs produced far better images and diffusion (still is) was\na lot more computationally intensive. Plus, all the people working on these\ndiffusion models were in the same camp as those working on Normalizing Flows\nand other density based models, and fewer people are interested in\nunderstanding density estimation.\n\nSo the entire problem is that the community hopped on a singular railroad for\nresearch direction. There was still working going on in that direction but it\nwasn't nearly getting the attention that GANs got. It's hard to know if things\nwere blocked from publication because they weren't as good as GANs. I can say\nfrom personal experience I had a Flow publication blocked because reviewers\nwere concerned with its quality compared to GANs (this was 2019/2020, this\npaper will never be published because now it is even hard to publish a GAN\nwork).\n\nSo yes and no because there is certainly railroading happening but there are\nalso real critiques to Mamba. But what people often forget is that it is\nincredibly naive to compare new methods to existing methods on a direct one-\nto-one comparison. You're comparing something that has hundreds of hours to\nthousands of hours from a handful to a few dozen eyes against works with\nmillions of hours and millions of eyes.", "start_char_idx": 40611, "end_char_idx": 44314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c2f69e74-8aad-438f-b1b3-6ec9c307741f": {"__data__": {"id_": "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "574edc77-b679-4f4b-811a-cf5ec3565d88", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "52279eb9a8c314578720d3dfeeecfffc3aced0c20457a38688b7daf569f21bae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "node_type": "1", "metadata": {}, "hash": "6a82a80df9e18cb4ab06b8d4e59229846ac186bfd4f23047859d8ea3cb0749a6", "class_name": "RelatedNodeInfo"}}, "text": "There was still working going on in that direction but it\nwasn't nearly getting the attention that GANs got. It's hard to know if things\nwere blocked from publication because they weren't as good as GANs. I can say\nfrom personal experience I had a Flow publication blocked because reviewers\nwere concerned with its quality compared to GANs (this was 2019/2020, this\npaper will never be published because now it is even hard to publish a GAN\nwork).\n\nSo yes and no because there is certainly railroading happening but there are\nalso real critiques to Mamba. But what people often forget is that it is\nincredibly naive to compare new methods to existing methods on a direct one-\nto-one comparison. You're comparing something that has hundreds of hours to\nthousands of hours from a handful to a few dozen eyes against works with\nmillions of hours and millions of eyes. Evaluation is just a really fucking\nhard thing to do but it is easy to just look at some benchmarks, even if they\ndon't mean much. This is a fairly generalization notion though, so take the\nlesson to heart. But Mamba seems a bit different than our diffusion/GAN story,\nin that it is getting more attention than diffusion did in the 2016-2019.\n\n[0] [https://arxiv.org/abs/1503.03585](https://arxiv.org/abs/1503.03585)\n\n[1] [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)\n\n[2] [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503345&how=up&goto=item%3Fid%3D39501982) |\n\n[lamson](user?id=lamson) [77 days ago](item?id=39503345) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI don't really think big tech have that much control. They are aiming for\noptimal profit solution, thing likes AI monopoly with huge self-supervised\nlearning just popped up recently when ChatGPT performs really well, a couple\nyears ago, people still believed modular and supervised learning is the key to\nAI application. So simply current scaling deep learning/llm is most promising\nand it works while tradional methods don't. If there is something that works\nas good as current solution and requires less resource, they will go for it\nvery fast, see the implementation of Flash attention as an example.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503424&how=up&goto=item%3Fid%3D39501982) |\n\n[imjonse](user?id=imjonse) [77 days ago](item?id=39503424) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nLow adoption is primarily caused by it being relatively recent and there are\nno 7B or larger public Mamba-based models to start a comparison in earnest\nwith widely used transformer based LLMs.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504027&how=up&goto=item%3Fid%3D39501982) |\n\n[ssivark](user?id=ssivark) [77 days ago](item?id=39504027) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nIt\u2019s less compute _for the same model sizes_. Rest assured that there will\nstill be a race to scale model sizes (and data) to achieve better performance.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503269&how=up&goto=item%3Fid%3D39501982) |\n\n[jahewson](user?id=jahewson) [77 days ago](item?id=39503269) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nThis early in the game? No. If LLMs become vastly cheaper and faster then\nadoption (and model size) will increase in line with that.  \n  \n---|---|---  \n| !", "start_char_idx": 43450, "end_char_idx": 46808, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53": {"__data__": {"id_": "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "47a82d6e34cccf0c0eb15ff971f0cceee8e91f9eedf34b599e7204eb26297699", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2907aaa9-a729-444e-b31d-012d61232289", "node_type": "1", "metadata": {}, "hash": "99e75d292645aaafca0fb16aba4fae5ab4138008518b3fa1b3a005d1c56c9c31", "class_name": "RelatedNodeInfo"}}, "text": "Rest assured that there will\nstill be a race to scale model sizes (and data) to achieve better performance.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503269&how=up&goto=item%3Fid%3D39501982) |\n\n[jahewson](user?id=jahewson) [77 days ago](item?id=39503269) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nThis early in the game? No. If LLMs become vastly cheaper and faster then\nadoption (and model size) will increase in line with that.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503600&how=up&goto=item%3Fid%3D39501982) |\n\n[nbardy](user?id=nbardy) [77 days ago](item?id=39503600) | parent | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nNo these things just take time.\n\nThere is no conspiracy again efficient training. Companies aren\u2019t going to\nlower compute budgets with more efficiency.\n\nAll the top labs are increasing efficiency, but they are using that to get\nmore out of their large runs not spend less. Most companies have a relatively\nfixed training budget for their large runs and are trying to get the most out\nof it, bot save money,\n\nMamba is actually being scaled up and tested across other fields(bio) at a\nrapid pace compared to other architectures  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503694&how=up&goto=item%3Fid%3D39501982) |\n\n[godelski](user?id=godelski) [77 days ago](item?id=39503694) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\n> There is no conspiracy\n\nFwiw, the OP isn't suggesting conspiracy. The notion is more about convergent\nthinking.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39508014&how=up&goto=item%3Fid%3D39501982) |\n\n[kgeist](user?id=kgeist) [77 days ago](item?id=39508014) | prev | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nIf Mamba selectively forgets \"unnecessary\" details, can it repeat the input\nverbatim (if asked)?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39511174&how=up&goto=item%3Fid%3D39501982) |\n\n[hackerlight](user?id=hackerlight) [77 days ago](item?id=39511174) | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nIf you ask at the end of the prompt then it may have already deliberately\ntossed the information it deemed irrelevant prior to the question. These\naren't transformers. In general the recall for arbitrary information will be\nworse.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39513509&how=up&goto=item%3Fid%3D39501982) |\n\n[kgeist](user?id=kgeist) [76 days ago](item?id=39513509) | root | parent |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nSo the questions should come before the content and it might work?\n\nI think that's how also RWKV works.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39518481&how=up&goto=item%3Fid%3D39501982) |\n\n[hackerlight](user?id=hackerlight) [76 days ago](item?id=39518481) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nIt's known to help although I wouldn't expect it to be perfect recall unless\nthe network is big enough.\n\nThe network will read the data token by token. So if you put the question at\nthe beginning it will know what information it needs to pay attention to\ninside the rest of your context. Of course, if the network is too small, it\nstill won't be perfect recall for a sufficiently complicated/large\nquestion/context.  \n  \n---|---|---  \n| !", "start_char_idx": 46337, "end_char_idx": 49528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2907aaa9-a729-444e-b31d-012d61232289": {"__data__": {"id_": "2907aaa9-a729-444e-b31d-012d61232289", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "8035ac9cc3caf5820a837bee7495e4811c99d963d0c9059abb62fa81b3aa03e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5881d9b-b288-4022-8f13-c4984cba5118", "node_type": "1", "metadata": {}, "hash": "38d16ebec3bc573ba081608b37e44a80dbd775c93894c1f4cf4a9805f7ff00e3", "class_name": "RelatedNodeInfo"}}, "text": "I think that's how also RWKV works.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39518481&how=up&goto=item%3Fid%3D39501982) |\n\n[hackerlight](user?id=hackerlight) [76 days ago](item?id=39518481) | root |\nparent | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nIt's known to help although I wouldn't expect it to be perfect recall unless\nthe network is big enough.\n\nThe network will read the data token by token. So if you put the question at\nthe beginning it will know what information it needs to pay attention to\ninside the rest of your context. Of course, if the network is too small, it\nstill won't be perfect recall for a sufficiently complicated/large\nquestion/context.  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39511249&how=up&goto=item%3Fid%3D39501982) |\n\n[alok-g](user?id=alok-g) [77 days ago](item?id=39511249) | prev | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nA potentially naive question. Isn't this modeled like a Kalman Filter?\n\nEdit: Sounds like it is.\n[https://openreview.net/pdf?id=AL1fq05o7H](https://openreview.net/pdf?id=AL1fq05o7H)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39505684&how=up&goto=item%3Fid%3D39501982) |\n\n[kken](user?id=kken) [77 days ago](item?id=39505684) | prev | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nThere is also this:\n[https://jackcook.com/2024/02/23/mamba.html](https://jackcook.com/2024/02/23/mamba.html)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39502299&how=up&goto=item%3Fid%3D39501982) |\n\n[fancyfredbot](user?id=fancyfredbot) [77 days ago](item?id=39502299) | prev |\nnext [[\u2013]](javascript:void\\(0\\))\n\n  \n\nSee also:\n[https://jackcook.com/2024/02/23/mamba.html](https://jackcook.com/2024/02/23/mamba.html)  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39502653&how=up&goto=item%3Fid%3D39501982) |\n\n[thecolorgreen](user?id=thecolorgreen) [77 days ago](item?id=39502653) | prev\n| next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nWhy doesn't Equation 1b use the h' defined in Equation 1a?  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504144&how=up&goto=item%3Fid%3D39501982) |\n\n[koayon](user?id=koayon) [77 days ago](item?id=39504144) | parent | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nHey! OP here Great question - h' in Equation 1a refers to the derivative of h\nwith respect to time (t). This is a differential equation which we can solve\nmathematically when we have x in order to get a closed-form solution for h. We\nwould then plug in that h (the hidden state) into equation 1b.\n\nIn our case, we don't actually wait for a closed-form solution but instead\ncompute the discrete representation (Equation 2)\n\nHope that helps!  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39502900&how=up&goto=item%3Fid%3D39501982) |\n\n[atlacatl_sv](user?id=atlacatl_sv) [77 days ago](item?id=39502900) | parent |\nprev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI believe h' is for the next state. y(t) is to predict the next word so it\nuses the current hidden state h(t).  \n  \n---|---|---  \n| !", "start_char_idx": 48840, "end_char_idx": 51741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e5881d9b-b288-4022-8f13-c4984cba5118": {"__data__": {"id_": "e5881d9b-b288-4022-8f13-c4984cba5118", "embedding": null, "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d", "node_type": "4", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "7a05068eed8924dbeac825421075f174ac8fc9665479b9661d751847ccb5cc1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2907aaa9-a729-444e-b31d-012d61232289", "node_type": "1", "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}, "hash": "212b5242487a43595e227d3ae0fa2260b5e26db59254b782596b4bf49540c6c4", "class_name": "RelatedNodeInfo"}}, "text": "In our case, we don't actually wait for a closed-form solution but instead\ncompute the discrete representation (Equation 2)\n\nHope that helps!  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39502900&how=up&goto=item%3Fid%3D39501982) |\n\n[atlacatl_sv](user?id=atlacatl_sv) [77 days ago](item?id=39502900) | parent |\nprev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nI believe h' is for the next state. y(t) is to predict the next word so it\nuses the current hidden state h(t).  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39504538&how=up&goto=item%3Fid%3D39501982) |\n\n[givemeethekeys](user?id=givemeethekeys) [77 days ago](item?id=39504538) |\nprev | next [[\u2013]](javascript:void\\(0\\))\n\n  \n\nTransformers, Rise of the Mambas, coming to a theater near you!  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39506645&how=up&goto=item%3Fid%3D39501982) |\n\n[vanjajaja1](user?id=vanjajaja1) [77 days ago](item?id=39506645) | prev | next\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nI really enjoyed this article, thanks  \n  \n---|---|---  \n| ![](s.gif)|  [](vote?id=39503923&how=up&goto=item%3Fid%3D39501982) |\n\n[AndrewKemendo](user?id=AndrewKemendo) [77 days ago](item?id=39503923) | prev\n[[\u2013]](javascript:void\\(0\\))\n\n  \n\nSomeone is going to re-invent Bellman's equations and call it Learnformer  \n  \n---|---|---  \n  \n  \n  \n![](s.gif)|  \n---  \n  \n[Guidelines](newsguidelines.html) | [FAQ](newsfaq.html) | [Lists](lists) |\n[API](https://github.com/HackerNews/API) | [Security](security.html) |\n[Legal](https://www.ycombinator.com/legal/) | [Apply to\nYC](https://www.ycombinator.com/apply/) | [Contact](mailto:hn@ycombinator.com)  \n  \nSearch:", "start_char_idx": 51252, "end_char_idx": 52855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b0c70401-bfb8-460f-8f12-23f21c5e8a7e": {"__data__": {"id_": "b0c70401-bfb8-460f-8f12-23f21c5e8a7e", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e62f7b37-ded4-4d30-a2e7-5574679b2e2c", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "8c92f38ed71552b9e0a7108f6b90a564f13fa65891b3f13a5a60a6b2fe49513d", "class_name": "RelatedNodeInfo"}}, "text": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces\nAlbert Gu *1and Tri Dao *2\n1Machine Learning Department, Carnegie Mellon University\n2Department of Computer Science, Princeton University\nagu@cs.cmu.edu ,tri@tridao.me\nAbstract\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally\nbased on the Transformer architecture and its core attention module. Many subquadratic-time architectures\nsuch as linear attention, gated convolution and recurrent models, and structured state space models (SSMs)\nhave been developed to address Transformers\u2019 computational ine\ufb03ciency on long sequences, but they have not\nperformed as well as attention on important modalities such as language. We identify that a key weakness of\nsuch models is their inability to perform content-based reasoning, and make several improvements. First, simply\nletting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing\nthe model to selectively propagate or forget information along the sequence length dimension depending on\nthe current token. Second, even though this change prevents the use of e\ufb03cient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpli\ufb01ed\nend-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast\ninference (5 \u00d7higher throughput than Transformers) and linear scaling in sequence length, and its performance\nimproves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves\nstate-of-the-art performance across several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice\nits size, both in pretraining and downstream evaluation.\n1 Introduction\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have\nemerged as an e\ufb00ective paradigm in modern machine learning. The backbone of these FMs are often sequence\nmodels, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images,\nspeech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019;\nOord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to\na particular choice of model architecture, modern FMs are predominantly based on a single type of sequence\nmodel: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015)\nThe e\ufb03cacy of self-attention is attributed to its ability to route information densely within a context window,\nallowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model\nanything outside of a \ufb01nite window, and quadratic scaling with respect to the window length. An enormous body\nof research has appeared on more e\ufb03cient variants of attention to overcome these drawbacks (Tay, Dehghani,\nBahri, et al. 2022), but often at the expense of the very properties that makes it e\ufb00ective. As of yet, none of these\nvariants have been shown to be empirically e\ufb00ective at scale across domains.\nRecently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021)\nhave emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a\ncombination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration\nfrom classical state space models (Kalman 1960). This class of models can be computed very e\ufb03ciently as either a\nrecurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled\n*Equal contribution.\n1", "start_char_idx": 0, "end_char_idx": 3891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "acae008e-b7f6-41f8-937a-f4314768f9ee": {"__data__": {"id_": "acae008e-b7f6-41f8-937a-f4314768f9ee", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb7aec31-7ff3-4868-ba84-4cb19de2258e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "f1ef4c77042682e0b69793ed87dd14f11534d48a73b872c79f4be9a2b7f392dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8083740-bf46-4e25-847f-f48a846b583e", "node_type": "1", "metadata": {}, "hash": "5aa0cbd3d9386d091dd410ed26f718a45020a561dae2503f08621262d0afe660", "class_name": "RelatedNodeInfo"}}, "text": "mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have\ndominated benchmarks such as the Long Range Arena (Tay, Dehghani, Abnar, et al. 2021). Many \ufb02avors of\nSSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al.\n2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving\ncontinuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui\n2023). However, they have been less e\ufb00ective at modeling discrete and information-dense data such as text.\nWe propose a new class of selective state space models, that improves on prior work on several axes to achieve the\nmodeling power of Transformers while scaling linearly in sequence length.\nSelection Mechanism. First, we identify a key limitation of prior models: the ability to e\ufb03ciently select\ndata in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on\nimportant synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by\nparameterizing the SSM parameters based on the input. This allows the model to \ufb01lter out irrelevant information\nand remember relevant information inde\ufb01nitely.\nHardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model;\nin fact, all prior SSMs models must be time- and input-invariant in order to be computationally e\ufb03cient. We\novercome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of\nconvolution, but does not materialize the expanded state in order to avoid IO access between di\ufb00erent levels of the\nGPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling\nlinearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware\n(up to 3 \u00d7faster on A100 GPUs).\nArchitecture. We simplify prior deep sequence model architectures by combining the design of prior SSM\narchitectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a\nsimple and homogenous architecture design (Mamba) incorporating selective state spaces.\nSelective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that\nmake them suitable as the backbone of general foundation models operating on sequences. (i)High quality:\nselectivity brings strong performance on dense modalities such as language and genomics. (ii)Fast training and\ninference: computation and memory scales linearly in sequence length during training, and unrolling the model\nautoregressively during inference requires only constant time per step since it does not require a cache of previous\nelements. (iii)Long context: the quality and e\ufb03ciency together yield performance improvements on real data up\nto sequence length 1M.\nWe empirically validate Mamba\u2019s potential as a general sequence FM backbone, in both pretraining quality and\ndomain-speci\ufb01c task performance, on several types of modalities and settings:\n\u2022Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being\nkey to large language models, Mamba not only solves them easily but can extrapolate solutions inde\ufb01nitely long\n(>1M tokens).\n\u2022Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform-\ners on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g.\nreducing FID on a challenging speech generation dataset by more than half). In both settings, its performance\nimproves with longer context up to million-length sequences.\n\u2022Language Modeling. Mamba is the \ufb01rst linear-time sequence model that truly achieves Transformer-quality\nperformance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters,\nwe show that Mamba exceeds the performance of a large range of baselines, including very strong modern\nTransformer training recipes based on LLaMa (Touvron et al. 2023).", "start_char_idx": 0, "end_char_idx": 4195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e8083740-bf46-4e25-847f-f48a846b583e": {"__data__": {"id_": "e8083740-bf46-4e25-847f-f48a846b583e", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb7aec31-7ff3-4868-ba84-4cb19de2258e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "f1ef4c77042682e0b69793ed87dd14f11534d48a73b872c79f4be9a2b7f392dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acae008e-b7f6-41f8-937a-f4314768f9ee", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c28880fe837b47c37933987ada5b50e76c2a6b312881ff972caad166df3f64ab", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform-\ners on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g.\nreducing FID on a challenging speech generation dataset by more than half). In both settings, its performance\nimproves with longer context up to million-length sequences.\n\u2022Language Modeling. Mamba is the \ufb01rst linear-time sequence model that truly achieves Transformer-quality\nperformance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters,\nwe show that Mamba exceeds the performance of a large range of baselines, including very strong modern\nTransformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5 \u00d7\ngeneration throughput compared to Transformers of similar size, and Mamba-3B\u2019s quality matches that of\nTransformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and\neven exceeding Pythia-7B).\nModel code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba .\n2", "start_char_idx": 3422, "end_char_idx": 4566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4954475e-ca1f-477c-be65-12867b8b31d1": {"__data__": {"id_": "4954475e-ca1f-477c-be65-12867b8b31d1", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c8587f2-e3ce-47e6-be70-740471f08c36", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "45200a27b7332971c2d0677d54d7075088fd2d402aad8071521211463edcf5fd", "class_name": "RelatedNodeInfo"}}, "text": "ProjectDiscretize\ud835\udc65!\u210e!\"#\u210e!\ud835\udc66!\ud835\udc34\ud835\udc36!\ud835\udc35!Selection MechanismGPU SRAMGPU HBM\u2206!Selective State Space ModelwithHardware-aware State ExpansionFigure 1: ( Overview .) Structured SSMs independently map each channel (e.g. /u1D437= 5) of an input xto output /u1D466through a higher\ndimensional latent state /uni210E(e.g./u1D441= 4). Prior SSMs avoid materializing this large e\ufb00ective state ( /u1D437/u1D441, times batch size /u1D435and sequence\nlength /u1D43F) through clever alternate computation paths requiring time-invariance: the .\u2206,A,B,C)parameters are constant across\ntime. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to\nonly materialize the expanded states in more e\ufb03cient levels of the GPU memory hierarchy.\n2 State Space Models\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are\nbroadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous\nsystem (1)that maps a 1-dimensional function or sequence x./u1D461) /uni2208/uni211D/uni21A6/u1D466./u1D461) /uni2208/uni211Dthrough an implicit latent state\n/uni210E./u1D461) /uni2208/uni211D/u1D441.\nConcretely, S4 models are de\ufb01ned with four parameters .\u2206,A,B,C), which de\ufb01ne a sequence-to-sequence trans-\nformation in two stages.\n/uni210E/minute.var./u1D461) =A/uni210E./u1D461) +Bx./u1D461) (1a)\n/u1D466./u1D461) =C/uni210E./u1D461) (1b)/uni210E/u1D461=A/uni210E/u1D461/uni22121+Bx/u1D461 (2a)\n/u1D466/u1D461=C/uni210E/u1D461 (2b)/u1D472= .C/u1D469,C/u1D468/u1D469,\u2026,C/u1D468/u1D458\n/u1D469,\u2026 ) (3a)\n/u1D466=x</u1D472 (3b)\nDiscretization. The \ufb01rst stage transforms the \u201ccontinuous parameters\u201d .\u2206,A,B)to \u201cdiscrete parameters\u201d .A,B)\nthrough \ufb01xed formulas A=/u1D453/u1D434.\u2206,A)andB=/u1D453/u1D435.\u2206,A,B), where the pair ./u1D453/u1D434, /u1D453/u1D435)is called a discretization rule.\nVarious rules can be used such as the zero-order hold (ZOH) de\ufb01ned in equation (4).\nA= exp.\u2206 A)B= .\u2206A)/uni22121.exp.\u2206A) /uni2212I)/uni22C5\u2206B (4)\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties\nsuch as resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly\nnormalized (Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023). It also has connections to gating mechanisms\nof RNNs (Gu, Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5. However, from\na mechanical point of view discretization can simply be viewed as the \ufb01rst step of the computation graph in the\nforward pass of an SSM. Alternate \ufb02avors of SSMs can bypass the discretization step and parameterize .A,B)\ndirectly instead (Zhang et al. 2023), which may be easier to reason about.\nComputation. After the parameters have been transformed from .\u2206,A,B,C)/uni21A6.A,B,C), the model can be\ncomputed in two ways, either as a linear recurrence (2)or a global convolution (3).\n3", "start_char_idx": 0, "end_char_idx": 2982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "11a8c62a-4d0a-4df1-a783-ac0db5a52475": {"__data__": {"id_": "11a8c62a-4d0a-4df1-a783-ac0db5a52475", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5feee89b-d75f-4a27-b176-f436afeafc46", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "949650d126bca1fc0d3d8640b075c476bc9a1fca766e53d2c7642bed6027621c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1304b34-ed89-4aee-ad07-2596c341ea9c", "node_type": "1", "metadata": {}, "hash": "5ebd556bb5a49fd21fb8d9719ac1ac36c5f21297f1d0e1917ca214bdc4185812", "class_name": "RelatedNodeInfo"}}, "text": "Commonly, the model uses the convolutional mode (3)for e\ufb03cient parallelizable training (where the whole input\nsequence is seen ahead of time), and switched into recurrent mode (2)for e\ufb03cient autoregressive inference (where\nthe inputs are seen one timestep at a time).\nLinear Time Invariance (LTI). An important property of equations (1)to(3)is that the model\u2019s dynamics are\nconstant through time. In other words .\u2206,A,B,C), and consequently .A,B)as well, are \ufb01xed for all time-steps.\nThis property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions.\nInformally, we think of LTI SSMs as being equivalent to any linear recurrence (2a)or convolution (3b), and use\nLTI as an umbrella term for these classes of models.\nThus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental e\ufb03ciency\nconstraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental\nlimitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint\nwhile overcoming the e\ufb03ciency bottlenecks.\nStructure and Dimensions. Finally, we note that structured SSMs are so named because computing them\ne\ufb03ciently also requires imposing structure on the Amatrix. The most popular form of structure is diagonal\n(Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also\nuse.\nIn this case, the A/uni2208/uni211D/u1D441\u00d7/u1D441,B/uni2208/uni211D/u1D441\u00d71,C/uni2208/uni211D1\u00d7/u1D441matrices can all be represented by /u1D441numbers. To operate over\nan input sequence xof batch size /u1D435and length /u1D43Fwith /u1D437channels, the SSM is applied independently to each\nchannel. Note that in this case, the total hidden state has dimension /u1D437/u1D441per input, and computing it over the\nsequence length requires /u1D442./u1D435/u1D43F/u1D437/u1D441 )time and memory; this is the root of the fundamental e\ufb03ciency bottleneck\naddressed in Section 3.3.\nGeneral State Space Models. We note that the term state space model has a very broad meaning which simply\nrepresents the notion of any recurrent process with a latent state. It has been used to refer to many disparate\nconcepts in di\ufb00erent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner\net al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)),\nKalman \ufb01lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS)\n(machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).\nThroughout this entire paper we use the term \u201cSSM\u201d to refer exclusively to the class of structured SSMs or S4\nmodels (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al.\n2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably. For convenience we may also\ninclude derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution\nviewpoints (Y. Li et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary.\nSSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end\nneural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as\nCNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of\nwhich will also serve as our primary baselines.\n\u2022Linear attention (Katharopoulos et al.", "start_char_idx": 0, "end_char_idx": 3623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c1304b34-ed89-4aee-ad07-2596c341ea9c": {"__data__": {"id_": "c1304b34-ed89-4aee-ad07-2596c341ea9c", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5feee89b-d75f-4a27-b176-f436afeafc46", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "949650d126bca1fc0d3d8640b075c476bc9a1fca766e53d2c7642bed6027621c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11a8c62a-4d0a-4df1-a783-ac0db5a52475", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "9c25c46494c01e496b8d67677a9c653515cc6f552f1e6ac9c7169b9d58d562c2", "class_name": "RelatedNodeInfo"}}, "text": "2023; Ma et al.\n2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably. For convenience we may also\ninclude derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution\nviewpoints (Y. Li et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary.\nSSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end\nneural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as\nCNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of\nwhich will also serve as our primary baselines.\n\u2022Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which\ncan be viewed as a degenerate linear SSM.\n\u2022H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with\nan SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which\nthey frame as a shift-SSM, before the main SSM layer.\n\u2022Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized\nglobal convolution (Romero et al. 2021).\n\u2022RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing\nan alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of\nconvolutions.\n4", "start_char_idx": 2873, "end_char_idx": 4391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "53585b40-5885-495c-b4d4-48f394faecf3": {"__data__": {"id_": "53585b40-5885-495c-b4d4-48f394faecf3", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "922c8bcd-01c9-4d72-b78a-c5e4b19dd874", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "138d6618aecf4c3e4791c1cd41511237a338cecb8c21ba1f62454ad6ec1882c5", "class_name": "RelatedNodeInfo"}}, "text": "\u2022RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention\napproximation (attention-free Transformer (S. Zhai et al. 2021)). Its main \u201cWKV\u201d mechanism involves LTI\nrecurrences and can be viewed as the ratio of two SSMs.\nOther closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We\nhighlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei\net al. 2017), which we view as the most closely related methods to our core selective SSM.\n3 Selective State Space Models\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to\nincorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot\nuse convolutions, presenting a technical challenge of how to compute them e\ufb03ciently. We overcome this with\na hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then\ndescribe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some\nadditional properties of selection mechanisms (Section 3.5).\n3.1 Motivation: Selection as a Means of Compression\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact,\nwe can view the tradeo\ufb00s of popular sequence models from this point of view. For example, attention is both\ne\ufb00ective and ine\ufb03cient because it explicitly does not compress context at all. This can be seen from the fact that\nautoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the\nslow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are\ne\ufb03cient because they have a \ufb01nite state, implying constant-time inference and linear-time training. However, their\ne\ufb00ectiveness is limited by how well this state has compressed the context.\nTo understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\n\u2022The Selective Copying task modi\ufb01es the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying\nthe position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant\ntokens (colored) and \ufb01lter out the irrelevant ones (white).\n\u2022The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning\nabilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct\noutput in the appropriate context (black).\nThese tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the\n.A,B)transitions in (2)) cannot let them select the correct information from their context, or a\ufb00ect the hidden\nstate passed along the sequence an in input-dependent way. From the convolutional view, it is known that global\nconvolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness,\nbut that they have di\ufb03culty with the Selective Copying task because of lack of content-awareness (Figure 2).\nMore concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution\nkernels.\nIn summary, the e\ufb03ciency vs. e\ufb00ectiveness tradeo\ufb00 of sequence models is characterized by how well they compress\ntheir state: e\ufb03cient models must have a small state, while e\ufb00ective models must have a state that contains all\nnecessary information from the context. In turn, we propose that a fundamental principle for building sequence\nmodels is selectivity: or the context-aware ability to focus on or \ufb01lter out inputs into a sequential state. In\nparticular, a selection mechanism controls how information propagates or interacts along the sequence dimension\n(see Section 3.5for more discussion).\n3.2 Improving SSMs with Selection\nOne method of incorporating a selection mechanism into models is by letting their parameters that a\ufb00ect\ninteractions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN) be\ninput-dependent.\n5", "start_char_idx": 0, "end_char_idx": 4190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4936d8bd-2b2d-47e2-aeca-f9b2b24f20cc": {"__data__": {"id_": "4936d8bd-2b2d-47e2-aeca-f9b2b24f20cc", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dc0c5ce-56b6-4f45-ac77-040d89765751", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "7a74afcf123dbddf0e3b1219c338f7c3303d86764888906511aaaae74f731cdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c81d1514-1080-4955-9760-44da8e3e7033", "node_type": "1", "metadata": {}, "hash": "399a6e951ce9c2672e52a012625fd6f622e31bbce4f71f1e2cfa698d5f07228c", "class_name": "RelatedNodeInfo"}}, "text": "InputOutput\n?OutputCopyingSelective CopyingInputInduction HeadsSolutionPerfectly solved by LTI (e.g.convolutional) models that do not need to look at the actual inputsFigure 2: ( Left) The standard version of the Copying task involves constant spacing between input and output elements and is\neasily solved by time-invariant models such as linear recurrences and global convolutions. ( Right Top ) The Selective Copying task\nhas random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending\non their content. ( Right Bottom ) The Induction Heads task is an example of associative recall that requires retrieving an answer\nbased on context, a key ability for LLMs.\nAlgorithm 1 SSM (S4)\nInput: x/uni2236 ./u1D671,/u1D67B,/u1D673)\nOutput: /u1D466/uni2236 ./u1D671,/u1D67B,/u1D673)\n1:A/uni2236 ./u1D673,/u1D67D)} /u1D5AF/u1D5BA/u1D5CB/u1D5BA/u1D5C6e/u1D5CDe/u1D5CB\n.Represents structured /u1D441\u00d7/u1D441matrix\n2:B/uni2236 ./u1D673,/u1D67D)} /u1D5AF/u1D5BA/u1D5CB/u1D5BA/u1D5C6e/u1D5CDe/u1D5CB\n3:C/uni2236 ./u1D673,/u1D67D)} /u1D5AF/u1D5BA/u1D5CB/u1D5BA/u1D5C6e/u1D5CDe/u1D5CB\n4:\u2206 /uni2236 .", "start_char_idx": 0, "end_char_idx": 1147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c81d1514-1080-4955-9760-44da8e3e7033": {"__data__": {"id_": "c81d1514-1080-4955-9760-44da8e3e7033", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dc0c5ce-56b6-4f45-ac77-040d89765751", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "7a74afcf123dbddf0e3b1219c338f7c3303d86764888906511aaaae74f731cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4936d8bd-2b2d-47e2-aeca-f9b2b24f20cc", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "a379af8fd4083645bcadd3fbe46aaba84deec1bde9d2a4a5ffcfd34fba9de409", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "254dd1b3-fd70-441e-a921-87375324c580", "node_type": "1", "metadata": {}, "hash": "a31adbff4d4857e8041aa5ce4d159dde1ffbb0b7524e2183c4b74aff15c0d2e6", "class_name": "RelatedNodeInfo"}}, "text": "/u1D673)}/u1D70F\u2206./u1D5AF/u1D5BA/u1D5CB/u1D5BA/u1D5C6e/u1D5CDe/u1D5CB )\n5:A,B/uni2236 ./u1D673,/u1D67D)} /u1D5BD/u1D5C2/u1D5CC/u1D5BC/u1D5CBe/u1D5CD/u1D5C2/u1D5D3e .\u2206,A,B)\n6:/u1D466} /u1D5B2/u1D5B2/u1D5AC .A,B,C).x)\n.Time-invariant: recurrence or convolution\n7:return /u1D466Algorithm 2 SSM + Selection (S6)\nInput: x/uni2236 ./u1D671,/u1D67B,/u1D673)\nOutput: /u1D466/uni2236 ./u1D671,/u1D67B,/u1D673)\n1:A/uni2236 ./u1D673,/u1D67D)} /u1D5AF/u1D5BA/u1D5CB/u1D5BA/u1D5C6e/u1D5CDe/u1D5CB\n.Represents structured /u1D441\u00d7/u1D441matrix\n2:B/uni2236./u1D671,/u1D67B,/u1D67D)}/u1D460/u1D435.x)\n3:C/uni2236./u1D671,/u1D67B,/u1D67D)}/u1D460/u1D436.x)\n4:\u2206 /uni2236./u1D671,/u1D67B,/u1D673)}/u1D70F\u2206./u1D5AF/u1D5BA/u1D5CB/u1D5BA/u1D5C6e/u1D5CDe/u1D5CB +/u1D460\u2206.x))\n5:A,B/uni2236./u1D671,/u1D67B,/u1D673,/u1D67D)} /u1D5BD/u1D5C2/u1D5CC/u1D5BC/u1D5CBe/u1D5CD/u1D5C2/u1D5D3e .\u2206,A,B)\n6:/u1D466} /u1D5B2/u1D5B2/u1D5AC .A,B,C).x)\n.Time-varying : recurrence ( scan) only\n7:return /u1D466\nAlgorithms 1and2illustrates the main selection mechanism that we use. The main di\ufb00erence is simply making\nseveral parameters \u2206,B,Cfunctions of the input, along with the associated changes to tensor shapes throughout.\nIn particular, we highlight that these parameters now have a length dimension /u1D43F, meaning that the model has\nchanged from time-invariant to time-varying. (Note that shape annotations were described in Section 2). This\nloses the equivalence to convolutions (3)with implications for its e\ufb03ciency, discussed next.\nWe speci\ufb01cally choose /u1D460/u1D435.x) =/u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB /u1D441.x),/u1D460/u1D436.x) =/u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB /u1D441.x),/u1D460\u2206.x) =/u1D5A1/u1D5CBo/u1D5BA/u1D5BD/u1D5BC/u1D5BA/u1D5CC/u1D5CD /u1D437./u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB 1.x)), and /u1D70F\u2206=/u1D5CCo/u1D5BF/u1D5CDpl/u1D5CE/u1D5CC ,\nwhere /u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB /u1D451is a parameterized projection to dimension /u1D451.", "start_char_idx": 1148, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "254dd1b3-fd70-441e-a921-87375324c580": {"__data__": {"id_": "254dd1b3-fd70-441e-a921-87375324c580", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dc0c5ce-56b6-4f45-ac77-040d89765751", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "7a74afcf123dbddf0e3b1219c338f7c3303d86764888906511aaaae74f731cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c81d1514-1080-4955-9760-44da8e3e7033", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c48cb08d6d17de7ac9c162080d3f97983193719f9436e86e089c7744d041f25c", "class_name": "RelatedNodeInfo"}}, "text": "The choice of /u1D460\u2206and/u1D70F\u2206is due to a connection to\nRNN gating mechanisms explained in Section 3.5.\n3.3 E\ufb03cient Implementation of Selective SSMs\nHardware-friendly architectures such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and Transform-\ners (Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs e\ufb03cient on modern\nhardware (GPU) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate\nspecial cases of selection, such as letting \u2206vary over time in recurrent SSMs (Gu, Dao, et al. 2020). However, as\npreviously mentioned a core limitation in the usage of SSMs is their computational e\ufb03ciency, which was why S4\nand all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions.\n3.3.1 Motivation of Prior Models\nWe \ufb01rst revisit this motivation and overview our approach to overcome limitations of prior methods.\n\u2022At a high level, recurrent models such as SSMs always balance a tradeo\ufb00 between expressivity and speed: as\ndiscussed in Section 3.1, models with larger hidden state dimension should be more e\ufb00ective but slower. Thus\n6", "start_char_idx": 3084, "end_char_idx": 4241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c4d470f5-ad3d-4fba-9aa6-ac7ab97438d0": {"__data__": {"id_": "c4d470f5-ad3d-4fba-9aa6-ac7ab97438d0", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36b68b3c-dbfc-41b8-8155-fb1cbf250620", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "762d83e119241d9e98e5ba5841afd0cb3c2aeae91bc0fc3206eaee29c8f74298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff65cb59-cdd1-4749-87a3-d9d4a2921119", "node_type": "1", "metadata": {}, "hash": "3dc0fd43bca0431b17663367f5a1465662610a9303e58ad5b2defad22a6f75b5", "class_name": "RelatedNodeInfo"}}, "text": "we want to maximize hidden state dimension without paying speed and memory costs.\n\u2022Note that the recurrent mode is more \ufb02exible than the convolution mode, since the latter (3)is derived from\nexpanding the former (2)(Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require\ncomputing and materializing the latent state /uni210Ewith shape ./u1D671,/u1D67B,/u1D673,/u1D67D), much larger (by a factor of /u1D441, the SSM\nstate dimension) than the input xand output /u1D466of shape ./u1D671,/u1D67B,/u1D673). Thus the more e\ufb03cient convolution mode was\nintroduced which could bypass the state computation and materializes a convolution kernel (3a)of only ./u1D671,/u1D67B,/u1D673).\n\u2022Prior LTI SSMs leverage the dual recurrent-convolutional forms to increase the e\ufb00ective state dimension by a\nfactor of /u1D441(/uni2248 10 /uni2212 100 ), much larger than traditional RNNs, without e\ufb03ciency penalties.\n3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore\nneed to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion,\nparallel scan, and recomputation. We make two main observations:\n\u2022The naive recurrent computation uses /u1D442./u1D435/u1D43F/u1D437/u1D441 )FLOPs while the convolutional computation uses /u1D442./u1D435/u1D43F/u1D437 log./u1D43F))\nFLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension\n/u1D441, the recurrent mode can actually use fewer FLOPs.\n\u2022The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter,\njust like the convolutional mode, we can attempt to not actually materialize the full state /uni210E.\nThe main idea is to leverage properties of modern accelerators (GPUs) to materialize the state /uni210Eonly in more\ne\ufb03cient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded\nby memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson\n2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to\na signi\ufb01cant speedup compared to a standard implementation.\nConcretely, instead of preparing the scan input .A,B)of size ./u1D671,/u1D67B,/u1D673,/u1D67D)in GPU HBM (high-bandwidth memory),\nwe load the SSM parameters .\u2206,A,B,C)directly from slow HBM to fast SRAM, perform the discretization and\nrecurrence in SRAM, and then write the \ufb01nal outputs of size ./u1D671,/u1D67B,/u1D673)back to HBM.\nTo avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a\nwork-e\ufb03cient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman\n2023).\nFinally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully\napply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not\nstored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the\nfused selective scan layer has the same memory requirements as an optimized transformer implementation with\nFlashAttention.\nDetails of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is\nillustrated in Figure 1.\n3.4 A Simpli/f_ied SSM Architecture\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be \ufb02exibly incorporated\ninto neural networks.", "start_char_idx": 0, "end_char_idx": 3652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ff65cb59-cdd1-4749-87a3-d9d4a2921119": {"__data__": {"id_": "ff65cb59-cdd1-4749-87a3-d9d4a2921119", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36b68b3c-dbfc-41b8-8155-fb1cbf250620", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "762d83e119241d9e98e5ba5841afd0cb3c2aeae91bc0fc3206eaee29c8f74298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4d470f5-ad3d-4fba-9aa6-ac7ab97438d0", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "34511d24c9534837840155137a4a8b9a5a0bf0abbf9656cc51f3376dffbeb573", "class_name": "RelatedNodeInfo"}}, "text": "Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully\napply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not\nstored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the\nfused selective scan layer has the same memory requirements as an optimized transformer implementation with\nFlashAttention.\nDetails of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is\nillustrated in Figure 1.\n3.4 A Simpli/f_ied SSM Architecture\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be \ufb02exibly incorporated\ninto neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which\nare generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron)\nblock. We simplify this architecture by combining these two components into one, which is stacked homogenously\n(Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for\nattention.\nThis architecture involves expanding the model dimension /u1D437by a controllable expansion factor /u1D438. For each\nblock, most of the parameters ( 3/u1D438/u1D4372) are in the linear projections ( 2/u1D438/u1D4372for input projections, /u1D438/u1D4372for output\nprojection) while the inner SSM contributes less. The number of SSM parameters (projections for \u2206,B,C, and\n7", "start_char_idx": 2891, "end_char_idx": 4468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0036613c-beae-4296-9338-35ae15a49fd3": {"__data__": {"id_": "0036613c-beae-4296-9338-35ae15a49fd3", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7e37a30-e3ac-4a18-8790-e1dc7ee33043", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "61d9222fe0be4a0c2fec0eff0c6c8588faba49bf9df41fb82f879293a7e71ff2", "class_name": "RelatedNodeInfo"}}, "text": "H3Gated MLPMambaLinear projectionSequence transformationNonlinearity (activation or multiplication)XXX!XConvSSMX!!ConvSSM\n\u2a02Figure 3: ( Architecture .) Our simpli/f_ied block design combines the H3 block, which is the basis of most SSM architectures, with\nthe ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block\nhomogenously. Compared to the H3 block, Mamba replaces the /f_irst multiplicative gate with an activation function. Compared to\nthe MLP block, Mamba adds an SSM to the main branch. For /u1D70Ewe use the SiLU / Swish activation (Hendrycks and Gimpel 2016 ;\nRamachandran, Zoph, and Quoc V Le 2017 ).\nthe matrix A) are much smaller in comparison. We repeat this block, interleaved with standard normalization\nand residual connections, to form the Mamba architecture. We always \ufb01x to /u1D438= 2in our experiments and use two\nstacks of the block to match the 12/u1D4372parameters of a Transformer\u2019s interleaved MHA (multi-head attention) and\nMLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph,\nand Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \u201cSwiGLU\u201d variant (Chowdhery\net al. 2023; Shazeer 2020; Touvron et al. 2023). Finally, we additionally use an optional normalization layer (we\nchoose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)), motivated by RetNet\u2019s usage of a normalization layer in a\nsimilar location (Y. Sun et al. 2023).\n3.5 Properties of Selection Mechanisms\nThe selection mechanism is a broader concept that can be applied in di\ufb00erent ways, such as to more traditional\nRNNs or CNNs, to di\ufb00erent parameters (e.g. Ain Algorithm 2), or using di\ufb00erent transformations /u1D460.x).\n3.5.1 Connection to Gating Mechanisms\nWe highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection\nmechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time\nsystems is well established (Funahashi and Nakamura 1993; Tallec and Ollivier 2018). In fact, Theorem 1is\nan improvement of Gu, Johnson, Goel, et al. ( 2021, Lemma 3.1) generalizing to the ZOH discretization and\ninput-dependent gates (proof in Appendix C). More broadly, \u2206in SSMs can be seen to play a generalized role\nof the RNN gating mechanism. In line with prior work, we adopt the view that discretization of SSMs is the\nprincipled foundation of heuristic gating mechanisms.\nTheorem 1. When /u1D441= 1,A= /uni22121 ,B= 1, /u1D460\u2206=/u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB .x), and /u1D70F\u2206=/u1D5CCo/u1D5BF/u1D5CDpl/u1D5CE/u1D5CC , then the selective SSM recurrence\n(Algorithm 2) takes the form\ng/u1D461=/u1D70E./u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB .x/u1D461))\n/uni210E/u1D461= .1 /uni2212 g/u1D461)/uni210E/u1D461/uni22121+g/u1D461x/u1D461.(5)\nAs mentioned in Section 3.2, our speci\ufb01c choices of /u1D460\u2206, /u1D70F\u2206is from this connection. In particular, note that if a\ngiven input x/u1D461should be completely ignored (as necessary in the synthetic tasks), all /u1D437channels should ignore it,\nand so we project the input down to 1dimension before repeating/broadcasting with \u2206.\n8", "start_char_idx": 0, "end_char_idx": 3190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1cc903c2-a6b6-4266-a143-d3f8c3a751ea": {"__data__": {"id_": "1cc903c2-a6b6-4266-a143-d3f8c3a751ea", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "caa2a543-17b6-4a48-93b2-eb1d82a1bb15", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "2277c6f20a18636f6d4df6b23906911bb44f7b8ab44b2f02c7adbff5aaa955db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9aa89415-1382-4713-8b0e-af4e38d08b7f", "node_type": "1", "metadata": {}, "hash": "d19760a32295e73f05be3b0aefb06892ae4e37efb8db9c7faab8477ebfdce314", "class_name": "RelatedNodeInfo"}}, "text": "3.5.2 Interpretation of Selection Mechanisms\nWe elaborate on two particular mechanistic e\ufb00ects of selection.\nVariable Spacing. Selectivity allows \ufb01ltering out irrelevant noise tokens that may occur between inputs of\ninterest. This is exempli\ufb01ed by the Selective Copying task, but occurs ubiquitously in common data modalities,\nparticularly for discrete data \u2013 for example the presence of language \ufb01llers such as \u201cum\u201d. This property arises\nbecause the model can mechanistically \ufb01lter out any particular input x/u1D461, for example in the gated RNN case\n(Theorem 1) when g/u1D461/uni21920.\nFiltering Context. It has been empirically observed that many sequence models do not improve with longer\ncontext (F. Shi et al. 2023), despite the principle that more context should lead to strictly better performance. An\nexplanation is that many sequence models cannot e\ufb00ectively ignore irrelevant context when necessary; an intuitive\nexample are global convolutions (and general LTI models). On the other hand, selective models can simply reset\ntheir state at any time to remove extraneous history, and thus their performance in principle improves monotonicly\nwith context length (e.g. Section 4.3.2).\nBoundary Resetting. In settings where multiple independent sequences are stitched together, Transformers\ncan keep them separate by instantiating a particular attention mask, while LTI models will bleed information\nbetween the sequences. Selective SSMs can also reset their state at boundaries (e.g. \u2206/u1D461/uni2192/uni221Eor Theorem 1when\ng/u1D461/uni21921). These settings may occur arti\ufb01cially (e.g. packing documents together to improve hardware utilization)\nor naturally (e.g. episode boundaries in reinforcement learning (Lu et al. 2023)).\nAdditionally, we elaborate on e\ufb00ects of each selective parameter.\nInterpretation of \u2206.In general, \u2206controls the balance between how much to focus or ignore the current input\nx/u1D461. It generalizes RNN gates (e.g. g/u1D461in Theorem 1), mechanically, a large \u2206resets the state /uni210Eand focuses on the\ncurrent input x, while a small \u2206persists the state and ignores the current input. SSMs (1)-(2)can be interpreted as\na continuous system discretized by a timestep \u2206, and in this context the intuition is that large \u2206/uni2192/uni221Erepresents\nthe system focusing on the current input for longer (thus \u201cselecting\u201d it and forgetting its current state) while a\nsmall \u2206/uni21920represents a transient input that is ignored.\nInterpretation of A.We remark that while the Aparameter could also be selective, it ultimately a\ufb00ects the\nmodel only through its interaction with \u2206viaA= exp.\u2206 A)(the discretization (4)). Thus selectivity in \u2206is\nenough to ensure selectivity in .A,B), and is the main source of improvement. We hypothesize that making A\nselective in addition to (or instead of) \u2206would have similar performance, and leave it out for simplicity.\nInterpretation of BandC.As discussed in Section 3.1, the most important property of selectivity is \ufb01ltering\nout irrelevant information so that a sequence model\u2019s context can be compressed into an e\ufb03cient state. In an SSM,\nmodifying BandCto be selective allows \ufb01ner-grained control over whether to let an input x/u1D461into the state /uni210E/u1D461or\nthe state into the output /u1D466/u1D461. These can be interpreted as allowing the model to modulate the recurrent dynamics\nbased on content (input) and context (hidden states) respectively.\n3.6 Additional Model Details\nReal vs. Complex. Most prior SSMs use complex numbers in their state /uni210E, which is necessary for strong\nperformance on many tasks (Gu, Goel, and R\u00e9 2022). However, it has been empirically observed that completely\nreal-valued SSMs seem to work \ufb01ne, and possibly even better, in some settings (Ma et al. 2023).", "start_char_idx": 0, "end_char_idx": 3770, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9aa89415-1382-4713-8b0e-af4e38d08b7f": {"__data__": {"id_": "9aa89415-1382-4713-8b0e-af4e38d08b7f", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "caa2a543-17b6-4a48-93b2-eb1d82a1bb15", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "2277c6f20a18636f6d4df6b23906911bb44f7b8ab44b2f02c7adbff5aaa955db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cc903c2-a6b6-4266-a143-d3f8c3a751ea", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "b3aaa3063f141093e61ccc4d9746f18e74db609ab841b520afea9d630a6f5a6e", "class_name": "RelatedNodeInfo"}}, "text": "In an SSM,\nmodifying BandCto be selective allows \ufb01ner-grained control over whether to let an input x/u1D461into the state /uni210E/u1D461or\nthe state into the output /u1D466/u1D461. These can be interpreted as allowing the model to modulate the recurrent dynamics\nbased on content (input) and context (hidden states) respectively.\n3.6 Additional Model Details\nReal vs. Complex. Most prior SSMs use complex numbers in their state /uni210E, which is necessary for strong\nperformance on many tasks (Gu, Goel, and R\u00e9 2022). However, it has been empirically observed that completely\nreal-valued SSMs seem to work \ufb01ne, and possibly even better, in some settings (Ma et al. 2023). We use real\nvalues as the default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeo\ufb00 is\nrelated to the continuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous\nmodalities (e.g. audio, video) but not discrete (e.g. text, DNA).\n9", "start_char_idx": 3097, "end_char_idx": 4080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "35ee119c-8def-4e3d-aaea-ff360494777d": {"__data__": {"id_": "35ee119c-8def-4e3d-aaea-ff360494777d", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f10934d8-1bea-4c8f-80a5-3e7e6f7a6f2b", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "d7a623d94258e78cb3a62b9bcaeea4426ee97de9d6a7a0f4b3a1adc565282c37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26c45932-c2b4-455e-99d7-5019546a6a89", "node_type": "1", "metadata": {}, "hash": "b84472e08ea23f7ce7dc9712434f8566893f235e2ccc0b364cef745371b3780d", "class_name": "RelatedNodeInfo"}}, "text": "Initialization. Most prior SSMs also suggest special initializations, particularly in the complex-valued case,\nwhich can help in several settings such as low-data regimes. Our default initialization for the complex case is\nS4D-Lin and for the real case is S4D-Real (Gu, Gupta, et al. 2022), which is based on the HIPPO theory (Gu,\nDao, et al. 2020). These de\ufb01ne the /u1D45B-th element of Aas/uni22121/uni22152 + /u1D45B/u1D456and/uni2212./u1D45B+ 1) respectively. However, we expect\nmany initializations to work \ufb01ne, particularly in the large-data and real-valued SSM regimes; some ablations are\nconsidered in Section 4.6.\nParameterization of \u2206.We de\ufb01ned the selective adjustment to \u2206as/u1D460\u2206.x) =/u1D5A1/u1D5CBo/u1D5BA/u1D5BD/u1D5BC/u1D5BA/u1D5CC/u1D5CD /u1D437./u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB 1.x)), which was\nmotivated by the mechanics of \u2206(Section 3.5). We observe that it can be generalized from dimension 1to a larger\ndimension /u1D681. We set this to be a small fraction of /u1D673, which uses a negligible number of parameters compared to\nthe main Linear projections in the block. We additionally note that the broadcasting operation can instead be\nviewed as another Linear projection, initialized to a speci\ufb01c pattern of 1\u2019s and 0\u2019s; if this projection is trainable,\nthis leads to the alternative /u1D460\u2206.x) =/u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB /u1D437./u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB /u1D445.x)), which can be viewed as a low-rank projection.\nIn our experiments, the \u2206parameter (which can be viewed as a bias term) is initialized to /u1D70F/uni22121\n\u2206./u1D5B4/u1D5C7/u1D5C2/u1D5BFo/u1D5CB/u1D5C6 .[0.001,0.1])),\nfollowing prior work on SSMs (Gu, Johnson, Timalsina, et al. 2023).\nRemark 3.1. For brevity in our experimental results, we sometimes abbreviate selective SSMs as S6 models , because they\nare S4 models with a selection mechanism and computed with a scan.\n4 Empirical Evaluation\nIn Section 4.1we test Mamba\u2019s ability to solve the two synthetic tasks motivated in Section 3.1. We then evaluate\non three domains, each evaluated on autoregressive pretraining as well as downstream tasks.\n\u2022Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation.\n\u2022Section 4.3: DNA sequence pretraining, and \ufb01ne-tuning on a long-sequence classi\ufb01cation task.\n\u2022Section 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips.\nFinally, Section 4.5shows Mamba\u2019s computational e\ufb03ciency at both training and inference time, and Section 4.6\nablates various components of the architecture and selective SSMs.\n4.1 Synthetic Tasks\nFull experiment details for these tasks including task details and training protocol are in Appendix E.1.\n4.1.1 Selective Copying\nThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test\nthe memorization abilities of recurrent models. As discussed in Section 3.1, LTI SSMs (linear recurrences and\nglobal convolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for\nexample, by constructing a convolution kernel of exactly the right length (Figure 2). This was explicitly validated\nin earlier work on global convolutions (Romero et al. 2021). The Selective Copying task prevents this shortcut\nby randomizing the spacing between tokens.", "start_char_idx": 0, "end_char_idx": 3351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "26c45932-c2b4-455e-99d7-5019546a6a89": {"__data__": {"id_": "26c45932-c2b4-455e-99d7-5019546a6a89", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f10934d8-1bea-4c8f-80a5-3e7e6f7a6f2b", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "d7a623d94258e78cb3a62b9bcaeea4426ee97de9d6a7a0f4b3a1adc565282c37", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35ee119c-8def-4e3d-aaea-ff360494777d", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "d675b1db7d161584cbd760c3db4dba435f7cbd2d66b77fb48a2632f41fbfbfb4", "class_name": "RelatedNodeInfo"}}, "text": "4.1 Synthetic Tasks\nFull experiment details for these tasks including task details and training protocol are in Appendix E.1.\n4.1.1 Selective Copying\nThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test\nthe memorization abilities of recurrent models. As discussed in Section 3.1, LTI SSMs (linear recurrences and\nglobal convolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for\nexample, by constructing a convolution kernel of exactly the right length (Figure 2). This was explicitly validated\nin earlier work on global convolutions (Romero et al. 2021). The Selective Copying task prevents this shortcut\nby randomizing the spacing between tokens. Note that this task has been introduced before as the Denoising\ntask (Jing et al. 2019).\nNote that many previous works argue that adding architecture gating (multiplicative interactions) can endow\nmodels with \u201cdata-dependence\u201d and solve related tasks (Dao, Fu, Saab, et al. 2023; Poli et al. 2023). However,\nwe \ufb01nd this explanation insu\ufb03cient intuitively because such gating does not interact along the sequence axis,\nand cannot a\ufb00ect the spacing between tokens. In particular architecture gating is not an instance of a selection\nmechanism (Appendix A).\nTable 1con\ufb01rms that gated architectures such as H3 and Mamba only partially improve performance, while the\nselection mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more\npowerful architectures.\n10", "start_char_idx": 2587, "end_char_idx": 4149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6a3dc7ba-a926-468f-9ce8-0b6f010cc015": {"__data__": {"id_": "6a3dc7ba-a926-468f-9ce8-0b6f010cc015", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b01a0f79-ddda-4810-baeb-5d5498b98ef1", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "d46931e565382d81cb7ed6ac6786ce948bcde2aa37eb6b636f0bf9c053018e59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96855d91-a66d-4e78-82d8-a3dc61909875", "node_type": "1", "metadata": {}, "hash": "391fb3ff4e88e48cc50e6b0fc1529740dc385660f881e5aa109dbbeba6046948", "class_name": "RelatedNodeInfo"}}, "text": "Model Arch. Layer Acc.\nS4 No gate S4 18.3\n- No gate S6 97.0\nH3 H3 S4 57.0\nHyena H3 Hyena 30.1\n- H3 S6 99.7\n- Mamba S4 56.4\n- Mamba Hyena 28.4\nMamba Mamba S6 99.8\nTable 1: ( Selective Copying .)\nAccuracy for combinations of architectures\nand inner sequence layers.\n/uni00000015/uni00000014/uni00000016/uni00000015/uni00000014/uni00000017/uni00000015/uni00000014/uni00000018/uni00000015/uni00000014/uni00000019/uni00000015/uni00000014/uni0000001a\n/uni00000038/uni00000049/uni00000057/uni00000058/uni00000004/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000014/uni00000012/uni00000014/uni00000014/uni00000012/uni00000016/uni00000014/uni00000012/uni00000018/uni00000014/uni00000012/uni0000001a/uni00000014/uni00000012/uni0000001c/uni00000015/uni00000012/uni00000014/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d\n/uni0000002d/uni00000052/uni00000048/uni00000059/uni00000047/uni00000058/uni0000004d/uni00000053/uni00000052/uni00000004/uni0000002c/uni00000049/uni00000045/uni00000048/uni00000057/uni00000004/uni00000029/uni0000005c/uni00000058/uni00000056/uni00000045/uni00000054/uni00000053/uni00000050/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052\n/uni00000031/uni0000002c/uni00000025/uni00000011/uni00000025/uni00000046/uni00000057/uni00000053/uni00000050/uni00000059/uni00000058/uni00000049\n/uni00000031/uni0000002c/uni00000025/uni00000011/uni00000036/uni00000053/uni00000034/uni00000029\n/uni00000031/uni0000002c/uni00000025/uni00000011/uni0000005c/uni00000034/uni00000053/uni00000057\n/uni0000002c/uni00000017\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045\n/uni00000036/uni00000045/uni00000052/uni00000048/uni00000053/uni00000051\n/uni00000038/uni00000056/uni00000045/uni0000004d/uni00000052/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004cTable 2: ( Induction Heads .) Models are trained on sequence length\n28= 256 , and tested on increasing sequence lengths of 26= 64 up to\n220= 1048576 . Full numbers in Table 11.\n4.1.2 Induction Heads\nInduction heads (Olsson et al. 2022) is a simple task from the mechanistic interpretability lens (Elhage et al. 2021)\nthat is surprisingly predictive of the in-context learning ability of LLMs.", "start_char_idx": 0, "end_char_idx": 2442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "96855d91-a66d-4e78-82d8-a3dc61909875": {"__data__": {"id_": "96855d91-a66d-4e78-82d8-a3dc61909875", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b01a0f79-ddda-4810-baeb-5d5498b98ef1", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "d46931e565382d81cb7ed6ac6786ce948bcde2aa37eb6b636f0bf9c053018e59", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a3dc7ba-a926-468f-9ce8-0b6f010cc015", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "0d0695b83408a748269695639f90af0b5f4c1c269c230d7adb25e4900a43818d", "class_name": "RelatedNodeInfo"}}, "text": "Models are trained on sequence length\n28= 256 , and tested on increasing sequence lengths of 26= 64 up to\n220= 1048576 . Full numbers in Table 11.\n4.1.2 Induction Heads\nInduction heads (Olsson et al. 2022) is a simple task from the mechanistic interpretability lens (Elhage et al. 2021)\nthat is surprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative\nrecall and copy: for example, if the model has seen a bigram such as \u201cHarry Potter\u201d in the sequence, then the\nnext time \u201cHarry\u201d appears in the same sequence, the model should be able to predict \u201cPotter\u201d by copying from\nhistory.\nDataset. We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of\n16, which is comparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We\nadditionally investigate generalization and extrapolation abilities by evaluating on a range of sequence lengths\nfrom 26= 64 up to 220= 1048576 at test time.\nModels. Following established work on induction heads, we use 2 layer models, which allows attention to\nmechanistically solve the induction heads task (Olsson et al. 2022). We test both multi-head attention (8 heads,\nwith various positional encodings) and SSM variants. We use a model dimension /u1D437of64for Mamba and 128for\nthe other models.\nResults. Table 2shows that Mamba\u2014or more precisely, its selective SSM layer\u2014has the ability to solve the\ntask perfectly because of its ability to selectively remember the relevant token while ignoring everything else in\nbetween. It generalizes perfectly to million-length sequences, or 4000\u00d7 longer than it saw during training, while no\nother method goes beyond 2\u00d7.\nOut of positional encoding variants for attention models, xPos (which was designed for length extrapolation)\nis slightly better than the others; also note that all attention models were only tested up to sequence length\n214= 16384 due to memory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the \ufb01ndings in\nPoli et al. ( 2023).\n4.2 Language Modeling\nWe evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on\nboth pretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to\nmirror GPT3 speci\ufb01cations. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training\nrecipe described in Brown et al. ( 2020). All training details are in Appendix E.2.\n4.2.1 Scaling Laws\nFor baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the\nstrongest Transformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa\n11", "start_char_idx": 2080, "end_char_idx": 4819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3a5d19e0-c3b6-42d3-83c2-24cba268750a": {"__data__": {"id_": "3a5d19e0-c3b6-42d3-83c2-24cba268750a", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63daab92-9f2e-424b-bb0e-be5c3074d359", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c8685d022a7f4b384b5ef634fd2ff272b6e2ea723ed5d3b4db45b4ba59292e6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc8e156f-7bb6-415c-b400-33472b7d1cd1", "node_type": "1", "metadata": {}, "hash": "4b92312a5d80fe16050717fcfa94fb6f52f0ade402fbea319fbaf7281d6ffa76", "class_name": "RelatedNodeInfo"}}, "text": "/uni00000015/uni00000014/uni00000015/uni0000001d/uni00000015/uni00000014/uni00000016/uni00000014\n/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000057/uni00000004/uni0000000c/uni00000050/uni00000053/uni0000004b/uni00000004/uni00000057/uni00000047/uni00000045/uni00000050/uni00000049/uni0000000d/uni00000015/uni00000014/uni00000015\n/uni0000001a/uni00000082/uni00000015/uni00000014/uni00000014/uni00000016/uni00000082/uni00000015/uni00000014/uni00000015/uni00000034/uni00000049/uni00000056/uni00000054/uni00000050/uni00000049/uni0000005c/uni0000004d/uni00000058/uni0000005d/uni00000004/uni0000000c/uni00000050/uni00000053/uni0000004b/uni00000004/uni00000057/uni00000047/uni00000045/uni00000050/uni00000049/uni0000000d\n/uni00000037/uni00000047/uni00000045/uni00000050/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000053/uni00000052/uni00000004/uni00000038/uni0000004c/uni00000049/uni00000004/uni00000034/uni0000004d/uni00000050/uni00000", "start_char_idx": 0, "end_char_idx": 1008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fc8e156f-7bb6-415c-b400-33472b7d1cd1": {"__data__": {"id_": "fc8e156f-7bb6-415c-b400-33472b7d1cd1", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63daab92-9f2e-424b-bb0e-be5c3074d359", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c8685d022a7f4b384b5ef634fd2ff272b6e2ea723ed5d3b4db45b4ba59292e6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a5d19e0-c3b6-42d3-83c2-24cba268750a", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "f7f049b1dc3d2770363ede014c6b9612f7f22e4d46c007dc4139f9139e3fb203", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c88fdb2c-df10-42d3-b887-a7addb382bdd", "node_type": "1", "metadata": {}, "hash": "1d18f3be9bf319ceb111aa12d9f2c65ad2907dec4275b8c463d35008d635ce85", "class_name": "RelatedNodeInfo"}}, "text": "uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000053/uni00000052/uni00000004/uni00000038/uni0000004c/uni00000049/uni00000004/uni00000034/uni0000004d/uni00000050/uni00000049/uni00000004/uni0000000c/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000004/uni00000016/uni00000014/uni00000018/uni0000001c/uni0000000d\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045\n/uni00000036/uni0000003b/uni0000002f/uni0000003a\n/uni00000038/uni00000056/uni00000045/uni00000052/uni00000057/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000049/uni00000056\n/uni00000036/uni00000049/uni00000058/uni00000032/uni00000049/uni00000058\n/uni0000002c/uni00000017/uni0000000f/uni0000000f\n/uni00000038/uni00000056/uni00000045/uni00000052/uni00000057/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000049/uni00000056/uni0000000f/uni0000000f\n/uni00", "start_char_idx": 808, "end_char_idx": 1816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c88fdb2c-df10-42d3-b887-a7addb382bdd": {"__data__": {"id_": "c88fdb2c-df10-42d3-b887-a7addb382bdd", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63daab92-9f2e-424b-bb0e-be5c3074d359", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c8685d022a7f4b384b5ef634fd2ff272b6e2ea723ed5d3b4db45b4ba59292e6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc8e156f-7bb6-415c-b400-33472b7d1cd1", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "b78ca4fb5b810b59dfa0a0f1ffb560f88a617b3228bf9b5b176830215789dc16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07ea05e1-3399-4103-af08-f3766b6e774e", "node_type": "1", "metadata": {}, "hash": "e8cb7e30fb1d0eddd458576b531731e71093aa34b3037ea32ceb8947b6cf5511", "class_name": "RelatedNodeInfo"}}, "text": "/uni00000017/uni0000000f/uni0000000f\n/uni00000038/uni00000056/uni00000045/uni00000052/uni00000057/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000049/uni00000056/uni0000000f/uni0000000f\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045\n/uni00000015/uni00000014/uni00000015/uni0000001d/uni00000015/uni00000014/uni00000016/uni00000014\n/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000057/uni00000004/uni0000000c/uni00000050/uni00000053/uni0000004b/uni00000004/uni00000057/uni00000047/uni00000045/uni00000050/uni00000049/uni0000000d/uni00000015/uni00000014/uni00000015\n/uni0000001a/uni00000082/uni00000015/uni00000014/uni00000014/uni00000016/uni00000082/uni00000015/uni00000014/uni00000015/uni00000034/uni00000049/uni00000056/uni00000054/uni00000050/uni00000049/uni0000005c/uni0000004d/uni00000058/uni0000005d/uni00000004/uni0000000c/uni00000050/uni00000053/uni0000004b/uni00000004/uni00000057/uni00000047/uni00000045/uni00000050/uni00000049/uni0000000d\n/uni00000037/uni00000047/uni00", "start_char_idx": 1616, "end_char_idx": 2624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "07ea05e1-3399-4103-af08-f3766b6e774e": {"__data__": {"id_": "07ea05e1-3399-4103-af08-f3766b6e774e", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63daab92-9f2e-424b-bb0e-be5c3074d359", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c8685d022a7f4b384b5ef634fd2ff272b6e2ea723ed5d3b4db45b4ba59292e6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c88fdb2c-df10-42d3-b887-a7addb382bdd", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "00964035430237ff5176a35466b40db6f79d79e4de51598cf114ed6a13d159e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a609265-cf78-4480-a2b4-724b41e99d68", "node_type": "1", "metadata": {}, "hash": "8a7e4b4b3f84961334f30e46cbecb2bd383e7235ce17d691a08bf2f439ffb90d", "class_name": "RelatedNodeInfo"}}, "text": "d/uni00000058/uni0000005d/uni00000004/uni0000000c/uni00000050/uni00000053/uni0000004b/uni00000004/uni00000057/uni00000047/uni00000045/uni00000050/uni00000049/uni0000000d\n/uni00000037/uni00000047/uni00000045/uni00000050/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000053/uni00000052/uni00000004/uni00000038/uni0000004c/uni00000049/uni00000004/uni00000034/uni0000004d/uni00000050/uni00000049/uni00000004/uni0000000c/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000004/uni0000001c/uni00000015/uni0000001d/uni00000016/uni0000000d\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045\n/uni00000036/uni0000003b/uni0000002f/uni0000003a\n/uni00000038/uni00000056/uni00000045/uni00000052/uni00000057/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000049/uni00000056\n/uni00000036/uni00000049/uni00", "start_char_idx": 2424, "end_char_idx": 3432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5a609265-cf78-4480-a2b4-724b41e99d68": {"__data__": {"id_": "5a609265-cf78-4480-a2b4-724b41e99d68", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63daab92-9f2e-424b-bb0e-be5c3074d359", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c8685d022a7f4b384b5ef634fd2ff272b6e2ea723ed5d3b4db45b4ba59292e6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07ea05e1-3399-4103-af08-f3766b6e774e", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "bd2b6ae6c69d2a9c5b970480831847fdbe12f4a0d1e504d8eceb7f0cc1d0f573", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e009135f-ba6c-46b3-9799-57b6d4dc80a3", "node_type": "1", "metadata": {}, "hash": "5d4ca1a167970f1a875d39f6598e20ee25c19cd1c5285b2d62398a04adb0b798", "class_name": "RelatedNodeInfo"}}, "text": "/uni0000003b/uni0000002f/uni0000003a\n/uni00000038/uni00000056/uni00000045/uni00000052/uni00000057/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000049/uni00000056\n/uni00000036/uni00000049/uni00000058/uni00000032/uni00000049/uni00000058\n/uni0000002c/uni00000017/uni0000000f/uni0000000f\n/uni00000038/uni00000056/uni00000045/uni00000052/uni00000057/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000049/uni00000056/uni0000000f/uni0000000f\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045Figure 4: ( Scaling Laws .) Models of size /uni2248 125 /u1D440to/uni2248 1.3/u1D435parameters, trained on the Pile. Mamba scales better than all other\nattention-free models and is the /f_irst to match the performance of a very strong \u201cTransformer++\u201d recipe that has now become\nstandard, particularly as the sequence length grows.\narchitectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher\nlearning rates). We also compare against other recent subquadratic architectures (Figure 4). All model details are\nin Appendix E.2.\nFigure 4shows scaling laws under the standard Chinchilla (Ho\ufb00mann et al. 2022) protocol, on models from\n/uni2248 125 /u1D440to/uni2248 1.3/u1D435parameters. Mamba is the \ufb01rst attention-free model to match the performance of a very\nstrong Transformer recipe (Transformer++) that has now become standard, particularly as the sequence length\ngrows. We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior\nstrong recurrent models that can also be interpreted as SSMs, due to a lack of e\ufb03cient implementation leading to\nout-of-memory or unrealistic computation requirements.\n4.2.2 Downstream Evaluations\nTable 3shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We\ncompare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al.\n2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length\n(300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV\nwas trained with context length 1024.)\n4.3 DNA Modeling\nMotivated by the success of large language models, there has been recent exploration into using the foundation\nmodel paradigm for genomics.", "start_char_idx": 3232, "end_char_idx": 5579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e009135f-ba6c-46b3-9799-57b6d4dc80a3": {"__data__": {"id_": "e009135f-ba6c-46b3-9799-57b6d4dc80a3", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63daab92-9f2e-424b-bb0e-be5c3074d359", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c8685d022a7f4b384b5ef634fd2ff272b6e2ea723ed5d3b4db45b4ba59292e6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a609265-cf78-4480-a2b4-724b41e99d68", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "042bbe4a283499414d6d0e8f904ef5eea94d315cb53bf11115baf56f36b5fe17", "class_name": "RelatedNodeInfo"}}, "text": "4.2.2 Downstream Evaluations\nTable 3shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We\ncompare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al.\n2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length\n(300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV\nwas trained with context length 1024.)\n4.3 DNA Modeling\nMotivated by the success of large language models, there has been recent exploration into using the foundation\nmodel paradigm for genomics. DNA has been likened to language in that it consists of sequences of discrete\ntokens with a \ufb01nite vocab. It is also known for requiring long-range dependencies to model (Avsec et al. 2021).\nWe investigate Mamba as a FM backbone for pretraining and \ufb01ne-tuning in the same setting as recent works on\nlong-sequence models for DNA (Nguyen, Poli, et al. 2023). In particular, we focus on two explorations of scaling\nlaws across model size and sequence length (Figure 5), and a di\ufb03cult downstream synthetic classi\ufb01cation task\nrequiring long context (Figure 6).\nFor pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training\nand model details (see also Appendix E.2). For the dataset, we largely follow the setup of HyenaDNA (Nguyen,\nPoli, et al. 2023), which uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5\nbillion tokens (DNA base pairs) in the training split.\n4.3.1 Scaling: Model Size\nIn this experiment, we investigate the scaling properties of genomics foundation models with various model\nbackbones (Figure 5Left).\nTraining. To advantage the baselines, we train on a short sequence length of 1024; as shown in Section 4.3.2, we\nexpect results to favor Mamba even more at longer sequence lengths. We \ufb01x a global batch size of 1024, for a\n12", "start_char_idx": 4925, "end_char_idx": 6915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8b7319bb-c3a5-47fe-914f-70fe113b4128": {"__data__": {"id_": "8b7319bb-c3a5-47fe-914f-70fe113b4128", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61d76b70-8bd2-44b0-8c12-5b7be023555b", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "b529918567623f402e91216144a09bfe7b72adeb49ee4e8d71c21211d4e10d68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc3d187c-a058-4e3b-8230-b6bf738e164a", "node_type": "1", "metadata": {}, "hash": "40c044d1fb6968ae06f52331530c4d44f71088cccd370caf3f615ce34d3fe518", "class_name": "RelatedNodeInfo"}}, "text": "Table 3: ( Zero-shot Evaluations .) Best results for each size in bold. We compare against open source LMs with various tokenizers,\ntrained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and\ntokenizer (GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches\nbaselines at twice the model size.\nModel Token. Pile LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande Average\nppl/uni2193 ppl/uni2193 acc/uni2191 acc/uni2191 acc/uni2191 acc/uni2191 acc/uni2191 acc/uni2191 acc/uni2191\nHybrid H3-130M GPT2 \u2014 89.48 25.77 31.7 64.2 44.4 24.2 50.6 40.1\nPythia-160M NeoX 29.64 38.10 33.0 30.2 61.4 43.2 24.1 51.9 40.6\nMamba-130M NeoX 10.56 16.07 44.3 35.3 64.5 48.0 24.3 51.9 44.7\nHybrid H3-360M GPT2 \u2014 12.58 48.0 41.5 68.1 51.4 24.7 54.1 48.0\nPythia-410M NeoX 9.95 10.84 51.4 40.6 66.9 52.1 24.6 53.8 48.2\nMamba-370M NeoX 8.28 8.14 55.6 46.5 69.5 55.1 28.0 55.3 50.0\nPythia-1B NeoX 7.82 7.92 56.1 47.2 70.7 57.0 27.1 53.5 51.9\nMamba-790M NeoX 7.33 6.02 62.7 55.1 72.1 61.2 29.5 56.1 57.1\nGPT-Neo 1.3B GPT2 \u2014 7.50 57.2 48.9 71.1 56.2 25.9 54.9 52.4\nHybrid H3-1.3B GPT2 \u2014 11.25 49.6 52.6 71.3 59.2 28.1 56.9 53.0\nOPT-1.3B OPT \u2014 6.64 58.0 53.7 72.4 56.7 29.6 59.5 55.0\nPythia-1.4B NeoX 7.51 6.08 61.7 52.1 71.0 60.5 28.5 57.2 55.2\nRWKV-1.5B NeoX 7.70 7.04 56.4 52.5 72.4 60.5 29.4 54.6 54.3\nMamba-1.4B NeoX 6.80 5.04 64.9 59.1 74.2 65.5 32.8 61.5 59.7\nGPT-Neo 2.7B GPT2 \u2014 5.63 62.2 55.8 72.1 61.1 30.2 57.6 56.5\nHybrid H3-2.7B GPT2 \u2014 7.92 55.7 59.7 73.3 65.6 32.3 61.4 58.0\nOPT-2.7B OPT \u2014 5.12 63.6 60.6 74.8 60.8 31.3 61.0 58.7\nPythia-2.8B NeoX 6.73 5.04 64.7 59.3 74.0 64.1 32.9 59.7 59.1\nRWKV-3B NeoX 7.00 5.24 63.9 59.", "start_char_idx": 0, "end_char_idx": 1742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bc3d187c-a058-4e3b-8230-b6bf738e164a": {"__data__": {"id_": "bc3d187c-a058-4e3b-8230-b6bf738e164a", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61d76b70-8bd2-44b0-8c12-5b7be023555b", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "b529918567623f402e91216144a09bfe7b72adeb49ee4e8d71c21211d4e10d68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b7319bb-c3a5-47fe-914f-70fe113b4128", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "81535ed9cb0e26b7ea5793d0825ac05094eec73f59b42fb79a4460b3891d9b1d", "class_name": "RelatedNodeInfo"}}, "text": "7B GPT2 \u2014 5.63 62.2 55.8 72.1 61.1 30.2 57.6 56.5\nHybrid H3-2.7B GPT2 \u2014 7.92 55.7 59.7 73.3 65.6 32.3 61.4 58.0\nOPT-2.7B OPT \u2014 5.12 63.6 60.6 74.8 60.8 31.3 61.0 58.7\nPythia-2.8B NeoX 6.73 5.04 64.7 59.3 74.0 64.1 32.9 59.7 59.1\nRWKV-3B NeoX 7.00 5.24 63.9 59.6 73.7 67.8 33.1 59.6 59.6\nMamba-2.8B NeoX 6.22 4.23 69.2 66.1 75.2 69.7 36.3 63.5 63.3\nGPT-J-6B GPT2 \u2013 4.10 68.3 66.3 75.4 67.0 36.6 64.1 63.0\nOPT-6.7B OPT \u2013 4.25 67.7 67.2 76.3 65.6 34.9 65.5 62.9\nPythia-6.9B NeoX 6.51 4.45 67.1 64.0 75.2 67.3 35.5 61.3 61.7\nRWKV-7.4B NeoX 6.31 4.38 67.2 65.5 76.1 67.8 37.5 61.0 62.5\ntotal of 220/uni2248 1/u1D440tokens per batch. Models were trained for 10/u1D43Egradient steps for a total of 10/u1D435tokens.\nResults. Figure 5(Left) shows that Mamba\u2019s pretraining perplexity improves smoothly with model size, and\nthat Mamba scales better than both HyenaDNA and Transformer++. For example, at the largest model size of\n/uni2248 40/u1D440parameters, the curve shows that Mamba can match the Transformer++ and HyenaDNA models with\nroughly 3\u00d7to4\u00d7fewer parameters.\n4.3.2 Scaling: Context Length\nIn the next DNA experiment, we investigate the scaling properties of models with respect to sequence length.\nWe only compare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at\nlonger sequence lengths. We pretrain models on sequence lengths 210= 1024 ,212= 4096 ,214= 16384 ,216= 65536 ,\n218= 262144 ,220= 1048576 . We \ufb01x a model size of 6 layers by width 128(about 1.3M-1.4M parameters). Models\nwere trained for 20/u1D43Egradient steps for a total of /uni2248 330 /u1D435tokens. The longer sequence lengths used sequence length\nwarmup similar to (Nguyen, Poli, et al. 2023).\nResults. Figure 5(Right) shows that Mamba is able to make use of longer context even up to extremely long\nsequences of length 1M, and its pretraining perplexity improves as the context increases. On the other hand,\nthe HyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5on\nproperties of the selection mechanism. In particular, LTI models cannot selectively ignore information; from a\nconvolutional perspective, a very long convolution kernel is aggregating all information across a long sequence\n13", "start_char_idx": 1482, "end_char_idx": 3732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad": {"__data__": {"id_": "e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "92a9c70d77ff9b2ec29be483a2a640d33ee625c8cf8d3685f14709cfccf3c0f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f9774e7-a059-48a5-a2c7-e94681a8adfd", "node_type": "1", "metadata": {}, "hash": "bdf96bf5bf219e7ad461a729c41b700549cbfd5343ddccdcaf5f15510276b183", "class_name": "RelatedNodeInfo"}}, "text": "/uni00000015/uni00000014/uni0000001a/uni00000015/uni00000014/uni0000001b\n/uni00000034/uni00000045/uni00000056/uni00000045/uni00000051/uni00000049/uni00000058/uni00000049/uni00000056/uni00000057/uni00000004/uni0000000c/uni00000050/uni00000053/uni0000004b/uni00000004/uni00000057/uni00000047/uni00000045/uni00000050/uni00000049/uni0000000d/uni00000016/uni00000012/uni0000001b/uni00000016/uni00000012/uni0000001c/uni00000016/uni00000012/uni0000001d/uni00000017/uni00000012/uni00000014/uni00000017/uni00000012/uni00000015/uni00000034/uni00000049/uni00000056/uni00000054/uni00000050/uni00000049/uni0000005c/uni0000004d/uni00000058/uni0000005d\n/uni00000037/uni00000047/uni00000045/uni00000050/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000053/uni00000052/uni00000004/uni00000058/uni0000004c/uni00000049/uni00000004/uni0000002c/uni00000059/uni00000051/uni00000045/uni00000052/uni00000004/uni0000002b/uni00000049/uni00000052/uni00000053/uni000000", "start_char_idx": 0, "end_char_idx": 1008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8f9774e7-a059-48a5-a2c7-e94681a8adfd": {"__data__": {"id_": "8f9774e7-a059-48a5-a2c7-e94681a8adfd", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "92a9c70d77ff9b2ec29be483a2a640d33ee625c8cf8d3685f14709cfccf3c0f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "bf74885f4acfd091c0a77fade4f4e5adf706ec48b994e22096b742c8e2ea59cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08f70598-5610-425e-abd1-a97ebd201ac6", "node_type": "1", "metadata": {}, "hash": "b2aabfecac449fa97e4c8304eba4693560f329b945bc15cb5368a64abc500f1e", "class_name": "RelatedNodeInfo"}}, "text": "ni00000052/uni00000004/uni00000058/uni0000004c/uni00000049/uni00000004/uni0000002c/uni00000059/uni00000051/uni00000045/uni00000052/uni00000004/uni0000002b/uni00000049/uni00000052/uni00000053/uni00000051/uni00000049/uni00000004/uni0000000c/uni0000002c/uni0000002b/uni00000017/uni0000001c/uni0000000d\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045/uni00000028/uni00000032/uni00000025\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045\n/uni00000038/uni00000056/uni00000045/uni00000052/uni00000057/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000049/uni00000056/uni0000000f/uni0000000f\n/uni00000015/uni00000014/uni00000017/uni00000015/uni00000014/uni00000018/uni00000015/uni00000014/uni00000019/uni00000015/uni00000014/uni0000001a\n/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000016/uni00000012/uni0000001b/uni00000019/uni00000016/uni00000", "start_char_idx": 808, "end_char_idx": 1816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "08f70598-5610-425e-abd1-a97ebd201ac6": {"__data__": {"id_": "08f70598-5610-425e-abd1-a97ebd201ac6", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "92a9c70d77ff9b2ec29be483a2a640d33ee625c8cf8d3685f14709cfccf3c0f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f9774e7-a059-48a5-a2c7-e94681a8adfd", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ead5e5f6c0917bf955bb76f7472fd6eb1d7a806b8f7d4254d0771dd828f321aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f83ff51-d146-49cc-a71f-3dc88db543af", "node_type": "1", "metadata": {}, "hash": "09effb3abf6fd533a82a3ff10eef4fb6d91e71073b6241b9d7f3efab338cb71d", "class_name": "RelatedNodeInfo"}}, "text": "uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000016/uni00000012/uni0000001b/uni00000019/uni00000016/uni00000012/uni0000001c/uni00000014/uni00000016/uni00000012/uni0000001c/uni00000019/uni00000016/uni00000012/uni0000001d/uni00000014/uni00000016/uni00000012/uni0000001d/uni00000019/uni00000017/uni00000012/uni00000014/uni00000014/uni00000034/uni00000049/uni00000056/uni00000054/uni00000050/uni00000049/uni0000005c/uni0000004d/uni00000058/uni0000005d\n/uni00000037/uni00000047/uni00000045/uni00000050/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000011/uni00000004/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000004/uni0000000c/uni0000002c/uni0000002b/uni00000017/uni0000001c/uni0000000d\n/uni0000002c/uni0000005", "start_char_idx": 1616, "end_char_idx": 2624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1f83ff51-d146-49cc-a71f-3dc88db543af": {"__data__": {"id_": "1f83ff51-d146-49cc-a71f-3dc88db543af", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "92a9c70d77ff9b2ec29be483a2a640d33ee625c8cf8d3685f14709cfccf3c0f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08f70598-5610-425e-abd1-a97ebd201ac6", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "797dc54836dcbb8ebef664847c8d728e30d1853a7edae7bf9ec966ef6443e694", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dba88b25-1fbf-4bb0-9645-991de23588d8", "node_type": "1", "metadata": {}, "hash": "dd22e4301b69c2ffd104fdde5cdd7adc2dbee3041fa909726a75ce5f2f1b9827", "class_name": "RelatedNodeInfo"}}, "text": "00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000004/uni0000000c/uni0000002c/uni0000002b/uni00000017/uni0000001c/uni0000000d\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045/uni00000028/uni00000032/uni00000025/uni00000004/uni00000015/uni00000012/uni00000018/uni00000031\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000004/uni00000015/uni00000012/uni00000018/uni00000031\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000004/uni0000001b/uni00000031Figure 5: ( DNA Scaling Laws .) Pretraining on the HG38 (human genome) dataset. ( Left) Fixing short context length 210= 1024\nand increasing size from /uni2248 200 /u1D43Eto/uni2248 40/u1D440parameters, Mamba scales better than baselines. ( Right ) Fixing model size and increasing\nsequence lengths while keeping tokens/batch and total training tokens /f_ixed. Unlike baselines, the selection mechanism of Mamba\nfacilitates better performance with increasing context length.", "start_char_idx": 2424, "end_char_idx": 3449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dba88b25-1fbf-4bb0-9645-991de23588d8": {"__data__": {"id_": "dba88b25-1fbf-4bb0-9645-991de23588d8", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "92a9c70d77ff9b2ec29be483a2a640d33ee625c8cf8d3685f14709cfccf3c0f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f83ff51-d146-49cc-a71f-3dc88db543af", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "d6baf612f4a6f283edaa45c1f2af50b9fe87e2b9b1f1f14b2cc4f31bace48b94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dea454af-e062-43f9-8cdc-a456c19ec801", "node_type": "1", "metadata": {}, "hash": "94c0528ff80e9c91fd7263cc5c741d6d420c94ec2ef12a5e8b7d877cdab6aec8", "class_name": "RelatedNodeInfo"}}, "text": "ni00000051/uni00000046/uni00000045/uni00000004/uni0000001b/uni00000031Figure 5: ( DNA Scaling Laws .) Pretraining on the HG38 (human genome) dataset. ( Left) Fixing short context length 210= 1024\nand increasing size from /uni2248 200 /u1D43Eto/uni2248 40/u1D440parameters, Mamba scales better than baselines. ( Right ) Fixing model size and increasing\nsequence lengths while keeping tokens/batch and total training tokens /f_ixed. Unlike baselines, the selection mechanism of Mamba\nfacilitates better performance with increasing context length.\n/uni00000015/uni00000014/uni00000017/uni00000015/uni00000014/uni00000018/uni00000015/uni00000014/uni00000019/uni00000015/uni00000014/uni0000001a\n/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000014/uni00000012/uni00000016/uni00000014/uni00000012/uni00000017/uni00000014/uni00000012/uni00000018/uni00000014/uni00000012/uni00000019/uni00000014/uni00000012/uni0000001a/uni00000014/uni00000012/uni0000001b/uni00000014/uni00000012/uni0000001c/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d\n/uni0000002a/uni0000004d/uni00000052/uni00000049/uni00000058/uni00000059/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d/uni00000004/uni0000000c/uni00000037/uni00000054/uni00000049/uni00000047/uni0000004d/uni00000049/uni00000057/uni00000004/uni00000028/uni00000032/uni00000025/uni00000004/uni00000027/uni00000050/uni00000045/uni00000057/uni00000057/uni0000004d/uni0000004a/uni0000004d/uni00000047/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052/uni0000000d\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045/uni00000028/uni00000032/uni00000025/uni00000004/uni00000015/uni00000012/uni00000018/uni00000031\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000004/uni00000015/uni00000012/uni00000018/uni00000031\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000004/uni0000001b/uni00000031\n/uni00000036/uni00000045/uni00000052/uni00000048/uni00000053/uni00000051\nFigure 6: ( Great Apes DNA Classi/f_ication .) Accuracy after\n/f_ine-tuning on sequences of length 210= 1024 up to 220=\n1048576 using pretrained models of the same context length. Nu-\nmerical results in Table 13.", "start_char_idx": 2905, "end_char_idx": 5361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dea454af-e062-43f9-8cdc-a456c19ec801": {"__data__": {"id_": "dea454af-e062-43f9-8cdc-a456c19ec801", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "92a9c70d77ff9b2ec29be483a2a640d33ee625c8cf8d3685f14709cfccf3c0f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dba88b25-1fbf-4bb0-9645-991de23588d8", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "19977072ca44f5f1162d2f2162e04738ea8cc38c233a55b6f7f871fd01711cc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c", "node_type": "1", "metadata": {}, "hash": "f8aa2e31ad9429f06fd3a84dca0d164177666fe5b8d26a1910258f0fb2ad3c38", "class_name": "RelatedNodeInfo"}}, "text": "Accuracy after\n/f_ine-tuning on sequences of length 210= 1024 up to 220=\n1048576 using pretrained models of the same context length. Nu-\nmerical results in Table 13.\n/uni00000015/uni00000014/uni00000018/uni00000015/uni00000014/uni00000019/uni00000015/uni00000014/uni0000001a\n/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000015/uni00000012/uni00000017/uni00000014/uni00000014/uni00000015/uni00000012/uni00000017/uni00000016/uni00000019/uni00000015/uni00000012/uni00000017/uni00000019/uni00000014/uni00000015/uni00000012/uni00000017/uni0000001b/uni00000019/uni00000015/uni00000012/uni00000018/uni00000014/uni00000014/uni00000015/uni00000012/uni00000018/uni00000016/uni00000019/uni00000015/uni00000012/uni00000018/uni00000019/uni00000014/uni00000015/uni00000012/uni00000018/uni0000001b/uni00000019/uni00000026/uni0000004d/uni00000058/uni00000057/uni00000004/uni00000034/uni00000049/uni00000056/uni00000004/uni00000026/uni0000005d/uni00000058/uni00000049\n/uni00000037/uni00000047/uni00000045/uni00000050/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000011/uni00000004/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000004/uni0000000c/uni0000003d/uni00000053/uni00000059/uni00000038/uni00000059/uni00000046/uni00000049/uni00000031/uni0000004d/uni0000005c/uni0000000d\n/uni00000037/uni00000018/uni0000000f/uni0000002a/uni0000002a/uni00000032\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045Figure 7: ( Audio Pretraining .) Mamba improves performance\nover prior state-of-the-art (Sashimi) in autoregressive audio mod-\neling, while improving up to minute-long context or million-\nlength sequences (controlling for computation).\nwhich may be very noisy. Note that while HyenaDNA claims to improve with longer context, their results do not\ncontrol for computation time.\n4.3.3 Synthetic Species Classi/f_ication\nWe evaluate models on a downstream task of classifying between 5 di\ufb00erent species by randomly sampling a contigu-\nous segment of their DNA. This task is adapted from HyenaDNA, which used the species {human ,lemur ,mouse ,pig,hippo }.\nWe modify the task to be signi\ufb01cantly more challenging by classifying between the \ufb01ve great apes species\n{human ,chimpanzee ,gorilla ,orangutan ,bonobo }, which are known to share 99% of their DNA.\n4.4 Audio Modeling and Generation\nFor the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel\net al. 2022).", "start_char_idx": 5196, "end_char_idx": 7945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c": {"__data__": {"id_": "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "92a9c70d77ff9b2ec29be483a2a640d33ee625c8cf8d3685f14709cfccf3c0f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dea454af-e062-43f9-8cdc-a456c19ec801", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "7adc802021797380d5484733eb020348ce4b4f5749285bba1ace70f05273f3f1", "class_name": "RelatedNodeInfo"}}, "text": "which may be very noisy. Note that while HyenaDNA claims to improve with longer context, their results do not\ncontrol for computation time.\n4.3.3 Synthetic Species Classi/f_ication\nWe evaluate models on a downstream task of classifying between 5 di\ufb00erent species by randomly sampling a contigu-\nous segment of their DNA. This task is adapted from HyenaDNA, which used the species {human ,lemur ,mouse ,pig,hippo }.\nWe modify the task to be signi\ufb01cantly more challenging by classifying between the \ufb01ve great apes species\n{human ,chimpanzee ,gorilla ,orangutan ,bonobo }, which are known to share 99% of their DNA.\n4.4 Audio Modeling and Generation\nFor the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel\net al. 2022). This model comprises\n1.a U-Net backbone with two stages of pooling by a factor pthat doubles the model dimension /u1D437per stage,\n2.alternating S4 and MLP blocks in each stage.\nWe consider replacing the S4+MLP blocks with Mamba blocks. Experiment details are in Appendix E.4.\n4.4.1 Long-Context Autoregressive Pretraining\nWe evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017), a\nstandard piano music dataset used by prior work consisting of 4hours of solo piano music, sampled at a rate of\n14", "start_char_idx": 7174, "end_char_idx": 8488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "96b7d701-bab3-496e-9983-f91c7c10d99f": {"__data__": {"id_": "96b7d701-bab3-496e-9983-f91c7c10d99f", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "365a0e8d-416c-4077-931f-4527a21aa813", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "6a47b91c2e01235124d81732ceed25688c4f85468ad828f79727f34bad719924", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87c9e0a3-9ea4-4dcc-9b3c-548848424704", "node_type": "1", "metadata": {}, "hash": "5f269826bab6af00aaf6a777f82c4d0cadf79cb703004e528265e8329288ecdc", "class_name": "RelatedNodeInfo"}}, "text": "16000 Hz Pretraining details largely follow the standard language modeling setup (Section 4.2). Figure 7evaluates\nthe e\ufb00ect of increasing training sequence lengths from 213= 8192 to220/uni2248 106, while keeping computation \ufb01xed.\n(There are some slight edge cases to the way the data is curated, which may lead to kinks in the scaling curves.\nFor example, only minute-long clips were available so the maximum sequence length is actually bounded by\n60/u1D460/uni22C516000 /u1D43B/u1D467= 960000 .)\nBoth Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba is\nbetter throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a\nconstant factor log.2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.\nWe note one important detail: this is the only experiment in this paper in which we switched from the real\nparameterization to complex (Section 3.6). We show additional ablations in Appendix E.4.\n4.4.2 Autoregressive Speech Generation\nSC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting\nof1-second clips sampled at 16000 Hz of the digits \u201czero\u201d through \u201cnine\u201d with highly variable characteristics. We\nlargely follow the autoregressive training setup and generation protocol of Goel et al. ( 2022).\nTable 4shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al.\n(2022): WaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette\n2019), Di\ufb00Wave (Z. Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art\n(and much larger) GAN- and di\ufb00usion- based models. A larger model parameter-matched to the baselines further\nimproves on \ufb01delity metrics dramatically.\nTable 5takes the small Mamba model and investigates combinations of di\ufb00erent architectures for the outer stages\nand center stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba >\nS4+MLP >MHA+MLP in the center blocks.\nTable 4: ( SC09 ) Automated metrics for unconditional generation\non a challenging dataset of /f_ixed-length speech clips. ( Top to\nBottom ) Autoregressive baselines, non-autoregressive baselines,\nMamba, and dataset metrics.\nModel Params NLL /uni2193 FID/uni2193 IS/uni2191 mIS/uni2191 AM/uni2193\nSampleRNN 35.0M 2.042 8.96 1.71 3.02 1.76\nWaveNet 4.2M 1.925 5.08 2.27 5.80 1.47\nSaShiMi 5.8M 1.873 1.99 5.13 42.57 0.74\nWaveGAN 19.1M - 2.03 4.90 36.10 0.80\nDi\ufb00Wave 24.1M - 1.92 5.26 51.21 0.68\n+ SaShiMi 23.0M - 1.42 5.94 69.17 0.59\nMamba 6.1M 1.852 0.94 6.26 88.54 0.52\nMamba 24.3M 1.860 0.67 7.33 144.9 0.36\nTrain - - 0.00 8 .56 292 .5 0 .16\nTest - - 0.02 8 .33 257 .6 0 .19Table 5: ( SC09 Model Ablations ) Models with 6M parameters.", "start_char_idx": 0, "end_char_idx": 2830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "87c9e0a3-9ea4-4dcc-9b3c-548848424704": {"__data__": {"id_": "87c9e0a3-9ea4-4dcc-9b3c-548848424704", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "365a0e8d-416c-4077-931f-4527a21aa813", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "6a47b91c2e01235124d81732ceed25688c4f85468ad828f79727f34bad719924", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96b7d701-bab3-496e-9983-f91c7c10d99f", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "e814b33745f281124e0db85d43ce12fa49e57ff02103ee7df2b1810ca90fdcdf", "class_name": "RelatedNodeInfo"}}, "text": "In SaShiMi\u2019s U-Net backbone, there are 8 center blocks operat-\ning on sequence length 1000 , sandwiched on each side by 8 outer\nblocks on sequence length 4000 , sandwiched by 8 outer blocks\non sequence length 16000 (40 blocks total). The architecture of\nthe 8 center blocks are ablated independently of the rest. Note\nthat Transformers (MHA+MLP) were not tested in the more im-\nportant outer blocks because of e\ufb03ciency constraints.\nOuter Center NLL /uni2193 FID/uni2193 IS/uni2191 mIS/uni2191 AM/uni2193\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\nMamba MHA+MLP 1.850 1.37 5.63 58.23 0.62\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\nMamba Mamba 1.852 0.94 6.26 88.54 0.52\n4.5 Speed and Memory Benchmarks\nWe benchmark the speed of the SSM scan operation (state expansion /u1D441= 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our e\ufb03cient SSM scan is faster than the best attention implementation that\nwe know of (FlashAttention-2 (Dao 2023)) beyond sequence length 2K, and up to 20-40 \u00d7faster than a standard\nscan implementation in PyTorch. Mamba achieves 4-5 \u00d7higher inference throughput than a Transformer of similar\nsize, since without the KV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained)\nwould have higher inference throughput than a 5\u00d7smaller Transformer-1.3B. Details in Appendix E.5, which\nadditionally includes a benchmark of memory consumption.\n15", "start_char_idx": 2831, "end_char_idx": 4324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "60f01090-cea4-4e5a-9afd-d0db06795648": {"__data__": {"id_": "60f01090-cea4-4e5a-9afd-d0db06795648", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ffca80b-def6-4e14-aa7e-c7d6b9fa1147", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "e839997d80dec116bbc7f42948d81e3cc5eaee4f38fd227bef1f6fa8029c2c03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbe28570-9918-4321-a3d3-d95aeee6725a", "node_type": "1", "metadata": {}, "hash": "8563e674a284e94c6ff42f66534a936420b4f3deda40fb204d531a402528a830", "class_name": "RelatedNodeInfo"}}, "text": "/uni00000019/uni00000015/uni00000016 /uni00000015/uni0000004f /uni00000016/uni0000004f /uni00000018/uni0000004f /uni0000001c/uni0000004f /uni00000015/uni0000001a/uni0000004f /uni00000017/uni00000016/uni0000004f /uni0000001a/uni00000018/uni0000004f /uni00000015/uni00000016/uni0000001c/uni0000004f /uni00000016/uni00000019/uni0000001a/uni0000004f /uni00000019/uni00000015/uni00000016/uni0000004f\n/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000050/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000014/uni00000012/uni00000015/uni00000015/uni00000015/uni00000014/uni00000015/uni00000014/uni00000014/uni00000015/uni00000014/uni00000014/uni00000014/uni00000038/uni0000004d/uni00000051/uni00000049/uni00000004/uni0000000c/uni00000051/uni00000057/uni0000000d\n/uni00000037/uni00000047/uni00000045/uni00000052/uni00000004/uni0000005a/uni00000057/uni00000004/uni00000027/uni00000053/uni00000052/uni0000005a/uni00000053/uni00000050/uni00000059/uni00000058/uni0000004d/uni00000053/uni00000052/uni00000004/uni0000005a/uni00000057/uni00000004/uni00000025/uni00000058/uni00000058/uni00000049/uni00000052/uni00000058/uni0000004d/uni00000053/uni00000052/uni00000004/uni00000058/uni0000004d/uni00000051/uni00000049/uni00000004/uni0000000c/uni00000025/uni00000015/uni00000014/uni00000014/uni00000004/uni0000001c/uni00000014/uni0000002b/uni00000026/uni00000004/uni00000034/uni00000027/uni0000002d/uni00000049/uni0000000d\n/uni0000002a/uni00000050/uni00000045/uni00000057/uni0000004c/uni00000025/uni00000058/uni00000058/uni00000049/uni00000052/uni00000058/uni0000004d/uni00000053/uni00000052/uni00000011/uni00000016\n/uni00000027/uni00000053/uni00000052/uni0000005a/uni00000053/uni00000050/uni00000059/uni00000058/uni0000004d/uni00000053/uni00000052\n/uni00000037/uni00000047/uni00000045/uni00000052/uni00000004/uni0000000c/uni00000034/uni0000005d/uni00000038/uni00000053/uni00000056/uni00000047/uni0000004c/uni0000000d\n/uni00000037/uni00000047/uni00000045/uni00000052/uni00000004/uni0000000c/uni00000053/uni00000059/uni00000056/uni00000057/uni0000000d\n/uni00000033/uni00000033/uni00000031\n/uni00000015 /uni00000016 /uni00000018 /uni0000001c /uni00000015/uni0000001a /uni00000017/uni00000016 /uni0000001a/uni00000018", "start_char_idx": 0, "end_char_idx": 2280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fbe28570-9918-4321-a3d3-d95aeee6725a": {"__data__": {"id_": "fbe28570-9918-4321-a3d3-d95aeee6725a", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ffca80b-def6-4e14-aa7e-c7d6b9fa1147", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "e839997d80dec116bbc7f42948d81e3cc5eaee4f38fd227bef1f6fa8029c2c03", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60f01090-cea4-4e5a-9afd-d0db06795648", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "96e38b6ca997b6ff459b30f8a86bd5290928966347b4eac27a952889c60bf373", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17cc3242-19f1-482e-8005-f2eda9ff14bc", "node_type": "1", "metadata": {}, "hash": "03edd60138dfbbaa2ab36c919c4d03d49aacea5ba2a46e7edd058bde5021a6e4", "class_name": "RelatedNodeInfo"}}, "text": "/uni00000016 /uni00000018 /uni0000001c /uni00000015/uni0000001a /uni00000017/uni00000016 /uni0000001a/uni00000018 /uni00000015/uni00000016/uni0000001c\n/uni00000026/uni00000045/uni00000058/uni00000047/uni0000004c/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049/uni00000019/uni00000014/uni00000014/uni00000015/uni00000014/uni00000014/uni00000014/uni00000015/uni00000019/uni00000014/uni00000014/uni00000038/uni0000004c/uni00000056/uni00000053/uni00000059/uni0000004b/uni0000004c/uni00000054/uni00000059/uni00000058/uni00000004/uni0000000c/uni00000058/uni00000053/uni0000004f/uni00000049/uni00000052/uni00000057/uni00000004/uni00000013/uni00000004/uni00000057/uni0000000d\n/uni00000015/uni00000018/uni00000014/uni00000016/uni00000018/uni0000001b/uni00000018/uni00000018/uni00000015/uni0000001b/uni00000018/uni00000018/uni00000015/uni00000014/uni0000001c/uni0000001d/uni00000015/uni00000018/uni00000018/uni00000019/uni00000015/uni0000001a/uni0000001c/uni0000001c/uni00000015/uni0000001c/uni00000015/uni00000018\n/uni0000001b/uni0000001d/uni00000015/uni00000017/uni00000016/uni00000015/uni0000001d/uni0000001d/uni00000016/uni0000001a/uni00000019/uni00000017/uni00000016/uni00000017/uni00000017/uni0000001a/uni00000018\n/uni00000033/uni00000033/uni00000031 /uni00000033/uni00000033/uni00000031/uni00000019/uni0000001c/uni00000015/uni00000014/uni00000015/uni00000015/uni0000001b/uni00000016/uni00000016/uni0000001a/uni00000015/uni00000017/uni0000001a/uni00000018/uni00000018/uni00000018/uni00000017/uni00000018/uni0000001d/uni00000014/uni00000019/uni00000015/uni00000019\n/uni00000018/uni0000001a/uni0000001a/uni0000001a/uni0000001d/uni00000015/uni00000015/uni00000014/uni0000001d /uni00000015/uni00000016/uni00000014\n/uni00000033/uni00000033/uni00000031 /uni00000033/uni00000033/uni00000031", "start_char_idx": 2167, "end_char_idx": 3958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "17cc3242-19f1-482e-8005-f2eda9ff14bc": {"__data__": {"id_": "17cc3242-19f1-482e-8005-f2eda9ff14bc", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ffca80b-def6-4e14-aa7e-c7d6b9fa1147", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "e839997d80dec116bbc7f42948d81e3cc5eaee4f38fd227bef1f6fa8029c2c03", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbe28570-9918-4321-a3d3-d95aeee6725a", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "debb35bdeccbc6957ee36c9fc832ad2d8f13bcec663537f7d0407334a2116c27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0621218-24f2-47e6-a2a6-a78f0e52da5a", "node_type": "1", "metadata": {}, "hash": "a34d53b556e13f7356f8bf5bf18fa815f12d729a1698089a05d94ccd366a34ee", "class_name": "RelatedNodeInfo"}}, "text": "/uni00000015/uni00000016/uni00000014\n/uni00000033/uni00000033/uni00000031 /uni00000033/uni00000033/uni00000031 /uni00000033/uni00000033/uni00000031/uni0000002d/uni00000052/uni0000004a/uni00000049/uni00000056/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000058/uni0000004c/uni00000056/uni00000053/uni00000059/uni0000004b/uni0000004c/uni00000054/uni00000059/uni00000058/uni00000004/uni00000053/uni00000052/uni00000004/uni00000025/uni00000015/uni00000014/uni00000014/uni00000004/uni0000001c/uni00000014/uni0000002b/uni00000026/uni00000004/uni0000000c/uni00000054/uni00000056/uni00000053/uni00000051/uni00000054/uni00000058/uni00000004/uni00000050/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000004/uni00000016/uni00000014/uni00000018/uni0000001c/uni0000000d\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000004/uni00000015/uni00000012/uni00000018/uni00000026\n/uni00000038/uni00000056/uni00000045/uni00000052/uni00000057/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000049/uni00000056/uni00000004/uni00000015/uni00000012/uni00000017/uni00000026\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000004/uni0000001a/uni00000012/uni0000001d/uni00000026\n/uni00000038/uni00000056/uni00000045/uni00000052/uni00000057/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000049/uni00000056/uni00000004/uni0000001a/uni00000012/uni0000001b/uni00000026Figure 8: ( E\ufb03ciency Benchmarks .) (Left) Training: our e\ufb03cient scan is 40\u00d7faster than a standard implementation. ( Right )\nInference: as a recurrent model, Mamba can achieve 5\u00d7higher throughput than Transformers.\n4.6 Model Ablations\nWe perform a series of detailed ablations on components of our model, focusing on the setting of language modeling\nwith size /uni2248 350 M models at Chinchilla token counts (same setting as Figure 4).\n4.6.1 Architecture\nTable 6investigates the e\ufb00ects of the architecture (block) and its inner SSM layer (Figure 3). We \ufb01nd that\n\u2022Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very\nsimilar.\n\u2022Replacing the complex-valued S4 variant from previous work with a real-valued one does not a\ufb00ect performance\nmuch, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware\ne\ufb03ciency.\n\u2022Replacing any of these with a selective SSM (S6) signi\ufb01cantly improves performance, validating the motivation\nof Section 3.\n\u2022The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a\nselective layer).\nWe also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA\n(a hybrid attention architecture) in Appendix E.2.2 .", "start_char_idx": 3848, "end_char_idx": 6592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f0621218-24f2-47e6-a2a6-a78f0e52da5a": {"__data__": {"id_": "f0621218-24f2-47e6-a2a6-a78f0e52da5a", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ffca80b-def6-4e14-aa7e-c7d6b9fa1147", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "e839997d80dec116bbc7f42948d81e3cc5eaee4f38fd227bef1f6fa8029c2c03", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17cc3242-19f1-482e-8005-f2eda9ff14bc", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "02f822cd42f4f3e68a1da5fe0bccc46560ecd00c9a7d9e7d1c1d5fbc725ade6a", "class_name": "RelatedNodeInfo"}}, "text": "We \ufb01nd that\n\u2022Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very\nsimilar.\n\u2022Replacing the complex-valued S4 variant from previous work with a real-valued one does not a\ufb00ect performance\nmuch, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware\ne\ufb03ciency.\n\u2022Replacing any of these with a selective SSM (S6) signi\ufb01cantly improves performance, validating the motivation\nof Section 3.\n\u2022The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a\nselective layer).\nWe also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA\n(a hybrid attention architecture) in Appendix E.2.2 .\n4.6.2 Selective SSM\nTable 7ablates the selective SSM layer by considering di\ufb00erent combinations of selective \u2206,B, andCparam-\neters (Algorithm 2), showing that \u2206is the most important parameter due to its connection to RNN gating\n(Theorem 1).\nTable 8considers di\ufb00erent initializations of the SSM, which have been shown to make a large di\ufb00erence in some\ndata modalities and settings (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022). On language modeling, we \ufb01nd\nthat simpler real-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued\nparameterizations (S4D-Lin, row 1) perform better. Random initializations also work well, consistent with \ufb01ndings\nfrom prior work (Mehta et al. 2023).\nTable 9and Table 10consider varying the dimension of the \u2206and .B,C)projections respectively. Changing\nthem from static to selective provides the most bene\ufb01t, while increasing the dimensions further generally improves\nperformance modestly with a small increase in parameter count.\nOf particular note is the dramatic improvement of the selective SSM when the state size /u1D441is increased, with over\na 1.0 perplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in\nSections 3.1and3.3.\n16", "start_char_idx": 5821, "end_char_idx": 7843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ebb21d80-1110-4bd8-bdf3-c3a6af5ac616": {"__data__": {"id_": "ebb21d80-1110-4bd8-bdf3-c3a6af5ac616", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95533d23-8d21-48de-9019-b7d57264539a", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c1eedb17e614c5eec4431ac615bf3459d0e280f56b8775675aee2ed24ff7e089", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b2e8f94-c841-4cf2-9857-e4b149c83199", "node_type": "1", "metadata": {}, "hash": "f3ef53f0e50cab4909280a74959182f2b163ebce9ef4d2ae06f789042845e24a", "class_name": "RelatedNodeInfo"}}, "text": "Table 6: ( Ablations: Architecture and SSM layer .) The Mamba block performs similarly to H3 while being simpler. In the\ninner layer, there is little di\ufb00erence among di\ufb00erent parameterizations of LTI models, while selective SSMs (S6) provide a large\nimprovement. More speci/f_ically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.\nModel Arch. SSM Layer Perplexity\nHyena H3 Hyena 10.24\nH3 H3 S4 (complex) 10.30\n- H3 S4 (real) 10.34\n- H3 S6 8.95Model Arch. SSM Layer Perplexity\n- Mamba Hyena 10.75\n- Mamba S4 (complex) 10.54\n- Mamba S4 (real) 10.56\nMamba Mamba S6 8.69\nTable 7: ( Ablations: Selective parameters .)\u2206is the most im-\nportant parameter (Theorem 1), but using multiple selective pa-\nrameters together synergizes.\nSelective \u2206Selective B Selective C Perplexity\n\u0017 \u0017 \u0017 10.93\n\u0017 \u0013 \u0017 10.15\n\u0017 \u0017 \u0013 9.98\n\u0013 \u0017 \u0017 9.81\n\u0013 \u0013 \u0013 8.71Table 8: ( Ablations: Parameterization of A.) The more\nstandard initializations based on S4D-Lin (Gu, Gupta, et al.\n2022 ) perform worse than S4D-Real or a random initializa-\ntion, when the SSM is selective.\nA/u1D45BInitialization Field Perplexity\nA/u1D45B= /uni22121\n2+/u1D45B/u1D456 Complex 9.16\nA/u1D45B= /uni22121/uni22152 Real 8.85\nA/u1D45B= /uni2212./u1D45B+ 1) Real 8.71\nA/u1D45B/uni223C exp./u1D4A9.0,1)) Real 8.71\nTable 9: ( Ablations: Expressivity of \u2206.)\nThe selection mechanism of \u2206constructs\nit with a projection of the input. Project-\ning it even to dim. 1provides a large in-\ncrease in performance; increasing it fur-\nther provides further improvements at the\ncost of a modest increase in parameters.\nState size /f_ixed to /u1D441= 16 .\nSize of \u2206proj. Params (M) Perplexity\n- 358.9 9.12\n1 359.1 8.97\n2 359.3 8.97\n4 359.7 8.91\n8 360.5 8.83\n16 362.1 8.84\n32 365.2 8.80\n64 371.5 8.71Table 10: ( Ablations: SSM state dimension .) (Top) Constant BandC(Bottom )\nSelective BandC. Increasing the SSM state dimension /u1D441, which can be viewed as\nan expansion factor on the dimension of the recurrent state, can signi/f_icantly improve\nperformance for a negligible cost in parameters/FLOPs, but only when BandCare\nalso selective. Size of \u2206projection /f_ixed to 64.\nState dimension /u1D441 Params (M) Perplexity\n1 367.1 9.88\n2 367.4 9.86\n4 368.0 9.82\n8 369.1 9.82\n16 371.5 9.81\n1 367.1 9.73\n2 367.4 9.40\n4 368.0 9.09\n8 369.1 8.84\n16 371.5 8.71\n5 Discussion\nWe discuss related work, limitations, and some future directions.\nRelated Work. Appendix Adiscusses how the selection mechanism relates to similar concepts. Appendix Bhas\nan extended related work of SSMs and other related models.\nNo Free Lunch: Continuous-Discrete Spectrum.", "start_char_idx": 0, "end_char_idx": 2590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0b2e8f94-c841-4cf2-9857-e4b149c83199": {"__data__": {"id_": "0b2e8f94-c841-4cf2-9857-e4b149c83199", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95533d23-8d21-48de-9019-b7d57264539a", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c1eedb17e614c5eec4431ac615bf3459d0e280f56b8775675aee2ed24ff7e089", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebb21d80-1110-4bd8-bdf3-c3a6af5ac616", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "421f28e370b94619d2938a6d670d5b04ae356faf15d467cbcf36bd03868db77a", "class_name": "RelatedNodeInfo"}}, "text": "Size of \u2206projection /f_ixed to 64.\nState dimension /u1D441 Params (M) Perplexity\n1 367.1 9.88\n2 367.4 9.86\n4 368.0 9.82\n8 369.1 9.82\n16 371.5 9.81\n1 367.1 9.73\n2 367.4 9.40\n4 368.0 9.09\n8 369.1 8.84\n16 371.5 8.71\n5 Discussion\nWe discuss related work, limitations, and some future directions.\nRelated Work. Appendix Adiscusses how the selection mechanism relates to similar concepts. Appendix Bhas\nan extended related work of SSMs and other related models.\nNo Free Lunch: Continuous-Discrete Spectrum. Structured SSMs were originally de\ufb01ned as discretizations\nof continuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as\nperceptual signals (e.g. audio, video). As discussed in Sections 3.1and3.5, the selection mechanism overcomes\ntheir weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance\n17", "start_char_idx": 2090, "end_char_idx": 2983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "17eb10ae-b5fc-4846-9c4f-fffb6a928323": {"__data__": {"id_": "17eb10ae-b5fc-4846-9c4f-fffb6a928323", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0d91f6e-f6d8-4314-b8d5-696e91b90d3a", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "0eb0479e358b6e2633b4992aee772d8e52b8c137c18df9d8397380b82b685b34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e775158-6f90-4613-a73d-0d6a27ea26ec", "node_type": "1", "metadata": {}, "hash": "4fbabcea32b81c678a01bff8822e8a90c489bf170d38c02799f82ee8e085a986", "class_name": "RelatedNodeInfo"}}, "text": "on data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeo\ufb00 in more detail.\nDownstream A\ufb00ordances. Transformer-based foundation models (particularly LLMs) have a rich ecosystem of\nproperties and modes of interaction with pretrained models, such as \ufb01ne-tuning, adaptation, prompting, in-context\nlearning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer\nalternatives such as SSMs have similar properties and a\ufb00ordances.\nScaling. Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source\nLLMs (e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023)\nand RetNet (Y. Sun et al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to\nassess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve\nfurther engineering challenges and adjustments to the model that are not discussed in this paper.\n6 Conclusion\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent\nreasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture,\nMamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance\nof strong Transformer models. We are excited about the broad applications of selective state space models to\nbuild foundation models for di\ufb00erent domains, especially in emerging modalities requiring long context such as\ngenomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model\nbackbone.\nAcknowledgments\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.\nReferences\n[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. \u201cUnitary Evolution Recurrent Neural Networks\u201d. In: The\nInternational Conference on Machine Learning (ICML) . 2016, pp. 1120\u20131128.\n[2] iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor,\nYannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. \u201cE\ufb00ective Gene Expression Prediction from\nSequence by Integrating Long-range Interactions\u201d. In: Nature Methods 18.10 (2021), pp. 1196\u20131203.\n[3] Jimmy Ba, Geo\ufb00rey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. \u201cUsing Fast Weights to\nAttend to the Recent Past\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016).\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geo\ufb00rey E Hinton. \u201cLayer Normalization\u201d. In: arXiv preprint arXiv:1607.06450\n(2016).\n[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \u201cNeural Machine Translation by Jointly Learning to\nAlign and Translate\u201d. In: The International Conference on Learning Representations (ICLR) . 2015.\n[6] David Balduzzi and Muhammad Ghifary. \u201cStrongly-typed Recurrent Neural Networks\u201d. In: International Con-\nference on Machine Learning . PMLR. 2016, pp. 1292\u20131300.\n[7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan,\nMohammad A/f_lah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Ra\ufb00, et al. \u201cPythia: A Suite for\nAnalyzing Large Language Models across Training and Scaling\u201d. In: The International Conference on Machine\nLearning (ICML) . PMLR. 2023, pp. 2397\u20132430.\n[8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. \u201cPIQA: Reasoning about Physical Commonsense\nin Natural Language\u201d. In: Proceedings of the AAAI conference on Arti/f_icial Intelligence . Vol. 34. 05. 2020, pp.", "start_char_idx": 0, "end_char_idx": 3636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3e775158-6f90-4613-a73d-0d6a27ea26ec": {"__data__": {"id_": "3e775158-6f90-4613-a73d-0d6a27ea26ec", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0d91f6e-f6d8-4314-b8d5-696e91b90d3a", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "0eb0479e358b6e2633b4992aee772d8e52b8c137c18df9d8397380b82b685b34", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17eb10ae-b5fc-4846-9c4f-fffb6a928323", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "2f759445fb0654e24e28276d99bf368ba602ad6477b88e683b7b90f10bba3a7a", "class_name": "RelatedNodeInfo"}}, "text": "1292\u20131300.\n[7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan,\nMohammad A/f_lah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Ra\ufb00, et al. \u201cPythia: A Suite for\nAnalyzing Large Language Models across Training and Scaling\u201d. In: The International Conference on Machine\nLearning (ICML) . PMLR. 2023, pp. 2397\u20132430.\n[8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. \u201cPIQA: Reasoning about Physical Commonsense\nin Natural Language\u201d. In: Proceedings of the AAAI conference on Arti/f_icial Intelligence . Vol. 34. 05. 2020, pp. 7432\u2013\n7439.\n[9] Guy E Blelloch. \u201cPre/f_ix Sums and Their Applications\u201d. In: (1990).\n[10] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. \u201cQuasi-recurrent Neural Networks\u201d. In:\narXiv preprint arXiv:1611.01576 (2016).\n18", "start_char_idx": 3038, "end_char_idx": 3874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "17b4198b-c2c7-4221-a269-37f0ab636d5d": {"__data__": {"id_": "17b4198b-c2c7-4221-a269-37f0ab636d5d", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8144dd54-a80a-4505-a0d8-5bc4cb857c7b", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "412e3aff6c41c5a8c917d9fd67e3ee9cdb32bb8ab7efa370fb6f6dec16bdaf7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "022487f1-5249-4fda-9dfc-9dbd89191093", "node_type": "1", "metadata": {}, "hash": "8a5f5480e5576a86b6d685b846c2db7223dff2d00d0019bcb72b769b2b92870b", "class_name": "RelatedNodeInfo"}}, "text": "[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. \u201cLanguage Models are Few-shot Learners\u201d. In:\nAdvances in Neural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877\u20131901.\n[12] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. \u201cScaling Transformer to 1M tokens and Beyond with RMT\u201d.\nIn:arXiv preprint arXiv:2304.11062 (2023).\n[13] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. \u201cGenerating Long Sequences with Sparse Trans-\nformers\u201d. In: arXiv preprint arXiv:1904.10509 (2019).\n[14] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Pe-\nter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. \u201cRethinking Attention with Performers\u201d. In:\nThe International Conference on Learning Representations (ICLR) . 2021.\n[15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. \u201cPaLM: Scaling Language Modeling\nwith Pathways\u201d. In: Journal of Machine Learning Research 24.240 (2023), pp. 1\u2013113. url: http://jmlr.org/\npapers/v24/22-1144.html .\n[16] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. \u201cEmpirical Evaluation of Gated Re-\ncurrent Neural Networks on Sequence Modeling\u201d. In: arXiv preprint arXiv:1412.3555 (2014).\n[17] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. \u201cThink you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\u201d. In: arXiv\npreprint arXiv:1803.05457 (2018).\n[18] Tri Dao. \u201cFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\u201d. In: (2023).\n[19] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. \u201cFlashAttention: Fast and Memory-\nE\ufb03cient Exact Attention with IO-Awareness\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) .\n2022.\n[20] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. \u201cHungry Hungry\nHippos: Towards Language Modeling with State Space Models\u201d. In: The International Conference on Learning\nRepresentations (ICLR) . 2023.\n[21] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. \u201cLanguage Modeling with Gated Convolu-\ntional Networks\u201d. In: The International Conference on Machine Learning (ICML) . PMLR. 2017, pp. 933\u2013941.\n[22] DeepSound. SampleRNN .https://github.com/deepsound-project/samplernn-pytorch . 2017.\n[23] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. \u201cLongNet:\nScaling Transformers to 1,000,000,000 Tokens\u201d. In: arXiv preprint arXiv:2307.02486 (2023).\n[24] Chris Donahue, Julian McAuley, and Miller Puckette. \u201cAdversarial Audio Synthesis\u201d. In: The International\nConference on Learning Representations (ICLR) . 2019.", "start_char_idx": 0, "end_char_idx": 2921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "022487f1-5249-4fda-9dfc-9dbd89191093": {"__data__": {"id_": "022487f1-5249-4fda-9dfc-9dbd89191093", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8144dd54-a80a-4505-a0d8-5bc4cb857c7b", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "412e3aff6c41c5a8c917d9fd67e3ee9cdb32bb8ab7efa370fb6f6dec16bdaf7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17b4198b-c2c7-4221-a269-37f0ab636d5d", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "98153ed7787123baee6320679c4f08aab99d099c3dcb42541e0129a98c982e40", "class_name": "RelatedNodeInfo"}}, "text": "\u201cLanguage Modeling with Gated Convolu-\ntional Networks\u201d. In: The International Conference on Machine Learning (ICML) . PMLR. 2017, pp. 933\u2013941.\n[22] DeepSound. SampleRNN .https://github.com/deepsound-project/samplernn-pytorch . 2017.\n[23] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. \u201cLongNet:\nScaling Transformers to 1,000,000,000 Tokens\u201d. In: arXiv preprint arXiv:2307.02486 (2023).\n[24] Chris Donahue, Julian McAuley, and Miller Puckette. \u201cAdversarial Audio Synthesis\u201d. In: The International\nConference on Learning Representations (ICLR) . 2019.\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. \u201cAn Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale\u201d. In: The International Conference on Learning Representations\n(ICLR) . 2020.\n[26] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hat/f_ield-Dodds, Danny\nHernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack\nClark, Jared Kaplan, Sam McCandlish, and Chris Olah. \u201cA Mathematical Framework for Transformer Circuits\u201d.\nIn:Transformer Circuits Thread (2021). https://transformer-circuits.pub/2021/framework/index.html.\n[27] Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. \u201cBlock-\nState Transformer\u201d. In: arXiv preprint arXiv:2306.09539 (2023).\n[28] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu,\nYangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. \u201cMulti-Head State Space Model for Sequence Modeling\u201d. In:\nINTERSPEECH . 2023.\n[29] Karl J Friston, Lee Harrison, and Will Penny. \u201cDynamic Causal Modelling\u201d. In: Neuroimage 19.4 (2003), pp. 1273\u2013\n1302.\n[30] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christo-\npher R\u00e9. \u201cSimple Hardware-e\ufb03cient Long Convolutions for Sequence Modeling\u201d. In: The International Confer-\nence on Machine Learning (ICML) (2023).\n[31] Ken-ichi Funahashi and Yuichi Nakamura. \u201cApproximation of Dynamical Systems by Continuous Time Recur-\nrent Neural Networks\u201d. In: Neural Networks 6.6 (1993), pp. 801\u2013806.\n19", "start_char_idx": 2327, "end_char_idx": 4747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c355c207-99d4-490f-8c8e-5812433f524a": {"__data__": {"id_": "c355c207-99d4-490f-8c8e-5812433f524a", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c3077941-d825-4144-9a16-4d06e432d7fd", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "e2a6f9a4debcaca97a49127dba7316f34b4cd26a0ad4c19ab33f67047ca0b3a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15d2effa-4713-4ebb-841a-4698fb93e294", "node_type": "1", "metadata": {}, "hash": "7dd23833a5f471c1df9376e0d1bea91c3b24c7068f9ecf21e0c5a3de124ea18e", "class_name": "RelatedNodeInfo"}}, "text": "[32] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,\nAnish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. \u201cThe Pile: An 800GB Dataset of Diverse Text\nfor Language Modeling\u201d. In: arXiv preprint arXiv:2101.00027 (2020).\n[33] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPo/f_i, Charles Foster, Laurence Golding, Je\ufb00rey\nHsu, Kyle McDonell, Niklas Muennigho\ufb00, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang,\nKevin Wang, and Andy Zou. A Framework for Few-shot Language Model Evaluation . Version v0.0.1. Sept. 2021.\ndoi:10.5281/zenodo.5371628 . url: https://doi.org/10.5281/zenodo.5371628 .\n[34] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. \u201cIt\u2019s Raw! Audio Generation with State-Space\nModels\u201d. In: The International Conference on Machine Learning (ICML) . 2022.\n[35] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. \u201cHIPPO: Recurrent Memory with Optimal\nPolynomial Projections\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) . 2020.\n[36] Albert Gu, Karan Goel, and Christopher R\u00e9. \u201cE\ufb03ciently Modeling Long Sequences with Structured State Spaces\u201d.\nIn:The International Conference on Learning Representations (ICLR) . 2022.\n[37] Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Ho\ufb00man, and Razvan Pascanu. \u201cImproving the Gating Mech-\nanism of Recurrent Neural Networks\u201d. In: The International Conference on Machine Learning (ICML) . 2020.\n[38] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. \u201cOn the Parameterization and Initialization of Diag-\nonal State Space Models\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) . 2022.\n[39] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. \u201cCombining Recur-\nrent, Convolutional, and Continuous-time Models with the Linear State Space Layer\u201d. In: Advances in Neural\nInformation Processing Systems (NeurIPS) . 2021.\n[40] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. \u201cHow to Train Your HIPPO: State\nSpace Models with Generalized Basis Projections\u201d. In: The International Conference on Learning Representations\n(ICLR) . 2023.\n[41] Ankit Gupta, Albert Gu, and Jonathan Berant. \u201cDiagonal State Spaces are as E\ufb00ective as Structured State\nSpaces\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 22982\u201322994.\n[42] David Ha, Andrew Dai, and Quoc V. Le. \u201cHyperNetworks\u201d. In: The International Conference on Learning Rep-\nresentations (ICLR) . 2017.\n[43] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. \u201cDream to Control: Learning Behav-\niors by Latent Imagination\u201d. In: The International Conference on Learning Representations (ICLR) . 2020.\n[44] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus.\n\u201cLiquid Structural State-Space Models\u201d. In: The International Conference on Learning Representations (ICLR) .\n2023.\n[45] Mikael Hena\ufb00, Arthur Szlam, and Yann LeCun. \u201cRecurrent Orthogonal Networks and Long-Memory Tasks\u201d.\nIn:The International Conference on Machine Learning (ICML) . 2016.\n[46] Dan Hendrycks and Kevin Gimpel. \u201cGaussian Error Linear Units (GELUs)\u201d. In: arXiv preprint arXiv:1606.08415\n(2016).\n[47] Sepp Hochreiter and J\u00fcrgen Schmidhuber.", "start_char_idx": 0, "end_char_idx": 3302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "15d2effa-4713-4ebb-841a-4698fb93e294": {"__data__": {"id_": "15d2effa-4713-4ebb-841a-4698fb93e294", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c3077941-d825-4144-9a16-4d06e432d7fd", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "e2a6f9a4debcaca97a49127dba7316f34b4cd26a0ad4c19ab33f67047ca0b3a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c355c207-99d4-490f-8c8e-5812433f524a", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "f9f44c2351531f325ff2fcce88c0d20bf8b8d934da66630651bbf49f67e9f368", "class_name": "RelatedNodeInfo"}}, "text": "In: The International Conference on Learning Representations (ICLR) . 2020.\n[44] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus.\n\u201cLiquid Structural State-Space Models\u201d. In: The International Conference on Learning Representations (ICLR) .\n2023.\n[45] Mikael Hena\ufb00, Arthur Szlam, and Yann LeCun. \u201cRecurrent Orthogonal Networks and Long-Memory Tasks\u201d.\nIn:The International Conference on Machine Learning (ICML) . 2016.\n[46] Dan Hendrycks and Kevin Gimpel. \u201cGaussian Error Linear Units (GELUs)\u201d. In: arXiv preprint arXiv:1606.08415\n(2016).\n[47] Sepp Hochreiter and J\u00fcrgen Schmidhuber. \u201cLong Short-Term Memory\u201d. In: Neural Computation 9.8 (1997),\npp. 1735\u20131780.\n[48] Jordan Ho\ufb00mann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. \u201cAn Empirical Analysis of Compute-\nOptimal Large Language Model Training\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 35\n(2022), pp. 30016\u201330030.\n[49] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. \u201cTransformer Quality in Linear Time\u201d. In: The Interna-\ntional Conference on Machine Learning (ICML) . PMLR. 2022, pp. 9099\u20139117.\n[50] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. \u201cDeep\nLearning for Time Series Classi/f_ication: A Review\u201d. In: Data Mining and Knowledge Discovery 33.4 (2019),\npp. 917\u2013963.\n[51] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoe/f_ler. \u201cData Movement is All You Need:\nA Case Study on Optimizing Transformers\u201d. In: Proceedings of Machine Learning and Systems 3 (2021), pp. 711\u2013\n732.\n[52] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. \u201cGated\nOrthogonal Recurrent Units: On Learning to Forget\u201d. In: Neural Computation 31.4 (2019), pp. 765\u2013783.\n[53] Rudolph Emil Kalman. \u201cA New Approach to Linear Filtering and Prediction Problems\u201d. In: (1960).\n20", "start_char_idx": 2672, "end_char_idx": 4690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "72956801-6df6-4b1e-9be0-a12be8f38a1e": {"__data__": {"id_": "72956801-6df6-4b1e-9be0-a12be8f38a1e", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58a7d5dc-a634-407b-9d1d-fec2ee973a1e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "53be758fe2eba292455ec3994135d9640f83aa6d5b4fc15eb9047b23c6afa703", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c7e8abe-f3bd-44ec-9aa5-cd6c1aed22d1", "node_type": "1", "metadata": {}, "hash": "3bad5039f9f9025a2305412c7c115d7b1288550d333e802801f2ec7327fd608f", "class_name": "RelatedNodeInfo"}}, "text": "[54] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. \u201cTransformers are RNNs: Fast\nAutoregressive Transformers with Linear Attention\u201d. In: International Conference on Machine Learning . PMLR.\n2020, pp. 5156\u20135165.\n[55] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. \u201cDi\ufb00Wave: A Versatile Di\ufb00usion Model\nfor Audio Synthesis\u201d. In: International Conference on Learning Representations . 2021.\n[56] Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. \u201cTime-Parameterized Convolutional Neu-\nral Networks for Irregularly Sampled Time Series\u201d. In: arXiv preprint arXiv:2308.03210 (2023).\n[57] Alex Krizhevsky, Ilya Sutskever, and Geo\ufb00rey E Hinton. \u201cImageNet Classi/f_ication with Deep Convolutional\nNeural Networks\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 25 (2012).\n[58] Tao Lei. \u201cWhen Attention Meets Fast Recurrence: Training Language Models with Reduced Compute\u201d. In:\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . 2021, pp. 7633\u20137648.\n[59] Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. \u201cSimple Recurrent Units for Highly Parallelizable\nRecurrence\u201d. In: arXiv preprint arXiv:1709.02755 (2017).\n[60] Mario Lezcano-Casado and David Mart\u00ednez-Rubio. \u201cCheap Orthogonal Constraints in Neural Networks: A\nSimple Parametrization of the Orthogonal and Unitary Group\u201d. In: The International Conference on Machine\nLearning (ICML) . 2019.\n[61] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. \u201cWhat Makes Convolutional Models\nGreat on Long Sequence Modeling?\u201d In: The International Conference on Learning Representations (ICLR) . 2023.\n[62] Vasileios Lioutas and Yuhong Guo. \u201cTime-aware Large Kernel Convolutions\u201d. In: The International Conference\non Machine Learning (ICML) . PMLR. 2020, pp. 6172\u20136183.\n[63] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behba-\nhani. \u201cStructured State Space Models for In-Context Reinforcement Learning\u201d. In: Advances in Neural Informa-\ntion Processing Systems (NeurIPS) . 2023.\n[64] Shahar Lutati, Itamar Zimerman, and Lior Wolf. \u201cFocus Your Attention (with Adaptive IIR Filters)\u201d. In: arXiv\npreprint arXiv:2305.14952 (2023).\n[65] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke\nZettlemoyer. \u201cMega: Moving Average Equipped Gated Attention\u201d. In: The International Conference on Learning\nRepresentations (ICLR) . 2023.\n[66] Eric Martin and Chris Cundy. \u201cParallelizing Linear Recurrent Neural Nets Over Sequence Length\u201d. In: The\nInternational Conference on Learning Representations (ICLR) . 2018.\n[67] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville,\nand Yoshua Bengio. \u201cSampleRNN: An Unconditional End-to-End Neural Audio Generation Model\u201d. In: The\nInternational Conference on Learning Representations (ICLR) . 2017.\n[68] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. \u201cLong Range Language Modeling via\nGated State Spaces\u201d. In: The International Conference on Learning Representations (ICLR) . 2023.\n[69] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. \u201cE\ufb03cient Orthogonal Parametri-\nsation of Recurrent Neural Networks using Householder Re/f_lections\u201d.", "start_char_idx": 0, "end_char_idx": 3332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1c7e8abe-f3bd-44ec-9aa5-cd6c1aed22d1": {"__data__": {"id_": "1c7e8abe-f3bd-44ec-9aa5-cd6c1aed22d1", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58a7d5dc-a634-407b-9d1d-fec2ee973a1e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "53be758fe2eba292455ec3994135d9640f83aa6d5b4fc15eb9047b23c6afa703", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72956801-6df6-4b1e-9be0-a12be8f38a1e", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "f582d9b851e064ac593eb18bf0f153ee46814ee49c00777faaa77c3209f2af65", "class_name": "RelatedNodeInfo"}}, "text": "2018.\n[67] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville,\nand Yoshua Bengio. \u201cSampleRNN: An Unconditional End-to-End Neural Audio Generation Model\u201d. In: The\nInternational Conference on Learning Representations (ICLR) . 2017.\n[68] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. \u201cLong Range Language Modeling via\nGated State Spaces\u201d. In: The International Conference on Learning Representations (ICLR) . 2023.\n[69] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. \u201cE\ufb03cient Orthogonal Parametri-\nsation of Recurrent Neural Networks using Householder Re/f_lections\u201d. In: International Conference on Machine\nLearning . PMLR. 2017, pp. 2401\u20132409.\n[70] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9.\n\u201cS4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces\u201d. In: Advances in Neural\nInformation Processing Systems (NeurIPS) . 2022.\n[71] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Pa-\ntel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. \u201cHyenaDNA: Long-range Genomic Sequence\nModeling at Single Nucleotide Resolution\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) .\n2023.\n[72] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hat/f_ield-Dodds, Danny\nHernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom\nBrown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. \u201cIn-context Learning and Induction Heads\u201d.\nIn:Transformer Circuits Thread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-\nheads/index.html.\n[73] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalch-\nbrenner, Andrew Senior, and Koray Kavukcuoglu. \u201cWaveNet: A Generative Model for Raw Audio\u201d. In: arXiv\npreprint arXiv:1609.03499 (2016).\n21", "start_char_idx": 2673, "end_char_idx": 4798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0f480892-3733-4265-ae7a-752ecc5f7335": {"__data__": {"id_": "0f480892-3733-4265-ae7a-752ecc5f7335", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8d930cb-81aa-40a0-97d8-b10fb05382dd", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "d98764ebc3902ba8c98b085d31231c762724b08535b175521048ba220ea58249", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a93f74f9-f1b2-419d-bb76-e17f653e5fb0", "node_type": "1", "metadata": {}, "hash": "7893b807178bf679c98d534446e289f0d4949f42f648090afc9b453b65812639", "class_name": "RelatedNodeInfo"}}, "text": "[74] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and So-\nham De. \u201cResurrecting Recurrent Neural Networks for Long Sequences\u201d. In: The International Conference on\nMachine Learning (ICML) . 2023.\n[75] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Ra\ufb00aella Bernardi, Sandro Pezzelle,\nMarco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. \u201cThe LAMBADA Dataset: Word Prediction Requiring\na Broad Discourse Context\u201d. In: Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics . 2016, pp. 1525\u20131534.\n[76] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. \u201cOn the Di\ufb03culty of Training Recurrent Neural Net-\nworks\u201d. In: International Conference on Machine Learning . 2013, pp. 1310\u20131318.\n[77] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael\nChung, Matteo Grella, Kranthi Kiran GV, et al. \u201cRWKV: Reinventing RNNs for the Transformer Era\u201d. In: arXiv\npreprint arXiv:2305.13048 (2023).\n[78] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. \u201cRandom\nFeature Attention\u201d. In: The International Conference on Learning Representations (ICLR) . 2021.\n[79] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano\nErmon, and Christopher R\u00e9. \u201cHyena Hierarchy: Towards Larger Convolutional Language Models\u201d. In: The\nInternational Conference on Machine Learning (ICML) . 2023.\n[80] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and\nYiran Zhong. \u201cToeplitz Neural Network for Sequence Modeling\u201d. In: The International Conference on Learning\nRepresentations (ICLR) . 2023.\n[81] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. \u201cThe devil\nin linear transformer\u201d. In: arXiv preprint arXiv:2210.10340 (2022).\n[82] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and\nYiran Zhong. \u201cCosFormer: Rethinking Softmax in Attention\u201d. In: The International Conference on Learning\nRepresentations (ICLR) . 2022.\n[83] Ali Rahimi and Benjamin Recht. \u201cRandom features for large-scale kernel machines\u201d. In: Advances in neural\ninformation processing systems 20 (2007).\n[84] Prajit Ramachandran, Barret Zoph, and Quoc V Le. \u201cSwish: A Self-gated Activation Function\u201d. In: arXiv preprint\narXiv:1710.05941 7.1 (2017), p. 5.\n[85] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. \u201cCKConv: Con-\ntinuous Kernel Convolution For Sequential Data\u201d. In: arXiv preprint arXiv:2102.02611 (2021).\n[86] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. \u201cWinogrande: An Adversarial Wino-\ngrad Schema Challenge at Scale\u201d. In: Communications of the ACM 64.9 (2021), pp. 99\u2013106.\n[87] George Saon, Ankit Gupta, and Xiaodong Cui. \u201cDiagonal State Space Augmented Transformers for Speech\nRecognition\u201d. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE. 2023, pp. 1\u20135.", "start_char_idx": 0, "end_char_idx": 3101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a93f74f9-f1b2-419d-bb76-e17f653e5fb0": {"__data__": {"id_": "a93f74f9-f1b2-419d-bb76-e17f653e5fb0", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8d930cb-81aa-40a0-97d8-b10fb05382dd", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "d98764ebc3902ba8c98b085d31231c762724b08535b175521048ba220ea58249", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f480892-3733-4265-ae7a-752ecc5f7335", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "0c32737bd429f86c8396021b9f01ac2e47ede080d673fcbb1e3d3239c2b1d19e", "class_name": "RelatedNodeInfo"}}, "text": "\u201cCKConv: Con-\ntinuous Kernel Convolution For Sequential Data\u201d. In: arXiv preprint arXiv:2102.02611 (2021).\n[86] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. \u201cWinogrande: An Adversarial Wino-\ngrad Schema Challenge at Scale\u201d. In: Communications of the ACM 64.9 (2021), pp. 99\u2013106.\n[87] George Saon, Ankit Gupta, and Xiaodong Cui. \u201cDiagonal State Space Augmented Transformers for Speech\nRecognition\u201d. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE. 2023, pp. 1\u20135.\n[88] Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. \u201cLinear Transformers are Secretly Fast Weight Program-\nmers\u201d. In: The International Conference on Machine Learning (ICML) . PMLR. 2021, pp. 9355\u20139366.\n[89] Noam Shazeer. \u201cGLU Variants Improve Transformer\u201d. In: arXiv preprint arXiv:2002.05202 (2020).\n[90] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and\nDenny Zhou. \u201cLarge Language Models can be Easily Distracted by Irrelevant Context\u201d. In: The International\nConference on Machine Learning (ICML) . PMLR. 2023, pp. 31210\u201331227.\n[91] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. \u201cSequence Modeling with Multiresolution Convolutional Mem-\nory\u201d. In: The International Conference on Machine Learning (ICML) . PMLR. 2023, pp. 31312\u201331327.\n[92] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. \u201cSimpli/f_ied State Space Layers for Sequence\nModeling\u201d. In: The International Conference on Learning Representations (ICLR) . 2023.\n[93] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. \u201cRoformer: Enhanced Trans-\nformer with Rotary Position Embedding\u201d. In: arXiv preprint arXiv:2104.09864 (2021).\n[94] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.\n\u201cRetentive network: A successor to transformer for large language models\u201d. In: arXiv preprint arXiv:2307.08621\n(2023).\n[95] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. \u201cSequence to Sequence Learning with Neural Networks\u201d. In:\nAdvances in Neural Information Processing Systems (NeurIPS) 27 (2014).\n22", "start_char_idx": 2552, "end_char_idx": 4690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0479d186-4c78-4e64-8b51-f370ade21803": {"__data__": {"id_": "0479d186-4c78-4e64-8b51-f370ade21803", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2af4c573-5453-4b45-bb77-cdc97e693bf0", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "4fa316fb53e2502c1903b1bf356248134db4e307d7e1dcd2cd1e68a76c1bdb15", "class_name": "RelatedNodeInfo"}}, "text": "[96] Corentin Tallec and Yann Ollivier. \u201cCan Recurrent Neural Networks Warp Time?\u201d In: The International Con-\nference on Learning Representations (ICLR) . 2018.\n[97] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Se-\nbastian Ruder, and Donald Metzler. \u201cLong Range Arena: A Benchmark for E\ufb03cient Transformers\u201d. In: Inter-\nnational Conference on Learning Representations (ICLR) . 2021.\n[98] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. \u201cE\ufb03cient Transformers: A Survey\u201d. In: ACM Com-\nputing Surveys 55.6 (2022), pp. 1\u201328.\n[99] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Bap-\ntiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. \u201cLlama: Open and E\ufb03cient Foundation Language\nModels\u201d. In: arXiv preprint arXiv:2302.13971 (2023).\n[100] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. \u201cAttention Is All You Need\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) .\n2017.\n[101] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. \u201cOn Orthogonality and Learning Recur-\nrent Networks with Long Term Dependencies\u201d. In: International Conference on Machine Learning . PMLR. 2017,\npp. 3570\u20133578.\n[102] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Ra\ufb00ay Hamid. \u201cSelective\nStructured State-Spaces for Long-form Video Understanding\u201d. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 2023, pp. 6387\u20136397.\n[103] Pete Warden. \u201cSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition\u201d. In: ArXiv abs/1804.03209\n(2018).\n[104] Samuel Williams, Andrew Waterman, and David Patterson. \u201cRoo/f_line: An Insightful Visual Performance Model\nfor Multicore Architectures\u201d. In: Communications of the ACM 52.4 (2009), pp. 65\u201376.\n[105] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. \u201cCondConv: Conditionally Parameterized Con-\nvolutions for E\ufb03cient Inference\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 32 (2019).\n[106] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. \u201cHellaSwag: Can a Machine Really\nFinish Your Sentence?\u201d In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-\ntics. 2019.\n[107] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind.\n\u201cAn Attention Free Transformer\u201d. In: arXiv preprint arXiv:2105.14103 (2021).\n[108] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. \u201cE\ufb00ectively Modeling\nTime Series with Simple Discrete State Spaces\u201d. In: The International Conference on Learning Representations\n(ICLR) . 2023.\n[109] Lin Zheng, Chong Wang, and Lingpeng Kong. \u201cLinear complexity randomized self-attention mechanism\u201d. In:\nInternational Conference on Machine Learning . PMLR. 2022, pp. 27011\u201327041.\n[110] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. \u201cE\ufb03cient\nLong Sequence Modeling via State Space Augmented Transformer\u201d. In: arXiv preprint arXiv:2212.08136 (2022).\n23", "start_char_idx": 0, "end_char_idx": 3204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2698f872-4a01-4db2-ac79-f5b203af1cda": {"__data__": {"id_": "2698f872-4a01-4db2-ac79-f5b203af1cda", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9bca7bcf-f0f9-4180-9d3d-826191e75964", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "93f65b73d0d498ee9f9446fce886daf936e492c9a9aa33a3c2592aeb950149d6", "class_name": "RelatedNodeInfo"}}, "text": "A Discussion: Selection Mechanism\nOur selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence.\nIt can also be viewed as related to \u201cfast weights\u201d (J. Ba et al. 2016), which connects classical RNNs with the\nmechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct\nconcept that is worth clarifying.\nGating. Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and\nSchmidhuber 1997) and GRU (J. Chung et al. 2014), or the gated equation (5)n Theorem 1. This was interpreted\nas a particular mechanism for controlling whether to let an input into the hidden state of an RNN. In particular,\nthis a\ufb00ects the propagation of signal through time and causes inputs to interact along the sequence length\ndimension.\nHowever, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative\ninteraction (often with an activation function). For example, elementwise multiplicative components of neural\nnetwork architectures (that do not interact along sequence length) are now commonly referred to as gated\narchitectures (Hua et al. 2022; Mehta et al. 2023), despite a very di\ufb00erent meaning than the original RNN sense.\nThus we believe the original concept of RNN gating versus the popular usage of multiplicative gating actually\nhave a very di\ufb00erent semantic meaning.\nHypernetworks. Hypernetworks refer to neural networks whose parameters are themselves generated by smaller\nneural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to de\ufb01ne a large\nRNN whose recurrent parameters are generated by a smaller RNN.\nData-dependence. Similar to hypernetworks, data-dependence can refer to any notion where some parameters\nof the model depend on the data (Poli et al. 2023).\nExample: GLU Activation. To illustrate the issues with these concepts, consider a simple diagonal linear\nlayer /u1D466=Dx, where Dis a diagonal weight parameter. Now suppose that Dis itself generated from a linear\ntransformation of x, with an optional nonlinearity: D=/u1D70E.Wx). Since it is diagonal, the multiplication becomes\nan elementwise product: /u1D466=/u1D70E.Wx)/uni25E6x.\nThis is a rather trivial transformation, yet it technically satis\ufb01es the common meanings of gating (since it has a\nmultiplicative \u201cbranch\u201d), hypernetworks (since the parameter Dis generated by another layer), and data-dependent\n(sinceDdepends on the data x). However, this in fact simply de\ufb01nes a GLU function, which is so simple that\nit is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful\nlayer.\nSelection. Thus, while selection mechanisms could be considered a special case of ideas such as architectural\ngating, hypernetworks, or data-dependence, so can an enormous range of other constructions\u2014essentially anything\nwith a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al.\n2017) as well\u2014and we \ufb01nd it uninformative to think of them as such.\nInstead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case\n(Theorem 1) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization\nof\u2206(Funahashi and Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term\n\u201cgating\u201d in favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to\nthe mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence\nlength (Section 3.1). Beyond selective SSMs and gated RNNs, other examples may include input-dependent\nconvolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and Guo 2020; Lutati, Zimerman, and Wolf\n2023; Yang et al. 2019) and even attention.\n24", "start_char_idx": 0, "end_char_idx": 3919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f8220d4a-b1c5-4c6c-92ed-de1d866d51e4": {"__data__": {"id_": "f8220d4a-b1c5-4c6c-92ed-de1d866d51e4", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c414de5c-48e5-4267-8e68-0b73cd20e3aa", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "a5cef7ea30bdd2158e1722db80e28c26eff4f686608005983127c20964882b77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b387f16-fb08-4562-97ab-3a7c78b9e595", "node_type": "1", "metadata": {}, "hash": "3e9a78ffd30df58231779347cd39a3451ba541367d43303b171168c4c463bf38", "class_name": "RelatedNodeInfo"}}, "text": "B Related Work\nWe overview several prior works related to our methods. We mention that some of the most closely related models\ninclude recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet,\nand RWKV.\nB.1 S4 Variants and Derivatives\nWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to\nour method.\n\u2022S4 (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) introduced the \ufb01rst structured SSM, describing\ndiagonal structure and diagonal plus low-rank (DPLR). It focused on e\ufb03cient convolutional algorithms for\nDPLR SSMs due to a connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020).\n\u2022DSS (Gupta, Gu, and Berant 2022) \ufb01rst discovered the empirical e\ufb00ectiveness of diagonal structured SSMs by\napproximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022).\n\u2022S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and\nis the \ufb01rst S4 model to be computed recurrently with the parallel scan. However, this required lowering the\ne\ufb00ective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input\nsingle-output) to MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but di\ufb00ers\nby (i) keeping the SISO dimensions, which provides a larger e\ufb00ective recurrent state, (ii) using a hardware-aware\nalgorithm to overcome the computation issue, (iii) adding the selection mechanism.\nLu et al. ( 2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories.\nTheir mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where Ais\nmanually set to 0, instead of our learnable mechanism that depends on the input. It would be interesting to\napply selective SSMs generically to this setting and probe if the model has learned to automatically reset its\nstate on episode boundaries.\n\u2022Mega (Ma et al. 2023) introduced a simpli\ufb01cation of S4 to be real- instead of complex- valued, giving it an\ninterpretation of being an exponential moving average (EMA). They additionally make an interesting connection\nof the discretization step of SSMs to an EMA damping term. Contrary to \ufb01ndings in the original S4 papers, this\nwas the \ufb01rst model to show that real-valued SSMs are empirically e\ufb00ective in certain settings or when combined\nwith di\ufb00erent architectural components.\n\u2022Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition.\nFrom this perspective it shares similarity to selection mechanisms, although in a limited form which is still\ncomputed convolutionally and close to LTI.\n\u2022SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A.\nWang, and Fox 2023), and Toeplitz Neural Network (Qin, Han, W. Sun, He, et al. 2023) all focus on the\nconvolutional representation of S4 and create global or long convolution kernels with di\ufb00erent parameterizations.\nHowever, these methods cannot do fast autoregressive inference directly.\nNotably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and\nusually strictly LTI (linear time invariant).\nB.2 SSM Architectures\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures\nincorporating one of the previous SSMs as a black box layer.\n\u2022GSS (Mehta et al. 2023) was the \ufb01rst gated neural network architecture incorporating SSMs. It is motivated by\nthe gated attention unit (GAU) of Hua et al. ( 2022) and looks quite similar to our block, except with additional\nprojections.", "start_char_idx": 0, "end_char_idx": 3751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5b387f16-fb08-4562-97ab-3a7c78b9e595": {"__data__": {"id_": "5b387f16-fb08-4562-97ab-3a7c78b9e595", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c414de5c-48e5-4267-8e68-0b73cd20e3aa", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "a5cef7ea30bdd2158e1722db80e28c26eff4f686608005983127c20964882b77", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8220d4a-b1c5-4c6c-92ed-de1d866d51e4", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "a5b73cb7c702d8ad0fae78847bdf43fee208272c73b5495fa1a82db8f96a23e6", "class_name": "RelatedNodeInfo"}}, "text": "2023) all focus on the\nconvolutional representation of S4 and create global or long convolution kernels with di\ufb00erent parameterizations.\nHowever, these methods cannot do fast autoregressive inference directly.\nNotably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and\nusually strictly LTI (linear time invariant).\nB.2 SSM Architectures\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures\nincorporating one of the previous SSMs as a black box layer.\n\u2022GSS (Mehta et al. 2023) was the \ufb01rst gated neural network architecture incorporating SSMs. It is motivated by\nthe gated attention unit (GAU) of Hua et al. ( 2022) and looks quite similar to our block, except with additional\nprojections. Most importantly, its projection contracts the model dimension to reduce the state size of the\nSSM, while ours expands the model dimension in order to increase the state size, based on the motivation in\nSection 3.1.\n25", "start_char_idx": 2955, "end_char_idx": 3970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "887caca0-64e5-470d-9886-dc1752cf86db": {"__data__": {"id_": "887caca0-64e5-470d-9886-dc1752cf86db", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "711a7c3a-394f-45c6-9ba5-2c65df8d710d", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "489e0089c636aa2815e35247c472ee56af98a4c76628be7addaa4bce65a4bf26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74422d86-3795-4868-99e3-f409c0ba8c1f", "node_type": "1", "metadata": {}, "hash": "b82147358e83f4020350c6e621566cded316647244ad29ff74738fa76f815774", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Mega (Ma et al. 2023) combined the EMA simpli\ufb01cation of S4 described above into a hybrid architecture using\nan e\ufb03cient attention approximation.\n\u2022H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020).\nIt is the \ufb01rst to generalize this formulation of linear attention to more general recurrences, which is also the\nbasis of later architectures.\n\u2022Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied\non the input. While sharing the \u201cselection\u201d name, we consider this an architectural modi\ufb01cation that is closer to\narchitectural gating than a selection mechanism (Appendix A). For example, we hypothesize that it would not\nsolve the Selective Copying task because simply masking out the irrelevant inputs does not a\ufb00ect the spacing\nbetween the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the\nnoise tokens are embedded to 0).\n\u2022RetNet (Y. Sun et al. 2023) is also based on Linear Attention and very similar to H3, but reduces the inner S4\nlayer to a special case where the state dimension is /u1D441= 1. Although not framed as such, its recurrence can be\nviewed as a special case of a linear SSM.\nIts primary source of improvement is using a linear attention with large head dimension, which can be viewed as\nanother method to perform input-dependent state expansion. Using a larger head dimension in the context\nof linear attention variants was \ufb01rst done by H3, but not extensively used since this requires a proportional\namount of extra computation. RetNet avoids this with an alternate way to parallelize the computation with a\nvariant of standard multi-head attention instead of convolutions, made feasible by their particular special case\nof SSMs which acts as a simple EMA.\n\u2022RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT\n(attention-free Transformer (S. Zhai et al. 2021)), another variant of linear attention. Its main \u201cWKV\u201d mechanism\ninvolves LTI recurrences and can be seen as the ratio of two SSMs.\nWe also highlight the gated attention unit (GAU) from Hua et al. ( 2022), which was motivated by combining the\nTransformer\u2019s MHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining\nthe H3 and MLP blocks.\nB.3 Relationship to RNNs\nRNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state.\nSeveral older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury\net al. 2016), and simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without\ntime-wise nonlinearities. Because of the connections of gating mechanisms and selection mechanisms, these can be\nviewed as cases of selective SSMs, and are thus more powerful in a sense than the family of LTI structured SSMs\nabove. The main di\ufb00erences are:\n\u2022They do not use state expansion ( /u1D441= 1) or selective B,Cparameters, both of which are important for\nperformance (Section 4.6).\n\u2022They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism +\ndiscretization (Theorem 1). The connections to principled SSM theory provides better parameterizations\nand initializations (Section 3.6).\nAdditionally, older RNNs famously su\ufb00ered from e\ufb03ciency issues and the vanishing gradients problem (Pascanu,\nMikolov, and Bengio 2013), both caused by their sequential nature. The latter could be solved for some of the\nabove RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the former was di\ufb03cult without theory\nlater developed for SSMs. For example, modern structured SSMs di\ufb00er in more careful parameterization of the\nrecurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson, Goel, et al.", "start_char_idx": 0, "end_char_idx": 3888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "74422d86-3795-4868-99e3-f409c0ba8c1f": {"__data__": {"id_": "74422d86-3795-4868-99e3-f409c0ba8c1f", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "711a7c3a-394f-45c6-9ba5-2c65df8d710d", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "489e0089c636aa2815e35247c472ee56af98a4c76628be7addaa4bce65a4bf26", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "887caca0-64e5-470d-9886-dc1752cf86db", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "75858c1fbfb954dea4d509a9aa62475ff9e6dab3a98eca4f713504f587246ee3", "class_name": "RelatedNodeInfo"}}, "text": "The connections to principled SSM theory provides better parameterizations\nand initializations (Section 3.6).\nAdditionally, older RNNs famously su\ufb00ered from e\ufb03ciency issues and the vanishing gradients problem (Pascanu,\nMikolov, and Bengio 2013), both caused by their sequential nature. The latter could be solved for some of the\nabove RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the former was di\ufb03cult without theory\nlater developed for SSMs. For example, modern structured SSMs di\ufb00er in more careful parameterization of the\nrecurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson, Goel, et al. 2021;\nGu, Johnson, Timalsina, et al. 2023)), or direct analysis (Orvieto et al. 2023)).\nWe also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio 2016; Hena\ufb00,\nSzlam, and LeCun 2016; Lezcano-Casado and Mart\u00ednez-Rubio 2019; Mhammedi et al. 2017; Vorontsov et al. 2017)\n26", "start_char_idx": 3234, "end_char_idx": 4194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "108f131a-7f2b-4092-82bf-7e561870a1ce": {"__data__": {"id_": "108f131a-7f2b-4092-82bf-7e561870a1ce", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "610c667e-43cc-4654-9295-89b872a08cfb", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "6a9d06b93841a75fe9191f66b39b9b3303d99ba29c36371a363a3e74ca925062", "class_name": "RelatedNodeInfo"}}, "text": "which are motivated by constraining the Atransition matrix to be orthogonal or unitary, in order to control\nits eigenvalues and prevent the vanishing gradient problem. However, these had other limitations; we believe\nthat these stem from the fact that orthogonal/unitary RNNs are also LTI. For example, they are almost always\nevaluated on the Copying task which they can solve perfectly, but observed to struggle on the Selective Copying\ntask (Jing et al. 2019).\nB.4 Linear Attention\nThe Linear Attention (LA) (Katharopoulos et al. 2020) framework is an important result popularizing kernel\nattention and showing how it relates to recurrent autoregressive models. Many variants have proposed alternative\nkernels and other modi\ufb01cations. Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature\nmap to approximate softmax attention (i.e. the expfeature map) using the random Fourier feature approximation\nof Gaussian kernels (Rahimi and Recht 2007). Performer (Choromanski et al. 2021) \ufb01nds an approximation\nto the exponential kernel involving only positive features, which also allows the softmax normalization term.\nTransNormer (Qin, Han, W. Sun, D. Li, et al. 2022) showed that the LA denominator term can be unstable\nand proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al. 2022) augments RFA with a\ncosine reweighting mechanism that incorporates positional information to emphasize locality. Linear Randomized\nAttention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importance sampling,\nand generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformed\nnumerator).\nAside from kernel attention, many other variants of e\ufb03cient attention exist; the survey Tay, Dehghani, Bahri,\net al. ( 2022) o\ufb00ers an extensive categorization of many of these.\nB.5 Long Context Models\nLong context has become a popular subject, and several recent models have claimed to scale to longer and longer\nsequences. However, these are often from a computational standpoint and have not been extensively validated.\nThese include:\n\u2022Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a\nTransformer backbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization\ntasks; their main result is similar to our Induction Heads extrapolation experiment (Table 2).\n\u2022LongNet (Ding et al. 2023), which claimed to scale to 1B length but only evaluated on length <100/u1D43Efor actual\ntasks.\n\u2022Hyena and HyenaDNA (Nguyen, Poli, et al. 2023; Poli et al. 2023), which claimed to leverage up to 1M context.\nHowever, their experiments trained on proportionally more data at longer contexts, making it hard to conclude\nif quality improvements at 1M context are due to context length or due to more data and computation.\n\u2022Sparse Transformer (Child et al. 2019) showed a proof-of-concept of using a strided sparse attention Transformer\nto model audio waveforms of length 220= 1048576 , although did not discuss performance tradeo\ufb00s when\ncontrolling for computation and model size.\nIn contrast, we believe this work presents one of the \ufb01rst approaches to meaningfully demonstrate increasing\nperformance with longer context.\nC Mechanics of Selective SSMs\nProof of Theorem 1.Consider a selective SSM (Algorithm 2) with /u1D441= 1,A= /uni22121 ,B= 1, /u1D460\u2206=/u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB .x), /u1D70F\u2206=/u1D5CCo/u1D5BF/u1D5CDpl/u1D5CE/u1D5CC .\nThe corresponding continuous-time SSM ( 1) is\n/uni210E./u1D461) = /uni2212 /uni210E./u1D461) +x./u1D461)\nwhich is also called a leaky integrator .\n27", "start_char_idx": 0, "end_char_idx": 3641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "20ef0d0d-4192-48b8-9ea4-5aea529af68b": {"__data__": {"id_": "20ef0d0d-4192-48b8-9ea4-5aea529af68b", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "065300df-b11f-4b7f-a419-2e206d0a664d", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "63e08ccfdaef7de269a8cb445d3b720fcfc27746cff0fc298f0a89ddef651dce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "665848e6-9bdd-4d67-bc2d-ca637060e2b5", "node_type": "1", "metadata": {}, "hash": "fdf9fccaad2bf55194a63217ab17a3fe98d052214ea86e1d004f2dfdb9790bad", "class_name": "RelatedNodeInfo"}}, "text": "The discretization step size is\n\u2206/u1D461=/u1D70F\u2206./u1D5AF/u1D5BA/u1D5CB/u1D5BA/u1D5C6e/u1D5CDe/u1D5CB +/u1D460\u2206.x/u1D461))\n=/u1D5CCo/u1D5BF/u1D5CDpl/u1D5CE/u1D5CC ./u1D5AF/u1D5BA/u1D5CB/u1D5BA/u1D5C6e/u1D5CDe/u1D5CB +/u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB .x/u1D461))\n=/u1D5CCo/u1D5BF/u1D5CDpl/u1D5CE/u1D5CC ./u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB .x/u1D461))\nwhere we observe that the parameter can be viewed as a learnable bias and folded into the linear projection.\nNow applying the zero-order hold (ZOH) discretization formulas:\nA/u1D461= exp.\u2206 A) =1\n1 + exp. /u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB .x/u1D461)=/u1D70E./uni2212/u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB .x/u1D461))\n= 1 /uni2212 /u1D70E./u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB .x/u1D461))\nB/u1D461= .\u2206A)/uni22121.exp.\u2206A) /uni2212I)/uni22C5\u2206B= /uni2212.exp.\u2206 A) /uni2212I) = 1 /uni2212 A\n=/u1D70E./u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB .x/u1D461)).\nThus the /f_inal discrete recurrence ( 2a) is\ng/u1D461=/u1D70E./u1D5AB/u1D5C2/u1D5C7e/u1D5BA/u1D5CB .x/u1D461))\n/uni210E/u1D461= .1 /uni2212 g/u1D461)/uni210E/u1D461/uni22121+g/u1D461x/u1D461\nas desired.\nD Hardware-aware Algorithm For Selective SSMs\nWithout input-dependent selectivity, SSMs can be e\ufb03ciently implemented as a convolution (Dao, Fu, Saab, et al.\n2023; Gu, Goel, and R\u00e9 2022), which leverages the fast Fourier transform (FFT) as primitive. With selectivity,\nSSMs are no-longer equivalent to convolution, but we leverage the parallel associative scan. While SSM scans\nare theoretically e\ufb03cient ( /u1D442./u1D435/u1D43F/u1D437/u1D441 )FLOPs, scaling linear in /u1D43F), training foundation models with selective SSMs\nrequires them to be e\ufb03cient on modern hardware (GPUs) as well. We describe how we use kernel fusion and\nrecomputation to make SSM scan fast and memory-e\ufb03cient. We evaluate the speed of our scan implementation\ncompared to convolution and attention in Section 4.5, showing that it is up to 7 \u00d7times faster than attention at\nsequence length 32K, and is as memory-e\ufb03cient as the best attention implementation (FlashAttention).\nSpeed. On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by\nmemory-bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009).\nThis the case with our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to\nsigni\ufb01cant speedup compared to a standard implementation.", "start_char_idx": 0, "end_char_idx": 2430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "665848e6-9bdd-4d67-bc2d-ca637060e2b5": {"__data__": {"id_": "665848e6-9bdd-4d67-bc2d-ca637060e2b5", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "065300df-b11f-4b7f-a419-2e206d0a664d", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "63e08ccfdaef7de269a8cb445d3b720fcfc27746cff0fc298f0a89ddef651dce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20ef0d0d-4192-48b8-9ea4-5aea529af68b", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ee5bb5bb38ccc287a9f0d2d93fcc05b46e1e3be6d44cfd991f7bc3729563356c", "class_name": "RelatedNodeInfo"}}, "text": "We describe how we use kernel fusion and\nrecomputation to make SSM scan fast and memory-e\ufb03cient. We evaluate the speed of our scan implementation\ncompared to convolution and attention in Section 4.5, showing that it is up to 7 \u00d7times faster than attention at\nsequence length 32K, and is as memory-e\ufb03cient as the best attention implementation (FlashAttention).\nSpeed. On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by\nmemory-bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009).\nThis the case with our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to\nsigni\ufb01cant speedup compared to a standard implementation.\nThe standard way to implement the scan algorithm in Section 3.2is to prepare the scan input A,Bof size\n./u1D435, /u1D43F, /u1D437, /u1D441 )in GPU HBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel\nassociative scan implementation to write the scan output of size ./u1D435, /u1D43F, /u1D437, /u1D441 )to GPU HBM, then multiply that scan\noutput with Cto produce an output of size ./u1D435, /u1D43F, /u1D437 ). However, this requires the number of memory reads/writes\non the order of /u1D442./u1D435/u1D43F/u1D437/u1D441 ). We can instead fuse the discretization step, the scan, and the multiplication with C\ninto one kernel:\n1.We read in /u1D442./u1D435/u1D43F/u1D437 +/u1D437/u1D441)bytes of memory ( \u2206,A,B,C) from slow HBM to fast SRAM.\n2.We discretize to produce A,Bof size ./u1D435, /u1D43F, /u1D437, /u1D441 )in SRAM.\n3.We perform a parallel associative scan, yielding intermediate states of size ./u1D435, /u1D43F, /u1D437, /u1D441 )in SRAM.\n4.We multiply and sum with C, producing outputs of size ./u1D435, /u1D43F, /u1D437 )and write it to HBM.\nThis way, we reduce IOs by a factor of /u1D442./u1D441)(the state dimension), which in practice speeds up the operation by\n20-40 times (Section 4.5).\n28", "start_char_idx": 1692, "end_char_idx": 3664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b9557003-8285-425a-8928-175c7616ebbb": {"__data__": {"id_": "b9557003-8285-425a-8928-175c7616ebbb", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cbd515e-f836-40e8-94c5-76d3bb93e560", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "2f91342d9a52acbf60f4e8ac2bae0917a9526aeb01b8941a472292d6a9361227", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0337ef6d-3da4-489d-af27-dc7f9d3e190d", "node_type": "1", "metadata": {}, "hash": "6337868e2a981bb0ef6629a0b539bdae59211aed764283d9ca93148e468fb79c", "class_name": "RelatedNodeInfo"}}, "text": "Table 11: ( Induction heads .) Models are trained on sequence length 28= 256 , and tested on various sequence lengths of 26= 64\nup to 220= 1048576 .\u0013denotes perfect generalization accuracy, while \u0017denotes out of memory.\nModel Params Test Accuracy (%) at Sequence Length\n26272829210211212213214215216217218219220\nMHA-Abs 137K \u0013 99.6 100.0 58.6 26.6 18.8 9.8 10.9 7.8 \u0017 \u0017 \u0017 \u0017 \u0017 \u0017\nMHA-RoPE 137K \u0013 \u0013 100.0 83.6 31.3 18.4 8.6 9.0 5.5 \u0017 \u0017 \u0017 \u0017 \u0017 \u0017\nMHA-xPos 137K \u0013 \u0013 100.0 99.6 67.6 25.4 7.0 9.0 7.8 \u0017 \u0017 \u0017 \u0017 \u0017 \u0017\nH3 153K \u0013 \u0013 100.0 80.9 39.5 23.8 14.8 8.2 5.9 6.6 8.2 4.7 8.2 6.3 7.4\nHyena 69M<97.7 \u0013 100.0 \u0013 44.1 12.5 6.6 5.1 7.0 5.9 6.6 6.6 5.9 6.3 9.8\nMamba 74K \u0013 \u0013 100.0 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013\n<Most of the parameters are in learnable positional encodings.\nFor sequence length /u1D43Ftoo long where we cannot \ufb01t the sequence in SRAM (which is much smaller than HBM), we\nsplit the sequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate\nscan states, we can continue the scan with the next chunk.\nMemory. We describe how we use the classical technique of recomputation to reduce the total amount of memory\nrequired to train selective SSM layers.\nFrom the way we fuse the forward pass, we do not save the intermediate states of size ./u1D435, /u1D43F, /u1D437, /u1D441 )to avoid memory\nblowup. However, these intermediate states are necessary for the backward pass to compute gradients. We instead\nrecompute those intermediate states in the backward pass. Since the inputs \u2206,A,B,Cand output gradient\nread from HBM to SRAM are of size /u1D442./u1D435/u1D43F/u1D441 +/u1D437/u1D441), and the input gradients are also of size /u1D442./u1D435/u1D43F/u1D441 +/u1D437/u1D441),\nrecomputation avoids the cost of reading /u1D442./u1D435/u1D43F/u1D441/u1D437 )elements from HBM. This means that recomputation of the\nSSM states in the backward pass speeds up the computation compared to storing them and reading them from\nHBM.\nBeyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize\nthe memory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output\nprojection). In particular, we do not save intermediate activations that take a lot of memory but are fast to\nrecompute (e.g. output of activation function or short convolution). As a result, the selective SSM layer has the\nsame memory requirement as an optimized Transformer implementation with FlashAttention. In particular, each\nattention layer (FlashAttention) stores around 12 bytes of activations per token, an each MLP layer stores around\n20 bytes of activations per token, for a total of 32 bytes ((assuming mixed-precision training in FP16 or BF16)).\nEach selective SSM stores around 16 bytes of activations per token. Hence two layers of selective SSMs have\naround the same activation memory as an attention layer and an MLP layer.\nE Experimental Details and Additional Results\nE.1 Synthetic Tasks\nSelective Copying.", "start_char_idx": 0, "end_char_idx": 3003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0337ef6d-3da4-489d-af27-dc7f9d3e190d": {"__data__": {"id_": "0337ef6d-3da4-489d-af27-dc7f9d3e190d", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cbd515e-f836-40e8-94c5-76d3bb93e560", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "2f91342d9a52acbf60f4e8ac2bae0917a9526aeb01b8941a472292d6a9361227", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9557003-8285-425a-8928-175c7616ebbb", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "40487c1fc3e1b843804137477bf5a62e67a1d9eb3c30ab3ab43afbd6212fc3f1", "class_name": "RelatedNodeInfo"}}, "text": "In particular, we do not save intermediate activations that take a lot of memory but are fast to\nrecompute (e.g. output of activation function or short convolution). As a result, the selective SSM layer has the\nsame memory requirement as an optimized Transformer implementation with FlashAttention. In particular, each\nattention layer (FlashAttention) stores around 12 bytes of activations per token, an each MLP layer stores around\n20 bytes of activations per token, for a total of 32 bytes ((assuming mixed-precision training in FP16 or BF16)).\nEach selective SSM stores around 16 bytes of activations per token. Hence two layers of selective SSMs have\naround the same activation memory as an attention layer and an MLP layer.\nE Experimental Details and Additional Results\nE.1 Synthetic Tasks\nSelective Copying. Our setting is on sequences of length 4096, with a vocab size of 16 possible tokens (including\nthe white \u201cnoise\u201d token from Figure 2) and requiring models to memorize 16 \u201cdata\u201d tokens. We use 2 layer models\nwith a model dimension of /u1D437= 64.\nModels are trained for 400K steps at a constant learning rate of 0.0001 with a batch size of 64.\nInduction Heads. Training consists of randomly generating data every step, with a batch size of 8. We choose\nan \u201cepoch\u201d size of 8192 steps, and track the accuracy on \ufb01xed validation sets (also randomly generated) of\neach target sequence length. For the MHA-Abs and Mamba models, results are reported after the 25th epoch\n(8192 \u00d7 25 = 204800 steps). For the MHA-RoPE and MHA-xPos models, results are reported after the 50th epoch\n(8192 \u00d7 50 = 409600 steps). For the LTI H3 and Hyena models, results are reported after the 10th epoch ( 81920\nsteps) because they had converged by then and failed to improve further.\n29", "start_char_idx": 2190, "end_char_idx": 3962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3dd2273a-8fe4-4038-a8ad-de6074197a1d": {"__data__": {"id_": "3dd2273a-8fe4-4038-a8ad-de6074197a1d", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f75b81fb-3f95-460d-a36a-f13a82c35003", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ad76355f4fa88baeea2ede5495e0b9a01ea4f6be09cc0fbdbef5f6a6a76ae4df", "class_name": "RelatedNodeInfo"}}, "text": "Table 12: ( Scaling Law Model Sizes .) Our model sizes and hyperparameters for scaling experiments. (Model dimension and\nnumber of heads applies only to Transformer models.)\nParams /u1D697_l/u1D68A/u1D6A2e/u1D69B/u1D69C /u1D68D _/u1D696o/u1D68Del /u1D697 _/u1D691e/u1D68A/u1D68D/u1D69C //u1D68D_/u1D691e/u1D68A/u1D68D Training steps Learning Rate Batch Size Tokens\n125M 12 768 12 / 64 4800 6e-4 0.5M tokens 2.5B\n350M 24 1024 16 / 64 13500 3e-4 0.5M tokens 7B\n760M 24 1536 16 / 96 29000 2.5e-4 0.5M tokens 15B\n1.3B 24 2048 32 / 64 50000 2e-4 0.5M tokens 26B\nWe use the Adam optimizer with no weight decay. All models are trained at constant learning rates 2e/uni2212 4and\n1e/uni2212 3, and the better results are reported for each model ( 2e/uni2212 4for all models except Mamba). The attention\nand Hyena models did not learn at LR 1e/uni2212 3. H3 learned at both LRs, but interestingly generalized better to\nshorter sequences at the smaller LR of 2e/uni2212 4. Mamba learned at both LRs, but extrapolated better at the larger\nLR of 1e/uni2212 3.\nE.2 Language Modeling\nE.2.1 Scaling Law Details\nAll models were trained on the Pile.\nModel Sizes. Table 12speci\ufb01es the model sizes we use for scaling laws. This is taken directly from the GPT3\nspeci\ufb01cations (Brown et al. 2020), with very minor modi\ufb01cations. First, we changed the batch size of the 1.3B\nmodel from 1M tokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch\nsize. Second, we changed the number of training steps and total tokens to roughly match Chinchilla scaling\nlaws (Ho\ufb00mann et al. 2022), which specify that training tokens should increase proportionally to model size.\nTraining Recipes. All models used the AdamW optimizer with\n\u2022gradient clip value 1.0\n\u2022weight decay 0.1\n\u2022no dropout\n\u2022linear learning rate warmup with cosine decay\nBy default, the peak learning rate is the GPT3 speci\ufb01cation.\nWe give several models an \u201cimproved recipe\u201d, inspired by changes adopted by popular large language models such\nas PaLM (Chowdhery et al. 2023) and LLaMa (Touvron et al. 2023). These include:\n\u2022linear learning rate warmup with cosine decay to 1e/uni2212 5, with a peak value of 5\u00d7the GPT3 value\n\u2022no linear bias terms\n\u2022RMSNorm instead of LayerNorm\n\u2022AdamW hyperparameter /u1D6FD= ..9, .95)(the GPT3 value) instead of the PyTorch default of /u1D6FD= ..9, .999)\nArchitecture and Training Details. Our models are:\n\u2022Transformer: The standard Transformer based on GPT3 (Table 12).\n\u2022Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al.\n2021) and SwiGLU MLP (Shazeer 2020), and the improved training recipe above.\n\u2022Hyena: Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an\nMLP) with standard MLP blocks. The MLP blocks have expansion factor 2instead of 4and the number of\nlayers is correspondingly increased by 1.5\u00d7to preserve parameter count.\n30", "start_char_idx": 0, "end_char_idx": 2936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0bea9ab5-a422-4970-869e-cfa62a8c55d5": {"__data__": {"id_": "0bea9ab5-a422-4970-869e-cfa62a8c55d5", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7647cdb5-7eae-48e8-99c7-54ff2b5a6a83", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "661212973307a8b85941f99a2a7fa3502691beeba644cbb32630f326ce135139", "class_name": "RelatedNodeInfo"}}, "text": "\u2022H3++: The H3 architecture with a few modi\ufb01cations, including (i) using the same \u201cthin\u201d Hyena dimensions\nabove (ii) the improved training recipe above (iii) a linear attention head dimension of 8.\n\u2022RWKV: The default RWKV model from B. Peng et al. ( 2023), including its modi\ufb01ed MLP block. We also used\nas much of its speci\ufb01ed training recipe as possible, such as increasing the learning rates by 2\u00d7or3\u00d7on certain\nparameters.\n\u2022RetNet: The default RetNet model from Y. Sun et al. ( 2023). We also gave it the improved training recipe\nabove.\n\u2022Mamba: The standard Mamba architecture, with the improved training recipe.\nE.2.2 Additional Scaling Law Ablations\nWe perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws\nin Figure 4(Left).\nMamba Architecture: Interleaving Blocks. We test the e\ufb00ect of di\ufb00erent architectural blocks combined with\nthe Mamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with\nan extra /u1D5BCo/u1D5C7/u1D5CF /uni2192 /u1D5B2/u1D5B2/u1D5AC path added. This leads to two natural ablations:\n\u2022What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This\ncan also be interpreted as taking Mamba and removing half of the SSMs.\n\u2022What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted\nas taking a Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the\nMLP blocks.\nFigure 9(Right) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly,\nneither change matters too much. The Mamba-MLP architecture is only slightly worse, and still better than\nall models except Transformer++. The Mamba-MHA architecture is only slightly better, which is somewhat\nsurprising in light of the fact that many recent works have found that combining (LTI) SSMs with Attention can\nlead to substantial improvements (Dao, Fu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta,\nand Cui 2023; Zuo et al. 2022).\nH3 Architecture: Training Recipes. Next we ablate di\ufb00erences between the Hyena and H3++ models, our\nweakest and strongest models outside of Transformer++ and Mamba, particularly to isolate the e\ufb00ect of training\nrecipes.\n\u2022Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).\n\u2022Hyena+: The same architecture but with the improved training recipe described above.\n\u2022H3+: The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution\nkernel.\n\u2022H3++: The same as H3+, but with a linear attention head dimension of 8. This increases computation inside\nthe SSM recurrence but does not increase parameters.\nOur general convention is that \u201cModel+\u201d represents the base model with the improved training recipe, and\n\u201cModel++\u201d also allows for architectural changes.\nFigure 9(Right) shows that\n\u2022A large improvement is achieved by the improved training recipe, which was used for many of the models in the\nmain Figure 4(RetNet, H3++, Transformer++, Mamba).\n\u2022The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with \ufb01ndings throughout this\npaper.\n\u2022The head dimension expansion improves performance, consistent with one of our main themes that expanded\nstate dimension improves performance for SSMs (Section 3).\n31", "start_char_idx": 0, "end_char_idx": 3399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f2fa0547-acda-458d-8062-abd84f97c018": {"__data__": {"id_": "f2fa0547-acda-458d-8062-abd84f97c018", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48570be3-fae9-48da-89ed-78e50dbb36ff", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "8505e29f21683369d4fd4ef2bcedc1dd9cd598701125c2a9bad3c84c8ad53b97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "913bf01b-9a9c-4f64-9283-24fb7ba5d020", "node_type": "1", "metadata": {}, "hash": "9859b6963aaeab0975448bfad1cf69873c6d1f531bc7935ad730f4c8ea6bd08c", "class_name": "RelatedNodeInfo"}}, "text": "/uni00000015/uni00000014/uni00000015/uni0000001d/uni00000015/uni00000014/uni00000016/uni00000014\n/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000057/uni00000004/uni0000000c/uni00000050/uni00000053/uni0000004b/uni00000004/uni00000057/uni00000047/uni00000045/uni00000050/uni00000049/uni0000000d/uni00000015/uni00000014/uni00000015\n/uni0000001b/uni00000082/uni00000015/uni00000014/uni00000014/uni0000001c/uni00000082/uni00000015/uni00000014/uni00000014/uni0000001d/uni00000082/uni00000015/uni00000014/uni00000014/uni00000034/uni00000049/uni00000056/uni00000054/uni00000050/uni00000049/uni0000005c/uni0000004d/uni00000058/uni0000005d/uni00000004/uni0000000c/uni00000050/uni00000053/uni0000004b/uni00000004/uni00000057/uni00000047/uni00000045/uni00000050/uni00000049/uni0000000d\n/uni00000037/uni00000047/uni00000045/uni00000050/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000053/uni00000052/uni00000004/uni00000038/uni0000004c/uni00000", "start_char_idx": 0, "end_char_idx": 1008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "913bf01b-9a9c-4f64-9283-24fb7ba5d020": {"__data__": {"id_": "913bf01b-9a9c-4f64-9283-24fb7ba5d020", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48570be3-fae9-48da-89ed-78e50dbb36ff", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "8505e29f21683369d4fd4ef2bcedc1dd9cd598701125c2a9bad3c84c8ad53b97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2fa0547-acda-458d-8062-abd84f97c018", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "71a148a6c94aac06a318cb19308d6ae461f0e4108833252e72f663dcbff70c8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aaef88a2-0435-4201-a4bc-dcbee4c52c08", "node_type": "1", "metadata": {}, "hash": "a336d5657713729a6821cbd3351ace1e9ac134bdc70f17c543e9ccdac8a610a9", "class_name": "RelatedNodeInfo"}}, "text": "uni00000045/uni00000050/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000053/uni00000052/uni00000004/uni00000038/uni0000004c/uni00000049/uni00000004/uni00000034/uni0000004d/uni00000050/uni00000049/uni00000004/uni0000000c/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000004/uni00000016/uni00000014/uni00000018/uni0000001c/uni0000000d\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000011/uni00000031/uni00000030/uni00000034\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000011/uni00000031/uni0000002c/uni00000025\n/uni00000015/uni00000014/uni00000015/uni0000001d/uni00000015/uni00000014/uni00000016/uni00000014\n/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000057/uni00000004/uni0000000c/uni0000", "start_char_idx": 808, "end_char_idx": 1816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "aaef88a2-0435-4201-a4bc-dcbee4c52c08": {"__data__": {"id_": "aaef88a2-0435-4201-a4bc-dcbee4c52c08", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48570be3-fae9-48da-89ed-78e50dbb36ff", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "8505e29f21683369d4fd4ef2bcedc1dd9cd598701125c2a9bad3c84c8ad53b97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "913bf01b-9a9c-4f64-9283-24fb7ba5d020", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "5400367225c34be45804b53e2a2dcee61bff96bf669edc9fd617bd9f1bf22598", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b50f0c47-443b-483f-bf88-ca52be52e591", "node_type": "1", "metadata": {}, "hash": "991ada04d77ef61655af9ae0420a781e847f7991492f579e1484fdb37ca5ccee", "class_name": "RelatedNodeInfo"}}, "text": "ni00000025\n/uni00000015/uni00000014/uni00000015/uni0000001d/uni00000015/uni00000014/uni00000016/uni00000014\n/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000057/uni00000004/uni0000000c/uni00000050/uni00000053/uni0000004b/uni00000004/uni00000057/uni00000047/uni00000045/uni00000050/uni00000049/uni0000000d/uni00000015/uni00000014/uni00000015/uni00000034/uni00000049/uni00000056/uni00000054/uni00000050/uni00000049/uni0000005c/uni0000004d/uni00000058/uni0000005d/uni00000004/uni0000000c/uni00000050/uni00000053/uni0000004b/uni00000004/uni00000057/uni00000047/uni00000045/uni00000050/uni00000049/uni0000000d\n/uni00000037/uni00000047/uni00000045/uni00000050/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000053/uni00000052/uni00000004/uni00000038/uni0000004c/uni00000049/uni00000004/uni00000034/uni0000004d/uni00000050/uni00000049/uni00000004/uni0000000c/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni0000004", "start_char_idx": 1616, "end_char_idx": 2624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b50f0c47-443b-483f-bf88-ca52be52e591": {"__data__": {"id_": "b50f0c47-443b-483f-bf88-ca52be52e591", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48570be3-fae9-48da-89ed-78e50dbb36ff", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "8505e29f21683369d4fd4ef2bcedc1dd9cd598701125c2a9bad3c84c8ad53b97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aaef88a2-0435-4201-a4bc-dcbee4c52c08", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "b292126e4ad66d522f4a4862978d44801f78937da64f114371b8b4a3488305f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4212a501-bb7b-47e1-a202-a3ad3e5a1585", "node_type": "1", "metadata": {}, "hash": "cbe3bd2786cdcac70efbe71752c41148bd00147901b0ff6b999b2d6c2f0acd4f", "class_name": "RelatedNodeInfo"}}, "text": "i00000038/uni0000004c/uni00000049/uni00000004/uni00000034/uni0000004d/uni00000050/uni00000049/uni00000004/uni0000000c/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000004/uni00000016/uni00000014/uni00000018/uni0000001c/uni0000000d\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045/uni0000000f\n/uni0000002c/uni00000017/uni0000000f\n/uni0000002c/uni00000017/uni0000000f/uni0000000fFigure 9: ( Scaling laws: extra ablations .) (Left) Instead of ( Right ) Instead of\nE.2.3 Downstream Evaluation Details\nThis pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens. For the 1.3B\nmodel, we use a batch size of 1M tokens to be consistent with the GPT3 speci\ufb01cations. We report the perplexity\non the Pile validation set, and for this metric only compare to models trained on the same dataset and with the\nsame tokenizer, in particular Pythia and RWKV.\nFor downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al. 2021),\nas done by most work in this area. We evaluate on the following tasks/datasets that measure common sense\nreasoning:\n\u2022LAMBADA (Paperno et al. 2016).\n\u2022HellaSwag (Zellers et al. 2019).\n\u2022PIQA (Bisk et al. 2020).\n\u2022ARC-challenge (P. Clark et al. 2018).\n\u2022ARC-easy: an easy subset of ARC-challenge.\n\u2022WinoGrande (Sakaguchi et al. 2021).\nWe report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence\nlength for HellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these\ntask).\nE.3 DNA Modeling\nE.3.1 Pretraining Details\nWe describe the dataset and training procedure of the HG38 pretraining task in more detail.\nThe dataset follows the splits from the prior Enformer work on genomics (Avsec et al.", "start_char_idx": 2424, "end_char_idx": 4380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4212a501-bb7b-47e1-a202-a3ad3e5a1585": {"__data__": {"id_": "4212a501-bb7b-47e1-a202-a3ad3e5a1585", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48570be3-fae9-48da-89ed-78e50dbb36ff", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "8505e29f21683369d4fd4ef2bcedc1dd9cd598701125c2a9bad3c84c8ad53b97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b50f0c47-443b-483f-bf88-ca52be52e591", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "150f0521cdbf469ab286b2146ca85b8f05dc409af583d0bd3e80cdcf7fec60b8", "class_name": "RelatedNodeInfo"}}, "text": "2016).\n\u2022HellaSwag (Zellers et al. 2019).\n\u2022PIQA (Bisk et al. 2020).\n\u2022ARC-challenge (P. Clark et al. 2018).\n\u2022ARC-easy: an easy subset of ARC-challenge.\n\u2022WinoGrande (Sakaguchi et al. 2021).\nWe report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence\nlength for HellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these\ntask).\nE.3 DNA Modeling\nE.3.1 Pretraining Details\nWe describe the dataset and training procedure of the HG38 pretraining task in more detail.\nThe dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021); the training split\ncontains a total of /u1D446= 34021 segments of length 217= 131072 that cover the genome, for a total of approximately\n4.5 billion tokens (DNA base pairs). These segments are pairs of (chromosome number, starting index, ending\nindex), and can be extended if necessary (e.g. to get longer segments).\nWe deviate from HyenaDNA when the training sequence length is not 217. HyenaDNA always takes a \ufb01xed\nsub-segment (e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length\neach epoch is \ufb01xed to 34021 samples and doesn\u2019t necessarily go through the whole genome. On the other hand, we\nuse the entire training data:\n\u2022When the context length /u1D43Fis less than (or equal to) 217, we divide up each segment into non-overlapping\nsub-segments of length /u1D43F, so that there are /u1D446\u00d7217\n/u1D43Ftotal samples and /u1D446\u00d7 217/uni2248 4.5/u1D435tokens per epoch.\n\u2022When the context length /u1D43Fis greater than 217, we turn each segment into two samples, one that begins with the\nprescribed segment and one that ends with the prescribed segment. Thus each epoch has 2/u1D446items and 2/u1D446/u1D43F\n32", "start_char_idx": 3758, "end_char_idx": 5544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "76089f80-822c-486d-b33f-d24b000c6d95": {"__data__": {"id_": "76089f80-822c-486d-b33f-d24b000c6d95", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4a959c3-b750-470e-81c6-56462b0a507e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "f5577e15c00bc0a93b91e0bb47e830482c3e8c7e290741d2aa62cbcb5a8b2c0e", "class_name": "RelatedNodeInfo"}}, "text": "tokens per epoch. For example, at sequence length 218= 262144 there are 4\u00d7as many tokens as the default,\nand at sequence length 220there are 16\u00d7as many tokens.\nOther training details generally follow the same protocol as our language modeling experiments (Appendix E.2).\nFor example, we use the AdamW with ./u1D6FD1, /u1D6FD2) = .0 .9,0.95), no dropout, weight decay 0.1. We use a cosine learning\nrate scheduler with linear warmup for 10% of total steps.\nE.3.2 Scaling: Model Size Details\nModels. The models we consider are:\n\u2022Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su\net al. 2021). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani\net al. 2017).\n\u2022HyenaDNA: the Hyena model from Nguyen, Poli, et al. ( 2023) and Poli et al. ( 2023), which is roughly a\nTransformer with the MHA block replaced by an H3 block using a global convolution parameterized by an MLP.\n\u2022Mamba: the standard Mamba architecture.\nModel Sizes. We use the following model sizes.\nBlocks 4 5 6 7 8 10 12\nModel Dimension 64 96 128 192 256 384 512\nParams (Approx.) 250K 700K 1.4M 3.5M 7.0M 19.3M 40.7M\nNote that the number of blocks for Mamba is doubled, because one Transformer \u201clayer\u201d includes both the MHA and\nMLP blocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4).\nTraining. For each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across {1e/uni2212 3,2e/uni2212\n3,4e/uni2212 3,8e/uni2212 3}. The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal\nMamba learning rate was 8e-3; note that Mamba performed better than baselines with matched learning rates\n(2e-3), but was more stable and improved even more at higher learning rates. (Furthermore, as this LR is on the\nupper range of the sweep, it is possible that our results are still suboptimal.)\nNote that, in contrast to standard LM scaling laws (Table 12), our LR held constant across model sizes for\nsimplicity. The optimal LR should go down for larger models, but we didn\u2019t \ufb01nd a noticeable e\ufb00ect at the small\nmodel sizes (at most a few million parameters) we considered.\nE.3.3 Scaling: Context Length Details\nWe use a total batch size of 224/uni2248 16/u1D440tokens per training step, for every sequence length (e.g. at length 220\nthere are 16segments per batch and at length 210there are 16384 segments per batch). This is a large batch size\nrelative to the model size by usual LM standards, but note that a batch size of 223is the minimum possible on a\nmachine with 8 GPUs and sequence length of 220, and that HyenaDNA used much larger batches of 228.\nThe learning rate used was 0.008for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same\nlearning rate of 0.002from the previous section for HyenaDNA, but found that it was unstable at the longest\ncontext length.\nSequence Length Warmup. Following (Nguyen, Poli, et al. 2023), we use sequence length warmup (SLW)\nduring pretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from\n210= 1024 . (Note that because of how data is curated, at the longest sequence lengths more steps and tokens are\nspent proportionally. In particular, each stage up to length 217processes the same number of tokens, but 4\u00d7as\nmany tokens are processed at length 218,8\u00d7as many at length 219, and 16\u00d7as many at length 220.)\nUnlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively\nhalved as the sequence lengths are doubled in each stage.\n33", "start_char_idx": 0, "end_char_idx": 3631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c4c07ec7-8fd5-4b20-8076-424428b7fc9d": {"__data__": {"id_": "c4c07ec7-8fd5-4b20-8076-424428b7fc9d", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df06da45-c846-425a-9064-c7ee89125f04", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ef99bf6aeaa9e98b2bcabb7f169860c4e404f06a199530d26d86c712c2a11c40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "427dc512-e4ba-4a5f-9b6b-15376ec8c99a", "node_type": "1", "metadata": {}, "hash": "f2019eafa002570967e5bad32158d178b4984336f625c5bc0dbbdde3833528c2", "class_name": "RelatedNodeInfo"}}, "text": "Table 13: ( Great Apes DNA Classi/f_ication .) Accuracy after /f_ine-tuning on sequences of length 210= 1024 up to 220= 1048576\nusing pretrained models of the same context length. Random guessing is 20%.\nModel Params Accuracy (%) at Sequence Length\n210212214216218220\nHyenaDNA 1.4M 28.04 28.43 41.17 42.22 31.10 54.87\nMamba 1.4M 31.47 27.50 27.66 40.72 42.41 71.67\nMamba 7M 30.00 29.01 31.48 43.73 56.60 81.31\nRemark E.1. We also note that the schedule was not tuned, and we never experimented with turning o\ufb00 sequence length\nwarmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at\nsimilar lengths (Section 4.4), and it is possible that it is not necessary for DNA pretraining either.\nE.3.4 Species (Great Apes) Classi/f_ication\nModels are causal and therefore only the last element (across the sequence length) of the model\u2019s output is used for\nthe classi\ufb01cation head. Note that we control for the total number of elements in the loss function per gradient step.\nThe pretraining objective includes all positions across the sequence length, so that /u1D68B/u1D68A/u1D69D/u1D68C/u1D691 _/u1D69C/u1D692/u1D6A3e \u00d7/u1D69Ce/u1D69A/u1D69Ee/u1D697/u1D68Ce _le/u1D697g/u1D69D/u1D691\nis held constant; in other words, the batch size decreases as the sequence length increases. However, for a\nclassi\ufb01cation task, since only the last position enters the loss, the batch size itself is held constant. Note that this\nalso means that \ufb01ne-tuning models with longer sequence lengths is more computationally expensive.\nTraining consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which\nare all independently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then\nuniformly picking a contiguous segment of DNA.\nFollowing (Nguyen, Poli, et al. 2023), models with a maximum context length greater than 214= 16384 use\nsequence length warmup with 1 epoch at length 214= 16384 , 1 epoch at length 215= 32768 , 1 epoch at length\n216= 65536 , and so on up to the maximum sequence length. For example, the model with 220= 1048576 context\nundergoes 6epochs of sequence length warmup before 4more epochs at its maximum sequence length.\nThe learning rate for all Hyena models is 4e/uni22125, while the learning rate for all Mamba models is 1e/uni22124. These\nwere found by performing learning rate sweeps for each model among {1e/uni2212 5,2e/uni2212 5,4e/uni2212 5,1e/uni2212 4,2e/uni2212 4} for\nthe smaller sequence lengths .210,212,214,216), and these values were consistently found to be the best for each\nmodel. An abridged learning rate sweep was done at length 218, which agreed with these values, and a single run\nat length 220was performed (as described above, the computational cost of these experiments is proportional to\nthe sequence length). The learning rate followed a cosine decay schedule with warmup with 5 epochs of linear\nwarmup to the maximum learning rate, and 5 epochs of cosine decay down to 1e/uni2212 6. The unusually long learning\nrate warmup schedule was chosen because the sequence length warmup was also long (e.g. comprising 6 out of 10\nepochs for the model with context length 220); we did not experiment with this choice.\nResults for the Species classi\ufb01cation task are in Table 13.\nE.4 Audio Details\nE.4.1 YouTubeMix Audio Pretraining\nModel.", "start_char_idx": 0, "end_char_idx": 3386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "427dc512-e4ba-4a5f-9b6b-15376ec8c99a": {"__data__": {"id_": "427dc512-e4ba-4a5f-9b6b-15376ec8c99a", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df06da45-c846-425a-9064-c7ee89125f04", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "ef99bf6aeaa9e98b2bcabb7f169860c4e404f06a199530d26d86c712c2a11c40", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4c07ec7-8fd5-4b20-8076-424428b7fc9d", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "9a11cbd39fb10086ccbb9ec89de0d1b82026bc3655317055df188070bc0c01d2", "class_name": "RelatedNodeInfo"}}, "text": "An abridged learning rate sweep was done at length 218, which agreed with these values, and a single run\nat length 220was performed (as described above, the computational cost of these experiments is proportional to\nthe sequence length). The learning rate followed a cosine decay schedule with warmup with 5 epochs of linear\nwarmup to the maximum learning rate, and 5 epochs of cosine decay down to 1e/uni2212 6. The unusually long learning\nrate warmup schedule was chosen because the sequence length warmup was also long (e.g. comprising 6 out of 10\nepochs for the model with context length 220); we did not experiment with this choice.\nResults for the Species classi\ufb01cation task are in Table 13.\nE.4 Audio Details\nE.4.1 YouTubeMix Audio Pretraining\nModel. We use a model with 3 blocks per stage ( 3 \u00d7 5 = 15 total Mamba blocks), pooling factor p= 16, and\nouter dimension /u1D437= 64, for about 3.5M parameters.\nDataset. The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of\n256.\nThe dataset consists of clips of up to 1 minute long, or length 960000 , which is subsampled and divided into\nsegments of any desired sequence length. Since the architecture involves two stages of pooling by a factor of 16,\n34", "start_char_idx": 2629, "end_char_idx": 3882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "618a4b15-25e3-4b14-a822-0f45295fc14f": {"__data__": {"id_": "618a4b15-25e3-4b14-a822-0f45295fc14f", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca4e743b-0712-40ee-9311-138ca40e566e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c7ef464021fc9b16bb13a15233e723a2728c5c2a5c85c2d06c1daa7fdc63b285", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "194b2c64-b2fc-4631-a2d6-684de9d2fd25", "node_type": "1", "metadata": {}, "hash": "c27b5019cd5f5f1299f7e6dc80e10da15ae668b3a1924c9084046d5d9ec5b8cc", "class_name": "RelatedNodeInfo"}}, "text": "Table 14: YouTubeMix length scaling sequence lengths and batch sizes.\nSequence length Batch size Tokens / batch\n468 \u00d7 2048 = 958464 1 958464\n234 \u00d7 2048 = 479232 2 958464\n117 \u00d7 2048 = 239616 4 958464\n59 \u00d7 2048 = 120832 8 966656\n30 \u00d7 2048 = 61440 16 983040\n15 \u00d7 2048 = 30720 32 983040\n8 \u00d7 2048 = 16384 64 1048576\n4 \u00d7 2048 = 8192 128 1048576\n/uni00000015/uni00000014/uni00000018/uni00000015/uni00000014/uni00000019\n/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000015/uni00000012/uni00000016/uni00000019/uni00000015/uni00000012/uni00000017/uni00000014/uni00000015/uni00000012/uni00000017/uni00000019/uni00000015/uni00000012/uni00000018/uni00000014/uni00000015/uni00000012/uni00000018/uni00000019/uni00000015/uni00000012/uni00000019/uni00000014/uni00000026/uni0000004d/uni00000058/uni00000057/uni00000004/uni00000034/uni00000049/uni00000056/uni00000004/uni00000026/uni0000005d/uni00000058/uni00000049\n/uni00000025/uni00000059/uni00000048/uni0000004d/uni00000053/uni00000004/uni0000003b/uni00000045/uni0000005a/uni00000049/uni0000004a/uni00000053/uni00000056/uni00", "start_char_idx": 0, "end_char_idx": 1199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "194b2c64-b2fc-4631-a2d6-684de9d2fd25": {"__data__": {"id_": "194b2c64-b2fc-4631-a2d6-684de9d2fd25", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca4e743b-0712-40ee-9311-138ca40e566e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c7ef464021fc9b16bb13a15233e723a2728c5c2a5c85c2d06c1daa7fdc63b285", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "618a4b15-25e3-4b14-a822-0f45295fc14f", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "12cd77c0a7234796b93f1f5ae4c20b30ff213f2ee2bb1e3869957e4437cd9402", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bde38b3-b5a3-4c28-9506-0291a6c83c25", "node_type": "1", "metadata": {}, "hash": "f841400c3f94485873b9a88bc580a1a3f711ed044a9d59c17fee9db1b860b570", "class_name": "RelatedNodeInfo"}}, "text": "6/uni0000005d/uni00000058/uni00000049\n/uni00000025/uni00000059/uni00000048/uni0000004d/uni00000053/uni00000004/uni0000003b/uni00000045/uni0000005a/uni00000049/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000057/uni00000004/uni00000011/uni00000004/uni00000037/uni00000037/uni00000031/uni00000004/uni00000034/uni00000045/uni00000056/uni00000045/uni00000051/uni00000049/uni00000058/uni00000049/uni00000056/uni0000004d/uni0000005e/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052\n/uni00000037/uni00000018/uni0000000f/uni00000031/uni00000030/uni00000034\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000004/uni0000000c/uni00000037/uni0000001a/uni0000000d\n/uni0000000f/uni00000047/uni00000053/uni00000051/uni00000054/uni00000050/uni00000049/uni0000005c\n/uni00000011/uni00000057/uni00000049/uni00000050/uni00000049/uni00000047/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004/uni00000026/uni00000013/uni00000027\n/uni00000011/uni00000057/uni00000049/uni00000050/uni0", "start_char_idx": 999, "end_char_idx": 2007, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6bde38b3-b5a3-4c28-9506-0291a6c83c25": {"__data__": {"id_": "6bde38b3-b5a3-4c28-9506-0291a6c83c25", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca4e743b-0712-40ee-9311-138ca40e566e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c7ef464021fc9b16bb13a15233e723a2728c5c2a5c85c2d06c1daa7fdc63b285", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "194b2c64-b2fc-4631-a2d6-684de9d2fd25", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "189e2ef5e92e7fc7b44b2b7b1ca42742b95a3d486e713e7d11544c841a5c05f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10e711de-a2e8-4ef5-9e05-42e9e928f939", "node_type": "1", "metadata": {}, "hash": "2407f44dfb0e7da7d10a5965e995cf1f22c5aa622fddad61c988d3abdfe658ea", "class_name": "RelatedNodeInfo"}}, "text": "57/uni00000049/uni00000050/uni00000049/uni00000047/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004/uni00000026/uni00000013/uni00000027\n/uni00000011/uni00000057/uni00000049/uni00000050/uni00000049/uni00000047/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004\n/uni00000004/uni00000004/uni00000004/uni0000000c/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000011/uni00000037/uni00000018/uni0000000d\n/uni00000015/uni00000014/uni00000018/uni00000015/uni00000014/uni00000019\n/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000015/uni00000012/uni00000016/uni00000019/uni00000015/uni00000012/uni00000017/uni00000014/uni00000015/uni00000012/uni00000017/uni00000019/uni00000015/uni00000012/uni00000018/uni00000014/uni00000015/uni00000012/uni00000018/uni00000019/uni00000026/uni0000004d/uni00000058/uni00000057/uni00000004/uni00000034/uni00", "start_char_idx": 1807, "end_char_idx": 2815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "10e711de-a2e8-4ef5-9e05-42e9e928f939": {"__data__": {"id_": "10e711de-a2e8-4ef5-9e05-42e9e928f939", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca4e743b-0712-40ee-9311-138ca40e566e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c7ef464021fc9b16bb13a15233e723a2728c5c2a5c85c2d06c1daa7fdc63b285", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bde38b3-b5a3-4c28-9506-0291a6c83c25", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "a8501e1deeeb357d385160ada4a0d2f0d0f6483d3b28df655532532548322eae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e80025e3-8ff5-44e2-90cb-969ab25d6499", "node_type": "1", "metadata": {}, "hash": "29d7eb6a5a161a3a400a85bd48f1eede2f5cfefa7a7eab6c8a350fae932087d4", "class_name": "RelatedNodeInfo"}}, "text": "12/uni00000017/uni00000019/uni00000015/uni00000012/uni00000018/uni00000014/uni00000015/uni00000012/uni00000018/uni00000019/uni00000026/uni0000004d/uni00000058/uni00000057/uni00000004/uni00000034/uni00000049/uni00000056/uni00000004/uni00000026/uni0000005d/uni00000058/uni00000049\n/uni00000025/uni00000059/uni00000048/uni0000004d/uni00000053/uni00000004/uni0000003b/uni00000045/uni0000005a/uni00000049/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000057/uni00000004/uni00000011/uni00000004/uni00000037/uni00000037/uni00000031/uni00000004/uni00000034/uni00000045/uni00000056/uni00000045/uni00000051/uni00000049/uni00000058/uni00000049/uni00000056/uni0000004d/uni0000005e/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052\n/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000004/uni0000000c/uni00000037/uni0000001a/uni0000000d\n/uni0000000f/uni00000047/uni00000053/uni00000051/uni00000054/uni00000050/uni00000049/uni0000005c\n/uni00000011/uni00000057/uni00000049/uni00000050/uni00", "start_char_idx": 2615, "end_char_idx": 3623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e80025e3-8ff5-44e2-90cb-969ab25d6499": {"__data__": {"id_": "e80025e3-8ff5-44e2-90cb-969ab25d6499", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca4e743b-0712-40ee-9311-138ca40e566e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c7ef464021fc9b16bb13a15233e723a2728c5c2a5c85c2d06c1daa7fdc63b285", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10e711de-a2e8-4ef5-9e05-42e9e928f939", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "d6b088de7880b52fbc38aebd66ffce719d1257e8c42c06d2b3100b87bf307d80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3855f25f-15dc-4623-b2ad-c023249d84d7", "node_type": "1", "metadata": {}, "hash": "56d27f1fcc466aa0b3af69631f1649a71d22688bd6c61d4ee95ff9b0a69a47a0", "class_name": "RelatedNodeInfo"}}, "text": "/uni0000000c/uni00000037/uni0000001a/uni0000000d\n/uni0000000f/uni00000047/uni00000053/uni00000051/uni00000054/uni00000050/uni00000049/uni0000005c\n/uni00000011/uni00000057/uni00000049/uni00000050/uni00000049/uni00000047/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004/uni00000026/uni00000013/uni00000027\n/uni00000011/uni00000057/uni00000049/uni00000050/uni00000049/uni00000047/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004\n/uni00000004/uni00000004/uni00000004/uni0000000c/uni00000031/uni00000045/uni00000051/uni00000046/uni00000045/uni00000011/uni00000037/uni00000018/uni0000000d\nFigure 10: ( Audio Pretraining (YouTubeMix) Ablations .) As a uniformly-sampled \u201ccontinuous\u201d signal modality, audio wave-\nforms actually bene/f_it from LTI models which have matching inductive bias. ( Left) Homogenous models (all blocks have the same\nparameterization) ( Right ) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as /f_igure\non left.\nand we want the resulting sequence length to be a a multiple of 8for hardware e\ufb03ciency, the longest possible\nsequence is 468 \u00d7 2048 = 958464 . The rest of our sequence lengths are de\ufb01ned by successively halving this and\nrounding up to the nearest multiple of 2048.\nTable 14lists the speci\ufb01cations used in Figure 7. Beyond the varying batch sizes, the number of valid segments in\nthe training set varied between di\ufb00erent sequence lengths (e.g. the number of training steps per epoch was not\nconstant for di\ufb00erent points in the graph), which may have contributed to kinks in the scaling curves.\nTraining. Models were trained for 200/u1D43Etraining steps with a maximum learning rate of 0.002,20/u1D43E(10%)\nwarmup steps, and weight decay 0.1(similar to our general pretraining recipe across domains).\nAdditional Ablations: SSM Parameterizations. We investigate SSM parameterizations on long-form audio\nwaveform pretraining in the setting of Figure 7.", "start_char_idx": 3423, "end_char_idx": 5369, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3855f25f-15dc-4623-b2ad-c023249d84d7": {"__data__": {"id_": "3855f25f-15dc-4623-b2ad-c023249d84d7", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca4e743b-0712-40ee-9311-138ca40e566e", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "c7ef464021fc9b16bb13a15233e723a2728c5c2a5c85c2d06c1daa7fdc63b285", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e80025e3-8ff5-44e2-90cb-969ab25d6499", "node_type": "1", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "4923b9664354a2385a1ce53c649f3681cb2aeb389548cc96c5fcf402d792f6c9", "class_name": "RelatedNodeInfo"}}, "text": "Table 14lists the speci\ufb01cations used in Figure 7. Beyond the varying batch sizes, the number of valid segments in\nthe training set varied between di\ufb00erent sequence lengths (e.g. the number of training steps per epoch was not\nconstant for di\ufb00erent points in the graph), which may have contributed to kinks in the scaling curves.\nTraining. Models were trained for 200/u1D43Etraining steps with a maximum learning rate of 0.002,20/u1D43E(10%)\nwarmup steps, and weight decay 0.1(similar to our general pretraining recipe across domains).\nAdditional Ablations: SSM Parameterizations. We investigate SSM parameterizations on long-form audio\nwaveform pretraining in the setting of Figure 7. The setting is modi\ufb01ed slightly to use larger models ( 8layers and\n/u1D437= 64 for 6M params, the SaShiMi default), shorter sequences ( 211= 2048 to218= 262144 instead of 213to220),\nlower LR ( 0.001from 0.002), and shorter training cycles (100K instead of 200K steps).\nFigure 10shows that the change from S4 /uni2192S6 (i.e. the selection mechanism) is not always bene\ufb01cial. On long-form\naudio waveforms, it in fact signi\ufb01cantly hampers performance, which may be intuitive from the point of view\nthat audio is uniformly sampled and very smooth, and therefore bene\ufb01ts from continuous linear time-invariant\n(LTI) methods. After ablating away the selection mechanism, note that the resulting model is the S4 layer\ninside the Mamba block. To disambiguate, we call this Mamba-S4 as opposed the default Mamba architecture\nMamba-S6.\nHowever, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers.\nThe performance di\ufb00erences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio\nsignal should be LTI, but once they are \u201ctokenized\u201d and compressed by the outer layers, the inner layers no longer\nneed to be LTI. In this setting however, the real-valued SSM still underperforms the complex-valued one.\n35", "start_char_idx": 4686, "end_char_idx": 6642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2bd34ae7-e2c5-4635-b3d1-c1cfe61510be": {"__data__": {"id_": "2bd34ae7-e2c5-4635-b3d1-c1cfe61510be", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c837738-8aa0-4c63-b7f6-c9cfa40a5a63", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "e1202972e033d3be97af98a55e236a892f2d6639d370501c653501e39fc6cef9", "class_name": "RelatedNodeInfo"}}, "text": "E.4.2 SC09 Speech Generation\nAutoregressive training largely followed the autoregressive language modeling protocol, such as\n\u2022Weight decay 0.1\n\u2022Learning rate warmup for 10% of total steps\n\u2022AdamW optimizer with /u1D6FD= .0.9,0.95)\n\u2022Gradient clip value 0.1\nWe used a learning rate of 0.002and200000 training steps at a batch size of 16.\nThe large Mamba model in Table 4has 15 layers per stage with an outer dimension of /u1D437= 96 and pooling factor\n4. We note that this dataset is small (training went through 100 epochs) and for this large model, there was\nsigni\ufb01cant over\ufb01tting of the BPB or NLL. However, automated metrics of generated samples continually improving\nthroughout training.\nThe models in the architecture ablations in Table 5all have 8 layers per stage with an outer dimension of /u1D673= 64\nand pooling factor 4. The S4+MLP block has roughly 2/u1D4372+ 4/u1D4372parameters (expansion factor 2in the MLP).\nThe Transformer block has 4/u1D4372+ 2/u1D4372parameters (expansion factor 1in the MLP). The Mamba block has the\nusual /uni2248 6/u1D4372parameters. All models have roughly 6M total parameters.\nE.5 E\ufb03ciency Benchmark\nScan Operation. We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3),\nagainst convolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost\nof other operations outside of this core operation, such as computing the convolutional kernel in global-convolution\nmodels, or computing the QKV projections in attention.\nAs a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing\nthe parameters A,B,Cin HBM.\nOur scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all\nthe large parameters in HBM.\nFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs\nand the \ufb01lters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The\ntheoretical complexity is /u1D442./u1D43Flog./u1D43F))for sequence length /u1D43F.\nFor attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2023)),\nwith causal mask. Note that FlashAttention-2 with causal mask is about 1.7 \u00d7faster than without causal mask,\nsince approximately only half of the attention entries are computed.\nWe use batch size of 1 and increase the sequence length from 29= 512 ,210/uni2248 1/u1D43E,211/uni2248 2/u1D43E, up to 219/uni2248 500 /u1D43E\n(some of the baselines run out of memory before reaching 500K). We use a model dimension of /u1D437= 1024 and state\ndimension /u1D441= 16. We measure with BF16 inputs, which is the data type most commonly used for large scale\ntraining.\nEnd-to-end Inference. We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba\n6.9B model, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard\nTransformer implementation in the Huggingface transformers library.\nWe set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16,\n32, 64, to 128, and measure time time taken to generate 128 tokens. We then calculate the throughput (tokens/s)\nasbatch size \u00d7 128/uni2215 time taken . We repeat the measurements 3 times and take the average. Measurements are\ndone on an A100 80GB PCIe GPU.\nMemory Benchmark. The memory usage simply scales proportionally to the size of the activation tensors, as\nwith most deep sequence models. We report measurements of the training memory requirements of 125M models\n36", "start_char_idx": 0, "end_char_idx": 3649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "18e9c44f-6e39-435e-bb89-5e044a356c61": {"__data__": {"id_": "18e9c44f-6e39-435e-bb89-5e044a356c61", "embedding": null, "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b044ba5-6d31-492d-850d-08107355bfd3", "node_type": "4", "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}, "hash": "793154d7011865ffdaeef77fd767f49cd0a9e43ee3ea813bd65c6be7d5431140", "class_name": "RelatedNodeInfo"}}, "text": "Table 15: ( Memory benchmark .) Mamba\u2019s memory footprint is comparable to the most optimized Transformer. Results for 125M\nmodels.\nBatch size Transformer (w/ FlashAttention-2) Mamba\n1 4.6GB 4.8GB\n2 5.2GB 5.8GB\n4 6.9GB 7.3GB\n8 11.5GB 12.3GB\n16 20.7GB 23.1GB\n32 34.5GB 38.2GB\non 1 A100 80GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-e\ufb03cient\nTransformer implementation we are aware of (with kernel fusion from torch.compile and with FlashAttention-2).\nTable 15shows that Mamba\u2019s memory requirement is comparable to a similar-sized Transformer with an extremely\noptimized implementation, and we expect further improvement in Mamba\u2019s memory footprint in the future.\n37", "start_char_idx": 0, "end_char_idx": 705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fcf9f991-59bd-445a-b3c4-8937606ef41a": {"__data__": {"id_": "fcf9f991-59bd-445a-b3c4-8937606ef41a", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "29e222db-c5b0-44d1-8709-416fb7974ac9", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "d27172c303978ed221a2820ec07efcd1e5be66488fe567136902f5485dec8b1d", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n1Mamb a\nMamb a , ho w e v er , is one of an al t er nativ e class of models c alled\u00a0 St at e  \nSp ace Models \u00a0( SSMs ). Impor t antl y , f or the first time, Mamb a pr omises  \nsimilar per f or mance (and cruciall y similar\u00a0 scaling laws ) as the T ransf or mer  \nwhilst being f e asible at long sequence lengths (sa y 1 million t ok ens). T o  \nachie v e this long cont e x t, the Mamb a authors r emo v e the \u201c quadratic  \nbot tleneck\u02ee in the A t t ention Mechanism. Mamb a also runs\u00a0 f ast \u00a0- lik e \u201cup t o  \n5x f ast er than T ransf or mer f ast\nSSM \ue1d7 RNN \ue09d GNN\nStructur ed St at e Sp aces: Combining Continuous- Time, R ecur r ent, and  \nCon v olutional Models \u00b7 Hazy R ese ar ch (st anf or d.edu)\nI n t r o d u c t i o n  t o  S t a t e  S p a c e  M o d e l s  \ue081 S S M \ue082\nA Blog post b y Lo\u00efck BOURDOIS on Hugging F ace\nht tps://huggingf ace.co/blog/lbour dois/get -on-the-ss\nm-train\n\ud83d\udc0d\ub9d8\ubc14  \ubc0f  \uc0c1\ud0dc  \uacf5\uac04  \ubaa8\ub378\uc5d0  \ub300\ud55c  \ube44\uc8fc\uc5bc  \uac00\uc774\ub4dc  \n2312. 00 7 52.pdf (ar xiv .or g)", "start_char_idx": 0, "end_char_idx": 967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f2406856-b1b7-4334-a778-7e491082dc15": {"__data__": {"id_": "f2406856-b1b7-4334-a778-7e491082dc15", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2867d599-e4d9-46fd-82ca-49b70e15bf54", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "ee3394e67b1259e2a94b7ae43164835eefbb597ce8e327283672cf12cb16427e", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n2\nS t a t e  S p a c e  M o d e l\n\u2192 del t a, A, B, C 4 \uac1c\uc758  p aramet er \ub97c  2 st ages \uc5d0  \uac78\uccd0  seq2seq T ransf or mation  \n\uc815\uc758\ud558\ub294  \ubaa8\ub378", "start_char_idx": 0, "end_char_idx": 136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7ceeb091-da72-4c0a-b770-ef128067b2ef": {"__data__": {"id_": "7ceeb091-da72-4c0a-b770-ef128067b2ef", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f722f3fc-7882-4527-b93c-28046e7c8db1", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "d7510d8d3598606f150ca95c087488e39f5717035f5d7a306222a1b62c72eb64", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n3\nR ecursiv e \uad00\uc810  \u2192 unbounded cont e x t \uc5d0  \ub300\ud574  \uc0ac\uc6a9  \uac00\ub2a5\n\uc0c1\ud0dc  \uacf5\uac04\uc774\ub780  \ubb34\uc5c7\uc778\uac00 ?\n\uc0c1\ud0dc  \uacf5\uac04\uc740  \uc2dc\uc2a4\ud15c\uc744  \uc644\uc804\ud788  \uc124\uba85\ud558\ub294  \ucd5c\uc18c\ud55c\uc758  \ubcc0\uc218\ub4e4\uc744  \ud3ec\ud568\ud569\ub2c8\ub2e4 . \uc774\ub294  \uc2dc\uc2a4\ud15c\uc758  \n\uac00\ub2a5\ud55c  \uc0c1\ud0dc\ub4e4\uc744  \uc815\uc758\ud568\uc73c\ub85c\uc368  \ubb38\uc81c\ub97c  \uc218\ud559\uc801\uc73c\ub85c  \ud45c\ud604\ud558\ub294  \ubc29\ubc95\uc785\ub2c8\ub2e4 .\n\uc774\ub97c  \uc880  \ub354  \ub2e8\uc21c\ud654\ud574  \ubcf4\uaca0\uc2b5\ub2c8\ub2e4 . \uc6b0\ub9ac\uac00  \ubbf8\ub85c\ub97c  \ud0d0\uc0c9\ud55c\ub2e4\uace0  \uc0c1\uc0c1\ud574  \ubd05\uc2dc\ub2e4 . \" \uc0c1\ud0dc  \uacf5\n\uac04 \"\uc740  \ubaa8\ub4e0  \uac00\ub2a5\ud55c  \uc704\uce58 ( \uc0c1\ud0dc ) \uc758  \uc9c0\ub3c4\uc785\ub2c8\ub2e4 . \uac01  \uc9c0\uc810\uc740  \ubbf8\ub85c\uc5d0\uc11c\uc758  \uace0\uc720\ud55c  \uc704\uce58\ub97c  \ub098\ud0c0", "start_char_idx": 0, "end_char_idx": 306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fc4433fd-b660-41cb-8b83-ea695cab760b": {"__data__": {"id_": "fc4433fd-b660-41cb-8b83-ea695cab760b", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "521916c5-52e9-43e2-bef7-0bab63da516b", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "bbec6889ae35987f5c0cdf34df1a8f1c11187b9098aa6ae95488832b975cabbe", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n4\ub0b4\uba70 , \ucd9c\uad6c\uae4c\uc9c0  \uc5bc\ub9c8\ub098  \ub5a8\uc5b4\uc838  \uc788\ub294\uc9c0\uc640  \uac19\uc740  \uad6c\uccb4\uc801\uc778  \uc138\ubd80  \uc0ac\ud56d\uc744  \ud3ec\ud568\ud569\ub2c8\ub2e4 .\n\"\uc0c1\ud0dc  \uacf5\uac04  \ud45c\ud604 \"\uc740  \uc774  \uc9c0\ub3c4\uc758  \uac04\ub2e8\ud55c  \uc124\uba85\uc785\ub2c8\ub2e4 . \uc5ec\uae30\uc5d0\ub294  \ud604\uc7ac  \uc704\uce58 ( \ud604\uc7ac  \uc0c1\ud0dc ), \ub2e4\uc74c\n\uc5d0  \uac08  \uc218  \uc788\ub294  \uacf3 ( \uac00\ub2a5\ud55c  \ubbf8\ub798  \uc0c1\ud0dc ), \ub2e4\uc74c  \uc0c1\ud0dc\ub85c  \uc774\ub3d9\ud558\ub294  \ubcc0\ud654 ( \uc624\ub978\ucabd\uc774\ub098  \uc67c\ucabd\uc73c\ub85c  \n\uac00\ub294  \uac83 )\uac00  \ud45c\uc2dc\ub429\ub2c8\ub2e4 .", "start_char_idx": 0, "end_char_idx": 212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4edf8599-f82a-4b26-8bbe-edda45a7a815": {"__data__": {"id_": "4edf8599-f82a-4b26-8bbe-edda45a7a815", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebd47b6e-9ff5-455e-a4cd-a899c1f06fed", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "8c89af02ffb596b36bad8aa7c76ef108ea116584b719f2e3c9d25066c7b4d7ad", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n5\n\ue1d7 St at e R epr esent ation \uc744  \uc5c5\ub370\uc774\ud2b8  \ud558\uae30\uc704\ud55c  matr ix A,B, C  \n\u2192del t a\ub294  skip connection \ub9c8\ub0e5  \ud589\ub3d9", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "270c6075-67cd-4045-b11e-7a85af1bb155": {"__data__": {"id_": "270c6075-67cd-4045-b11e-7a85af1bb155", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b871f9a-317f-445e-9e14-1dfdf6c40c5d", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "5bc490e6eba99a1112354ea5af81a05da4add33a518be5cde099263dc79bf285", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n6\nA is the transition st at e matr ix. I t sho ws ho w y ou transition the cur r ent  \nst at e int o the ne x t st at e. I t asks \u201cHo w should I f or get the less r ele v ant  \np ar t s of the st at e o v er time?\u02ee\nB is mapping the ne w input int o the st at e, asking \u201cWhat p ar t of m y ne w  \ninput should I r emember?\u02ee \ue1d7 T ransf or mer \uc758  Quer y matr ix \ub9c8\ub0e5  \uc5ed\ud560\uc744  \ud55c\n\ub2e4 .\nC is mapping the st at e t o the output of the SSM. I t asks, \u201cHo w c an I use  \nthe st at e t o mak e a good ne x t pr ediction?\u02ee \ue1d7 T ransf omer \uc758  Output  \nmatr ix \ub9c8\ub0e5  \uc5ed\ud560\nD is ho w the ne w input p asses thr ough t o the output. I t\u02bc s a kind of  \nmodified skip connection that asks \u201cHo w c an I use the ne w input in m y  \npr ediction?\u02ee", "start_char_idx": 0, "end_char_idx": 721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2c21dca9-aed1-4f3d-a1ac-3d47dff5e313": {"__data__": {"id_": "2c21dca9-aed1-4f3d-a1ac-3d47dff5e313", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24cfa21a-0a01-4a18-9439-46d856e33dcd", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "8d9f3c1984cd73bac480ab19d716a981fe58c6e3ee1dc5bef2efecda7bb61007", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n7\n\u2192 R ecur r ence \uc758  \uad00\uc810\n\uc6b0\ub9ac\uc758  \uc774\uc0b0\ud654\ub41c  SSM \uc740  \uc5f0\uc18d  \uc2e0\ud638  \ub300\uc2e0  \ud2b9\uc815  \uc2dc\uac04  \ub2e8\uacc4\uc5d0\uc11c  \ubb38\uc81c\ub97c  \uacf5\uc2dd\ud654\ud560  \uc218  \uc788\uac8c  \n\ud574\uc90d\ub2c8\ub2e4 . \uc6b0\ub9ac\uac00  \uc774\uc804\uc5d0  RNNs \uc640  \ud568\uaed8  \ubcf8  \uac83\ucc98\ub7fc , \uc5ec\uae30\uc5d0\uc11c  \uc7ac\uadc0\uc801  \uc811\uadfc  \ubc29\uc2dd\uc774  \ub9e4\uc6b0  \n\uc720\uc6a9\ud569\ub2c8\ub2e4 .\n\uc6b0\ub9ac\uac00  \uc5f0\uc18d  \uc2e0\ud638  \ub300\uc2e0  \uc774\uc0b0  \uc2dc\uac04  \ub2e8\uacc4\ub97c  \uace0\ub824\ud55c\ub2e4\uba74 , \uc6b0\ub9ac\ub294  \ubb38\uc81c\ub97c  \uc2dc\uac04  \ub2e8\uacc4\uc640  \ud568\uaed8  \n\uc7ac\uacf5\uc2dd\ud654\ud560  \uc218  \uc788\uc2b5\ub2c8\ub2e4 :", "start_char_idx": 0, "end_char_idx": 245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9c9d765c-3259-4066-a872-2f7513207958": {"__data__": {"id_": "9c9d765c-3259-4066-a872-2f7513207958", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "340e1743-5330-48ed-9ca4-67ed362a809d", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "8b374524c4b6e16ab0ad253cba872ae914a7199311f5000204fdd0a4ce1221e7", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n8", "start_char_idx": 0, "end_char_idx": 10, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3977971e-5c0b-464e-808c-e1c45a702dbd": {"__data__": {"id_": "3977971e-5c0b-464e-808c-e1c45a702dbd", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0d37b14-f0e5-4a09-9cd1-c8286ba4bf8a", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "82bf7edcef88cb2d32e7f8cb4c801ff2ecb1de0f10875633e4892ac696fe38f3", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n9\uac01  \uc2dc\uac04  \ub2e8\uacc4\uc5d0\uc11c , \uc6b0\ub9ac\ub294  \ud604\uc7ac  \uc785\ub825 (Bx\u2096)\uc774  \uc774\uc804  \uc0c1\ud0dc (Ah\u2096\u208b\u2081)\uc5d0  \uc5b4\ub5bb\uac8c  \uc601\ud5a5\uc744  \ubbf8\uce58\ub294\n\uc9c0\ub97c  \uacc4\uc0b0\ud55c  \ub2e4\uc74c  \uc608\uce21\ub41c  \ucd9c\ub825 (Ch\u2096)\uc744  \uacc4\uc0b0\ud569\ub2c8\ub2e4 .\nS4 \ue1d7 S6", "start_char_idx": 0, "end_char_idx": 119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4b3d1fa0-260b-438e-83ac-b1e68b7d090f": {"__data__": {"id_": "4b3d1fa0-260b-438e-83ac-b1e68b7d090f", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1f7f73d-7421-4468-a185-b505f9f09370", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "1de0979e2eabe59f85cce4548b182e09604baaccafb18b3fbae89deef6f6f046", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n10\nCop ying v Selectiv e Cop ying", "start_char_idx": 0, "end_char_idx": 42, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8749c05f-8e10-4b92-9e76-eeb583ba6139": {"__data__": {"id_": "8749c05f-8e10-4b92-9e76-eeb583ba6139", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f52cd5e9-bffd-43d9-a257-add613e6cc0d", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "0932ae25e90c60e8b1c3b42b582336998adac4659e43b9f4287cfa5631cc9489", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n11", "start_char_idx": 0, "end_char_idx": 11, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9f892b0b-30d4-4fe7-908b-34653ca3a238": {"__data__": {"id_": "9f892b0b-30d4-4fe7-908b-34653ca3a238", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2d06dbb-f9c0-449d-9fa4-829c91a1f550", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "165888f32f89d511950c4cf48bf2bc0ca6afaf69dc896bc67a0670dc239b617e", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n12\nMamb a \ue1d7 Selectiv e St at e Sp ace Models Contr ibution\nline ar pr ojection\nselectiv e SSM \uc740  input \ub9c8\ub2e4  B, C matr ix \uac00  \ub2ec\ub77c\uc9c0\uae30\uc5d0  con v \uc801\uc6a9\uc774  \ubd88\uac00\ub2a5\ud574\uc9d0 .\nK er nel F usion \u2192 \nParallel sc an\nR ecomput ation", "start_char_idx": 0, "end_char_idx": 208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7061cb5e-0334-43da-ae0c-8498f205a074": {"__data__": {"id_": "7061cb5e-0334-43da-ae0c-8498f205a074", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0cd9b2f-c39f-428f-b917-7d35df381cdc", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "7085255ce9f6703606c8fea27ae946b70a2bcc6eba93ea8d7f3f6b375bad5138", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n13", "start_char_idx": 0, "end_char_idx": 11, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "32f93012-0c92-4df4-b074-30139b4b38f2": {"__data__": {"id_": "32f93012-0c92-4df4-b074-30139b4b38f2", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab62cc09-c3b8-48eb-8808-47e4b425a70b", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "1547bfe1807f47a8566d843cdc220cbc3c7f26f76e05359665d09c4e9199b9ab", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n14\nMamb a \ub294  \uc5b8\uc5b4\ucc98\ub9ac , \uc720\uc804\uccb4\ud559 , \uc624\ub514\uc624\ubd84\uc11d  \uac19\uc740  \ubcf5\uc7a1\ud55c  \uc2dc\ud000\uc2a4\ub97c  \ub354  \uac00\ubccd\uace0  \ube60\ub974\uac8c  \ucc98\n\ub9ac\ud560  \uc218  \uc788\ub294  \ubaa8\ub378\n\ucd5c\uc2e0  \ud558\ub4dc\uc6e8\uc5b4\uc758  \uc694\uad6c\uc0ac\ud56d\uc5d0  \ub9de\ucdb0  \uba54\ubaa8\ub9ac  \uc0ac\uc6a9\uacfc  \ubcd1\ub82c\ucc98\ub9ac  \uae30\ub2a5\uc744  \ubaa8\ub450  \ucd5c\uc801\ud654\ud568 ,\nend-t o-end NN Ar chit ectur e \nMamb a \ub294  at t ention \uc5c6\uc774  \ud2b8\ub79c\uc2a4\ud3ec\uba38\ubcf4\ub2e4  5 \ubc30  \ube60\ub978  \ucd94\ub860\uacfc  \uc120\ud615  \uc2a4\ucf00\uc77c\ub9c1\nMamb a \uac00  \uc77c\ubc18  \uc2dc\ud000\uc2a4  \ubaa8\ub378  \ubc31\ubcf8\uc774  \ub420  \uc218  \uc788\ub294  \uac15\ub825\ud55c  \ud6c4\ubcf4", "start_char_idx": 0, "end_char_idx": 276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "42fdfacb-558e-405e-8d91-4056051bde4a": {"__data__": {"id_": "42fdfacb-558e-405e-8d91-4056051bde4a", "embedding": null, "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "268b0864-6c3e-4fed-89ce-02ea5c0477f3", "node_type": "4", "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}, "hash": "c6fb7772bc7d32adeeaf1e1acb8dea1c9be3610d5fa0c0897a948f5f5421828c", "class_name": "RelatedNodeInfo"}}, "text": "Untitled\n15\uc624\ud508\uc18c\uc2a4\nMamb a \ub294  \uc544\uc9c1  encoder -decoder \uac1c\ub150\uc774  \uc787\ub0d0\nSelectiv e \uc774\ubbc0\ub85c  \ucd94\ub860\uc5d0  \ud2b9\ud654\ub420  \uc218  \ubc16\uc5d0  \uc5c6\ub2e4 .", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/metadata": {"514e8902-9112-4d6e-815a-9c89ec369adc": {"doc_hash": "4b1a26908a7e5a98f08f1a1eb127222113f4d73d031166a31e9382c8ec7db812", "ref_doc_id": "e601a121-939c-42de-ab20-bd854caf9563"}, "93e1f66e-dd05-46d0-b978-3a3fee314407": {"doc_hash": "b61eb4bc8ae8be31b738e46961a0e1867b3f5453ce233b9f2190b9e2f2bc6d8c", "ref_doc_id": "e601a121-939c-42de-ab20-bd854caf9563"}, "4b5424f0-a83f-4033-8353-04207a8e8b2a": {"doc_hash": "e3f22f1a49d5698c44b86828b2ce447d13983e970bd836889e494c747a05a08b", "ref_doc_id": "e601a121-939c-42de-ab20-bd854caf9563"}, "2c60956e-1965-45c9-a1a8-ea49e2ddcf7f": {"doc_hash": "0b2bdc0b31c077221590ee8f557393e1011a28e2a66ccee90215ac86919aa614", "ref_doc_id": "e601a121-939c-42de-ab20-bd854caf9563"}, "c166faa4-3864-41b1-b200-0ac816dcc8b3": {"doc_hash": "525d0421793d7bb5369fca124a8393d964d6bf3a64f2ad6ecb23e28a435edcc9", "ref_doc_id": "e601a121-939c-42de-ab20-bd854caf9563"}, "e1664d56-4bdd-4636-b9c1-f4e36c5e525b": {"doc_hash": "6041c0654c23f6a4b03cfa61ecc59b7ab2032bfe796986e1843d6add95ca378a", "ref_doc_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a"}, "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a": {"doc_hash": "81487674a2a423858455dd79569ac4e4589d96023535233ad08aef614b272173", "ref_doc_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a"}, "924b1243-7403-4c09-95ee-813cd598c40c": {"doc_hash": "b76b406fc85e128ce7fd832a7541cda985830a04ccfd8053c992b2ab66a19074", "ref_doc_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a"}, "acf5d30f-b0b5-4086-91c7-6e0110bc22a4": {"doc_hash": "d59ba04957ea8b38cc713a28d76d666a85db5aaa829dab61ad3ab5bf51860d6e", "ref_doc_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a"}, "0cc59fcd-290a-4aa9-b3c6-71300a6a5585": {"doc_hash": "c36ce3dec943ce5c8cd4726db497e3781594d915672872375769d97e44d9b3c0", "ref_doc_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a"}, "f7777771-603f-4b46-b825-f9241a116be9": {"doc_hash": "6cc8092656b7f25840cb5304759ed50bcab9623a69858ba69333ca687bb1435e", "ref_doc_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a"}, "61e73cb9-f9b0-4942-b43e-c23414078342": {"doc_hash": "8776ca135f8445608afe6c67c0e09ec4b9d1fc63e094a024a01fe48cc144723d", "ref_doc_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a"}, "964455f0-6c97-46bd-8f85-968255a4761f": {"doc_hash": "43f5fa00bc0d922dfcd518800eb72f51957d085e1498afecafe342781270777c", "ref_doc_id": "1001c839-d0d1-4937-bc6e-6abbbc1b966a"}, "4ec08d13-d259-4320-ac14-c082c54fd81d": {"doc_hash": "9547d067b91240424a7d294c6271490fef500d77333c745dc308b784dec67257", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "46e23f78-fbb3-42c8-805e-ea02f869ba8d": {"doc_hash": "8a0e01e570391253daf319242991ed40984711d710c62cfd068a367b28a30ab5", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad": {"doc_hash": "682bcdf4917f31639571d0ae1e7b1290a2c985bf9286ff077f650eeba9a5de0b", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "f84033da-2946-4cb7-ab76-d68300f1d759": {"doc_hash": "5b53a5de6cbce785ffd25c9e324fed288a8a018e9dc8a40dc3dab30dd6c29289", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8": {"doc_hash": "e5dbcf2efc706601f77baedde81f0557c9fe633d7edeb22782a8c74c2ccd6653", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "0f6d5450-7650-44c1-becc-83c00dc01ce6": {"doc_hash": "da8525b6ac78561799ad83505c625031c0f1db6914a58335b747ba9161f70f01", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4": {"doc_hash": "23b3689a6d8c87d6817e077be4a5fa487c395d21feba17ceb51e9cf3cd7a13f9", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5": {"doc_hash": "6c2d10746f9a65d44c20f924b59571280441e1c5e68b4de89d22c1c9f2bc6839", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "82f37326-55ac-4255-926e-9642e2a69cc4": {"doc_hash": "f296abbc43349ae13ffa65aea1c4bc517af27a8e859151ff96eba860e5f8e785", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "32e6969d-b76c-41b3-a3a7-f7a74f51c11f": {"doc_hash": "6de3820e389ea15927e6ea439482ecf450186b47514b0a84eeaeab7f69a8fc9e", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "22f9084a-e999-4632-adb4-f197179525bf": {"doc_hash": "a3d13b5d2b8df4ad63607862d53404e79e346afbd406e78ac8acf3f401c6166a", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53": {"doc_hash": "15849e3d9f23b0511ee058239851eb29ebf62be07837946412a5f4a4b7a66fdc", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4": {"doc_hash": "81d1c5f77aa206a07b835813dec565a172bd99baad3e3ba910cfd8bf28a53e77", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9": {"doc_hash": "53e212a7a82cb16a3bbe9278d3b3d3c294e85243cd3b72f4b88f90aa346286a2", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "62484605-dde2-4655-90f8-4ed35db10f1b": {"doc_hash": "1f2f99fb17816bd5f9aa694ffd4b79e95714cb71b51d83b8c690c7f9fa854898", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "048a2342-c722-4cdd-a21d-2dc72b6ff821": {"doc_hash": "964ee75cff2d1eb637fa39e53e3423fadb5e3479f2f90f021847031d251a2a85", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "30b8ceed-c561-4ca1-b92c-bb404d6a63fe": {"doc_hash": "0e5ad01e7533878bb1d6d800fb2ef1358d268875f51957430b5c28aab8b8ab91", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "9f61e087-53ee-4370-9d00-16c6584e5618": {"doc_hash": "33a9fa1a2c08da3fb8b34ec5bd20b22e40e1116fb41e69332fe4ae686047382d", "ref_doc_id": "9607249a-1341-472a-81f8-e8c861654e2c"}, "3f13e06e-f447-4e0e-8452-71d61930bb4e": {"doc_hash": "e3133525acaa0c1e827f17f82ef485ca89ca8a15c2bcf3979f6f2bf39d2adfb3", "ref_doc_id": "4267f7ab-2ce7-48a5-9d2f-aeefc6afe469"}, "865ff251-d290-411e-8d16-5b981da7115c": {"doc_hash": "a1ebd2cc6ed1c84595cb3aba4c704940fb9f84f8aeb77ee9794e63fd98948224", "ref_doc_id": "4267f7ab-2ce7-48a5-9d2f-aeefc6afe469"}, "f96fcd1e-edb5-439d-9826-2eb7c0c9ddd4": {"doc_hash": "133af860089e9ef445beb2bc4790d995f882e1ee77ca00bd133912bd48bc36aa", "ref_doc_id": "4267f7ab-2ce7-48a5-9d2f-aeefc6afe469"}, "805a1371-c5b9-4499-a35a-bce0084e3581": {"doc_hash": "aeaa89a40890997accbf0b870b29d2d0563253b94f469985ae3c1a881983bb6d", "ref_doc_id": "4267f7ab-2ce7-48a5-9d2f-aeefc6afe469"}, "ee8e54f9-7071-442a-8c77-854a7ae27118": {"doc_hash": "0a34b2ad0fe5024b7858c67e5cefa0fa033491ba27b4231d6a3274cb6ba8249c", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "0800fd72-b783-4cda-882b-6e66085ad6b9": {"doc_hash": "9050292991656076707f306090efaea4d269837ef376d0789988d18e636617c5", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "8d684716-d0c5-414c-9280-b94a0eb99c1c": {"doc_hash": "708a792c80aa7bb9639bf1497e28a1d6d758adb5217170fe710a6ef92d305af6", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab": {"doc_hash": "7a90ae5f32bc1d6c8483761ece0235a7ab4c26adf699043402cfda7f57d07a08", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "308ad536-8781-4dd3-b5dc-f739aaf6202a": {"doc_hash": "e78067d4343627e698db76548c785be62e7c712e404111a31265bb76c82762b5", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "31d95e9c-1baa-4000-a9f6-123a33a9cfd7": {"doc_hash": "b70d42e20689174e1b483f0216b15b4f80f8b59bd54647ffe3ea52a1831203f4", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "38da4efd-d4bb-4902-82d2-20a078368d8b": {"doc_hash": "c0db6cc47ebcfdd241845f8af97ebe1391daa0afb85843b747fb99690d0aa1ab", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "72b6de12-23a7-461b-b9bc-ad45c10e81da": {"doc_hash": "6d4fed5cb4ab216c3c1f5fdf8911ab40070c7bc2e9758ec6e1812aebcaa11500", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "038cfb51-44d6-4872-953f-c57b4b388f11": {"doc_hash": "ed3e5dea9de293cd49151aaba12cd8e3f99be4ed1e81158600f6319d0022318d", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "06559292-7485-444b-b545-bd2404895948": {"doc_hash": "48e899353ce347ae11297d5317116fdf4518d93fbf56b78296e088dafe25d57d", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "84fbca06-688c-4ca1-9194-17209850a235": {"doc_hash": "e9592decc6d88f1de7814d421a86776db39478fa7f7834614e3a9a0e5a3377f0", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "9a913046-7b31-424a-9254-89d6ebf914e0": {"doc_hash": "4b9e6c79053baf23f662ac8dc41c15ba4b11a8cf0aed15dfc32f4a6fdfcce837", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "e9233990-d9c9-41e0-8cc7-d9d89e29cad2": {"doc_hash": "81017e65075dc008f74cb32ab4e5d2c9de01148fa88916e4fe2e0fb68d06d263", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "74e7593d-86c3-4e71-8614-8b8970cd57ef": {"doc_hash": "2fae57e431c15acd8e7ab5d0bdced3ae818feac099056b06a0b8705ae1a522f9", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "77b729dc-066f-457d-b665-9b982766edeb": {"doc_hash": "178ca7c51bbec269ed0c8f902c35c704c53f7bd1a9f0d19397dc6c3f284abf43", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "574edc77-b679-4f4b-811a-cf5ec3565d88": {"doc_hash": "52279eb9a8c314578720d3dfeeecfffc3aced0c20457a38688b7daf569f21bae", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "c2f69e74-8aad-438f-b1b3-6ec9c307741f": {"doc_hash": "47a82d6e34cccf0c0eb15ff971f0cceee8e91f9eedf34b599e7204eb26297699", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53": {"doc_hash": "8035ac9cc3caf5820a837bee7495e4811c99d963d0c9059abb62fa81b3aa03e0", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "2907aaa9-a729-444e-b31d-012d61232289": {"doc_hash": "212b5242487a43595e227d3ae0fa2260b5e26db59254b782596b4bf49540c6c4", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "e5881d9b-b288-4022-8f13-c4984cba5118": {"doc_hash": "bb91392ba586d17add492e5118e9287aaaf3e81f21916b0d661fe67f27379ac8", "ref_doc_id": "2eeb55d0-ce83-422e-9914-413abb9c9d2d"}, "b0c70401-bfb8-460f-8f12-23f21c5e8a7e": {"doc_hash": "8c92f38ed71552b9e0a7108f6b90a564f13fa65891b3f13a5a60a6b2fe49513d", "ref_doc_id": "e62f7b37-ded4-4d30-a2e7-5574679b2e2c"}, "acae008e-b7f6-41f8-937a-f4314768f9ee": {"doc_hash": "c28880fe837b47c37933987ada5b50e76c2a6b312881ff972caad166df3f64ab", "ref_doc_id": "fb7aec31-7ff3-4868-ba84-4cb19de2258e"}, "e8083740-bf46-4e25-847f-f48a846b583e": {"doc_hash": "d2280e567012b6bf77bae453ed572f8ba747a14f89d2902868012f0335abc873", "ref_doc_id": "fb7aec31-7ff3-4868-ba84-4cb19de2258e"}, "4954475e-ca1f-477c-be65-12867b8b31d1": {"doc_hash": "45200a27b7332971c2d0677d54d7075088fd2d402aad8071521211463edcf5fd", "ref_doc_id": "2c8587f2-e3ce-47e6-be70-740471f08c36"}, "11a8c62a-4d0a-4df1-a783-ac0db5a52475": {"doc_hash": "9c25c46494c01e496b8d67677a9c653515cc6f552f1e6ac9c7169b9d58d562c2", "ref_doc_id": "5feee89b-d75f-4a27-b176-f436afeafc46"}, "c1304b34-ed89-4aee-ad07-2596c341ea9c": {"doc_hash": "cc5ed67f76fc57305cf8a7c0e7f961fe5d02d65b6f2c3dd4f1d8218d49dd3362", "ref_doc_id": "5feee89b-d75f-4a27-b176-f436afeafc46"}, "53585b40-5885-495c-b4d4-48f394faecf3": {"doc_hash": "138d6618aecf4c3e4791c1cd41511237a338cecb8c21ba1f62454ad6ec1882c5", "ref_doc_id": "922c8bcd-01c9-4d72-b78a-c5e4b19dd874"}, "4936d8bd-2b2d-47e2-aeca-f9b2b24f20cc": {"doc_hash": "a379af8fd4083645bcadd3fbe46aaba84deec1bde9d2a4a5ffcfd34fba9de409", "ref_doc_id": "7dc0c5ce-56b6-4f45-ac77-040d89765751"}, "c81d1514-1080-4955-9760-44da8e3e7033": {"doc_hash": "c48cb08d6d17de7ac9c162080d3f97983193719f9436e86e089c7744d041f25c", "ref_doc_id": "7dc0c5ce-56b6-4f45-ac77-040d89765751"}, "254dd1b3-fd70-441e-a921-87375324c580": {"doc_hash": "efaa3394dcc79f4a3468d865e33538ad4b50f9d4822438393ae5491f363047fb", "ref_doc_id": "7dc0c5ce-56b6-4f45-ac77-040d89765751"}, "c4d470f5-ad3d-4fba-9aa6-ac7ab97438d0": {"doc_hash": "34511d24c9534837840155137a4a8b9a5a0bf0abbf9656cc51f3376dffbeb573", "ref_doc_id": "36b68b3c-dbfc-41b8-8155-fb1cbf250620"}, "ff65cb59-cdd1-4749-87a3-d9d4a2921119": {"doc_hash": "74582e96764d6ee5db64680acc5857a073de6bbe3ac829b77d97541d94696b6c", "ref_doc_id": "36b68b3c-dbfc-41b8-8155-fb1cbf250620"}, "0036613c-beae-4296-9338-35ae15a49fd3": {"doc_hash": "61d9222fe0be4a0c2fec0eff0c6c8588faba49bf9df41fb82f879293a7e71ff2", "ref_doc_id": "b7e37a30-e3ac-4a18-8790-e1dc7ee33043"}, "1cc903c2-a6b6-4266-a143-d3f8c3a751ea": {"doc_hash": "b3aaa3063f141093e61ccc4d9746f18e74db609ab841b520afea9d630a6f5a6e", "ref_doc_id": "caa2a543-17b6-4a48-93b2-eb1d82a1bb15"}, "9aa89415-1382-4713-8b0e-af4e38d08b7f": {"doc_hash": "d79587ca7cd529def3c1dea0ea0fbae6fd7e1889851edd4922235954338317fc", "ref_doc_id": "caa2a543-17b6-4a48-93b2-eb1d82a1bb15"}, "35ee119c-8def-4e3d-aaea-ff360494777d": {"doc_hash": "d675b1db7d161584cbd760c3db4dba435f7cbd2d66b77fb48a2632f41fbfbfb4", "ref_doc_id": "f10934d8-1bea-4c8f-80a5-3e7e6f7a6f2b"}, "26c45932-c2b4-455e-99d7-5019546a6a89": {"doc_hash": "6d4c6d2362993f053624d8295b962fbe6ff97063180d5d96dcb0d82a0e883fde", "ref_doc_id": "f10934d8-1bea-4c8f-80a5-3e7e6f7a6f2b"}, "6a3dc7ba-a926-468f-9ce8-0b6f010cc015": {"doc_hash": "0d0695b83408a748269695639f90af0b5f4c1c269c230d7adb25e4900a43818d", "ref_doc_id": "b01a0f79-ddda-4810-baeb-5d5498b98ef1"}, "96855d91-a66d-4e78-82d8-a3dc61909875": {"doc_hash": "54c0651f4347272bcc4699d3c983cb5705eb40caac77c827dd796766e179e539", "ref_doc_id": "b01a0f79-ddda-4810-baeb-5d5498b98ef1"}, "3a5d19e0-c3b6-42d3-83c2-24cba268750a": {"doc_hash": "f7f049b1dc3d2770363ede014c6b9612f7f22e4d46c007dc4139f9139e3fb203", "ref_doc_id": "63daab92-9f2e-424b-bb0e-be5c3074d359"}, "fc8e156f-7bb6-415c-b400-33472b7d1cd1": {"doc_hash": "b78ca4fb5b810b59dfa0a0f1ffb560f88a617b3228bf9b5b176830215789dc16", "ref_doc_id": "63daab92-9f2e-424b-bb0e-be5c3074d359"}, "c88fdb2c-df10-42d3-b887-a7addb382bdd": {"doc_hash": "00964035430237ff5176a35466b40db6f79d79e4de51598cf114ed6a13d159e7", "ref_doc_id": "63daab92-9f2e-424b-bb0e-be5c3074d359"}, "07ea05e1-3399-4103-af08-f3766b6e774e": {"doc_hash": "bd2b6ae6c69d2a9c5b970480831847fdbe12f4a0d1e504d8eceb7f0cc1d0f573", "ref_doc_id": "63daab92-9f2e-424b-bb0e-be5c3074d359"}, "5a609265-cf78-4480-a2b4-724b41e99d68": {"doc_hash": "042bbe4a283499414d6d0e8f904ef5eea94d315cb53bf11115baf56f36b5fe17", "ref_doc_id": "63daab92-9f2e-424b-bb0e-be5c3074d359"}, "e009135f-ba6c-46b3-9799-57b6d4dc80a3": {"doc_hash": "5f503aa16bbde9018407fff48161d2ce41f729ff51ac40cc265804cb60c903b8", "ref_doc_id": "63daab92-9f2e-424b-bb0e-be5c3074d359"}, "8b7319bb-c3a5-47fe-914f-70fe113b4128": {"doc_hash": "81535ed9cb0e26b7ea5793d0825ac05094eec73f59b42fb79a4460b3891d9b1d", "ref_doc_id": "61d76b70-8bd2-44b0-8c12-5b7be023555b"}, "bc3d187c-a058-4e3b-8230-b6bf738e164a": {"doc_hash": "74681ed8c5bdbed74f053505ff542ed06206bc9c79ea0b7702ca2b2f06a79737", "ref_doc_id": "61d76b70-8bd2-44b0-8c12-5b7be023555b"}, "e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad": {"doc_hash": "bf74885f4acfd091c0a77fade4f4e5adf706ec48b994e22096b742c8e2ea59cd", "ref_doc_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9"}, "8f9774e7-a059-48a5-a2c7-e94681a8adfd": {"doc_hash": "ead5e5f6c0917bf955bb76f7472fd6eb1d7a806b8f7d4254d0771dd828f321aa", "ref_doc_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9"}, "08f70598-5610-425e-abd1-a97ebd201ac6": {"doc_hash": "797dc54836dcbb8ebef664847c8d728e30d1853a7edae7bf9ec966ef6443e694", "ref_doc_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9"}, "1f83ff51-d146-49cc-a71f-3dc88db543af": {"doc_hash": "d6baf612f4a6f283edaa45c1f2af50b9fe87e2b9b1f1f14b2cc4f31bace48b94", "ref_doc_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9"}, "dba88b25-1fbf-4bb0-9645-991de23588d8": {"doc_hash": "19977072ca44f5f1162d2f2162e04738ea8cc38c233a55b6f7f871fd01711cc8", "ref_doc_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9"}, "dea454af-e062-43f9-8cdc-a456c19ec801": {"doc_hash": "7adc802021797380d5484733eb020348ce4b4f5749285bba1ace70f05273f3f1", "ref_doc_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9"}, "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c": {"doc_hash": "a6ec581559f5d87ccc33301c6a4af84ce75eec0982f95a7851b3a01ef0bf40ec", "ref_doc_id": "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9"}, "96b7d701-bab3-496e-9983-f91c7c10d99f": {"doc_hash": "e814b33745f281124e0db85d43ce12fa49e57ff02103ee7df2b1810ca90fdcdf", "ref_doc_id": "365a0e8d-416c-4077-931f-4527a21aa813"}, "87c9e0a3-9ea4-4dcc-9b3c-548848424704": {"doc_hash": "070e23b3faebf3ad170c270b7e7e07af2c633b7d174f9cd40df33338e0dcb599", "ref_doc_id": "365a0e8d-416c-4077-931f-4527a21aa813"}, "60f01090-cea4-4e5a-9afd-d0db06795648": {"doc_hash": "96e38b6ca997b6ff459b30f8a86bd5290928966347b4eac27a952889c60bf373", "ref_doc_id": "9ffca80b-def6-4e14-aa7e-c7d6b9fa1147"}, "fbe28570-9918-4321-a3d3-d95aeee6725a": {"doc_hash": "debb35bdeccbc6957ee36c9fc832ad2d8f13bcec663537f7d0407334a2116c27", "ref_doc_id": "9ffca80b-def6-4e14-aa7e-c7d6b9fa1147"}, "17cc3242-19f1-482e-8005-f2eda9ff14bc": {"doc_hash": "02f822cd42f4f3e68a1da5fe0bccc46560ecd00c9a7d9e7d1c1d5fbc725ade6a", "ref_doc_id": "9ffca80b-def6-4e14-aa7e-c7d6b9fa1147"}, "f0621218-24f2-47e6-a2a6-a78f0e52da5a": {"doc_hash": "0e9401b47c7a96fbb84a21d1631aacfb72888a9447c6c61049052bc45934fbef", "ref_doc_id": "9ffca80b-def6-4e14-aa7e-c7d6b9fa1147"}, "ebb21d80-1110-4bd8-bdf3-c3a6af5ac616": {"doc_hash": "421f28e370b94619d2938a6d670d5b04ae356faf15d467cbcf36bd03868db77a", "ref_doc_id": "95533d23-8d21-48de-9019-b7d57264539a"}, "0b2e8f94-c841-4cf2-9857-e4b149c83199": {"doc_hash": "f94fe32eefb99009344579c30a897d953c5b8fad535162c1c15e2e06e78f7e63", "ref_doc_id": "95533d23-8d21-48de-9019-b7d57264539a"}, "17eb10ae-b5fc-4846-9c4f-fffb6a928323": {"doc_hash": "2f759445fb0654e24e28276d99bf368ba602ad6477b88e683b7b90f10bba3a7a", "ref_doc_id": "b0d91f6e-f6d8-4314-b8d5-696e91b90d3a"}, "3e775158-6f90-4613-a73d-0d6a27ea26ec": {"doc_hash": "21dd48dd88c2efcacc1b824469ed040443e68f702a17c2d48536c2a56cb5ebb9", "ref_doc_id": "b0d91f6e-f6d8-4314-b8d5-696e91b90d3a"}, "17b4198b-c2c7-4221-a269-37f0ab636d5d": {"doc_hash": "98153ed7787123baee6320679c4f08aab99d099c3dcb42541e0129a98c982e40", "ref_doc_id": "8144dd54-a80a-4505-a0d8-5bc4cb857c7b"}, "022487f1-5249-4fda-9dfc-9dbd89191093": {"doc_hash": "889ef825055c074c9d6b7b212339415b24d4892a7485a1330a94be0bbe734001", "ref_doc_id": "8144dd54-a80a-4505-a0d8-5bc4cb857c7b"}, "c355c207-99d4-490f-8c8e-5812433f524a": {"doc_hash": "f9f44c2351531f325ff2fcce88c0d20bf8b8d934da66630651bbf49f67e9f368", "ref_doc_id": "c3077941-d825-4144-9a16-4d06e432d7fd"}, "15d2effa-4713-4ebb-841a-4698fb93e294": {"doc_hash": "c2bded3da5c395e23c35b384e26b4259386062fc6b131cc3d0d793714670e801", "ref_doc_id": "c3077941-d825-4144-9a16-4d06e432d7fd"}, "72956801-6df6-4b1e-9be0-a12be8f38a1e": {"doc_hash": "f582d9b851e064ac593eb18bf0f153ee46814ee49c00777faaa77c3209f2af65", "ref_doc_id": "58a7d5dc-a634-407b-9d1d-fec2ee973a1e"}, "1c7e8abe-f3bd-44ec-9aa5-cd6c1aed22d1": {"doc_hash": "1037f9c5da3f4c69745b041cf6cc458bae4f207a55a6f4bbf97f8bf89bb262c7", "ref_doc_id": "58a7d5dc-a634-407b-9d1d-fec2ee973a1e"}, "0f480892-3733-4265-ae7a-752ecc5f7335": {"doc_hash": "0c32737bd429f86c8396021b9f01ac2e47ede080d673fcbb1e3d3239c2b1d19e", "ref_doc_id": "d8d930cb-81aa-40a0-97d8-b10fb05382dd"}, "a93f74f9-f1b2-419d-bb76-e17f653e5fb0": {"doc_hash": "f699ae7d3229a143153dbdafd5a190f4cb448991cc9608b7ee6b6aa4a818f7c9", "ref_doc_id": "d8d930cb-81aa-40a0-97d8-b10fb05382dd"}, "0479d186-4c78-4e64-8b51-f370ade21803": {"doc_hash": "4fa316fb53e2502c1903b1bf356248134db4e307d7e1dcd2cd1e68a76c1bdb15", "ref_doc_id": "2af4c573-5453-4b45-bb77-cdc97e693bf0"}, "2698f872-4a01-4db2-ac79-f5b203af1cda": {"doc_hash": "93f65b73d0d498ee9f9446fce886daf936e492c9a9aa33a3c2592aeb950149d6", "ref_doc_id": "9bca7bcf-f0f9-4180-9d3d-826191e75964"}, "f8220d4a-b1c5-4c6c-92ed-de1d866d51e4": {"doc_hash": "a5b73cb7c702d8ad0fae78847bdf43fee208272c73b5495fa1a82db8f96a23e6", "ref_doc_id": "c414de5c-48e5-4267-8e68-0b73cd20e3aa"}, "5b387f16-fb08-4562-97ab-3a7c78b9e595": {"doc_hash": "d04653886bb5f6afefeddbabfa7b1ed3a1577cc723c5c33d7cccdabc823f17ff", "ref_doc_id": "c414de5c-48e5-4267-8e68-0b73cd20e3aa"}, "887caca0-64e5-470d-9886-dc1752cf86db": {"doc_hash": "75858c1fbfb954dea4d509a9aa62475ff9e6dab3a98eca4f713504f587246ee3", "ref_doc_id": "711a7c3a-394f-45c6-9ba5-2c65df8d710d"}, "74422d86-3795-4868-99e3-f409c0ba8c1f": {"doc_hash": "b79db410a125b2b9b6b024dfdc5c31b938fc11a1fce8a4110d57c8e9a8615220", "ref_doc_id": "711a7c3a-394f-45c6-9ba5-2c65df8d710d"}, "108f131a-7f2b-4092-82bf-7e561870a1ce": {"doc_hash": "6a9d06b93841a75fe9191f66b39b9b3303d99ba29c36371a363a3e74ca925062", "ref_doc_id": "610c667e-43cc-4654-9295-89b872a08cfb"}, "20ef0d0d-4192-48b8-9ea4-5aea529af68b": {"doc_hash": "ee5bb5bb38ccc287a9f0d2d93fcc05b46e1e3be6d44cfd991f7bc3729563356c", "ref_doc_id": "065300df-b11f-4b7f-a419-2e206d0a664d"}, "665848e6-9bdd-4d67-bc2d-ca637060e2b5": {"doc_hash": "9b11ebd28cb8599adf80cfe9a0668308ba240e8858bce15db291304566a4af00", "ref_doc_id": "065300df-b11f-4b7f-a419-2e206d0a664d"}, "b9557003-8285-425a-8928-175c7616ebbb": {"doc_hash": "40487c1fc3e1b843804137477bf5a62e67a1d9eb3c30ab3ab43afbd6212fc3f1", "ref_doc_id": "2cbd515e-f836-40e8-94c5-76d3bb93e560"}, "0337ef6d-3da4-489d-af27-dc7f9d3e190d": {"doc_hash": "1d6eda19a3b582c9e9dd4b9770d1a902749c0c56cf33b1563ce2070591ca3b79", "ref_doc_id": "2cbd515e-f836-40e8-94c5-76d3bb93e560"}, "3dd2273a-8fe4-4038-a8ad-de6074197a1d": {"doc_hash": "ad76355f4fa88baeea2ede5495e0b9a01ea4f6be09cc0fbdbef5f6a6a76ae4df", "ref_doc_id": "f75b81fb-3f95-460d-a36a-f13a82c35003"}, "0bea9ab5-a422-4970-869e-cfa62a8c55d5": {"doc_hash": "661212973307a8b85941f99a2a7fa3502691beeba644cbb32630f326ce135139", "ref_doc_id": "7647cdb5-7eae-48e8-99c7-54ff2b5a6a83"}, "f2fa0547-acda-458d-8062-abd84f97c018": {"doc_hash": "71a148a6c94aac06a318cb19308d6ae461f0e4108833252e72f663dcbff70c8c", "ref_doc_id": "48570be3-fae9-48da-89ed-78e50dbb36ff"}, "913bf01b-9a9c-4f64-9283-24fb7ba5d020": {"doc_hash": "5400367225c34be45804b53e2a2dcee61bff96bf669edc9fd617bd9f1bf22598", "ref_doc_id": "48570be3-fae9-48da-89ed-78e50dbb36ff"}, "aaef88a2-0435-4201-a4bc-dcbee4c52c08": {"doc_hash": "b292126e4ad66d522f4a4862978d44801f78937da64f114371b8b4a3488305f7", "ref_doc_id": "48570be3-fae9-48da-89ed-78e50dbb36ff"}, "b50f0c47-443b-483f-bf88-ca52be52e591": {"doc_hash": "150f0521cdbf469ab286b2146ca85b8f05dc409af583d0bd3e80cdcf7fec60b8", "ref_doc_id": "48570be3-fae9-48da-89ed-78e50dbb36ff"}, "4212a501-bb7b-47e1-a202-a3ad3e5a1585": {"doc_hash": "4c3322e52435b69bdbe501e7c59a04edd7636f1a4e5c68bc583113e251409dfa", "ref_doc_id": "48570be3-fae9-48da-89ed-78e50dbb36ff"}, "76089f80-822c-486d-b33f-d24b000c6d95": {"doc_hash": "f5577e15c00bc0a93b91e0bb47e830482c3e8c7e290741d2aa62cbcb5a8b2c0e", "ref_doc_id": "d4a959c3-b750-470e-81c6-56462b0a507e"}, "c4c07ec7-8fd5-4b20-8076-424428b7fc9d": {"doc_hash": "9a11cbd39fb10086ccbb9ec89de0d1b82026bc3655317055df188070bc0c01d2", "ref_doc_id": "df06da45-c846-425a-9064-c7ee89125f04"}, "427dc512-e4ba-4a5f-9b6b-15376ec8c99a": {"doc_hash": "c363cf45bc85ed0371185adb40a2fd417546532a4ff3193c2c5b0eabf0ea754a", "ref_doc_id": "df06da45-c846-425a-9064-c7ee89125f04"}, "618a4b15-25e3-4b14-a822-0f45295fc14f": {"doc_hash": "12cd77c0a7234796b93f1f5ae4c20b30ff213f2ee2bb1e3869957e4437cd9402", "ref_doc_id": "ca4e743b-0712-40ee-9311-138ca40e566e"}, "194b2c64-b2fc-4631-a2d6-684de9d2fd25": {"doc_hash": "189e2ef5e92e7fc7b44b2b7b1ca42742b95a3d486e713e7d11544c841a5c05f6", "ref_doc_id": "ca4e743b-0712-40ee-9311-138ca40e566e"}, "6bde38b3-b5a3-4c28-9506-0291a6c83c25": {"doc_hash": "a8501e1deeeb357d385160ada4a0d2f0d0f6483d3b28df655532532548322eae", "ref_doc_id": "ca4e743b-0712-40ee-9311-138ca40e566e"}, "10e711de-a2e8-4ef5-9e05-42e9e928f939": {"doc_hash": "d6b088de7880b52fbc38aebd66ffce719d1257e8c42c06d2b3100b87bf307d80", "ref_doc_id": "ca4e743b-0712-40ee-9311-138ca40e566e"}, "e80025e3-8ff5-44e2-90cb-969ab25d6499": {"doc_hash": "4923b9664354a2385a1ce53c649f3681cb2aeb389548cc96c5fcf402d792f6c9", "ref_doc_id": "ca4e743b-0712-40ee-9311-138ca40e566e"}, "3855f25f-15dc-4623-b2ad-c023249d84d7": {"doc_hash": "13139159f9fad383d27a0b99cb7b3c926bfbc959c4b183e8235da2e406cbcca6", "ref_doc_id": "ca4e743b-0712-40ee-9311-138ca40e566e"}, "2bd34ae7-e2c5-4635-b3d1-c1cfe61510be": {"doc_hash": "e1202972e033d3be97af98a55e236a892f2d6639d370501c653501e39fc6cef9", "ref_doc_id": "1c837738-8aa0-4c63-b7f6-c9cfa40a5a63"}, "18e9c44f-6e39-435e-bb89-5e044a356c61": {"doc_hash": "793154d7011865ffdaeef77fd767f49cd0a9e43ee3ea813bd65c6be7d5431140", "ref_doc_id": "9b044ba5-6d31-492d-850d-08107355bfd3"}, "fcf9f991-59bd-445a-b3c4-8937606ef41a": {"doc_hash": "a2ff787b8bf84520aed940f672254402bbb3eed5e1a6c85c19498e9dc3e7b0b2", "ref_doc_id": "29e222db-c5b0-44d1-8709-416fb7974ac9"}, "f2406856-b1b7-4334-a778-7e491082dc15": {"doc_hash": "ee3394e67b1259e2a94b7ae43164835eefbb597ce8e327283672cf12cb16427e", "ref_doc_id": "2867d599-e4d9-46fd-82ca-49b70e15bf54"}, "7ceeb091-da72-4c0a-b770-ef128067b2ef": {"doc_hash": "d7510d8d3598606f150ca95c087488e39f5717035f5d7a306222a1b62c72eb64", "ref_doc_id": "f722f3fc-7882-4527-b93c-28046e7c8db1"}, "fc4433fd-b660-41cb-8b83-ea695cab760b": {"doc_hash": "7c90389fd09418c6c6cc3d3c6fa3233ea7f82ef1f3adc13064cb3d573f917d2a", "ref_doc_id": "521916c5-52e9-43e2-bef7-0bab63da516b"}, "4edf8599-f82a-4b26-8bbe-edda45a7a815": {"doc_hash": "8c89af02ffb596b36bad8aa7c76ef108ea116584b719f2e3c9d25066c7b4d7ad", "ref_doc_id": "ebd47b6e-9ff5-455e-a4cd-a899c1f06fed"}, "270c6075-67cd-4045-b11e-7a85af1bb155": {"doc_hash": "5bc490e6eba99a1112354ea5af81a05da4add33a518be5cde099263dc79bf285", "ref_doc_id": "9b871f9a-317f-445e-9e14-1dfdf6c40c5d"}, "2c21dca9-aed1-4f3d-a1ac-3d47dff5e313": {"doc_hash": "8d9f3c1984cd73bac480ab19d716a981fe58c6e3ee1dc5bef2efecda7bb61007", "ref_doc_id": "24cfa21a-0a01-4a18-9439-46d856e33dcd"}, "9c9d765c-3259-4066-a872-2f7513207958": {"doc_hash": "7579e1246fcfa8b9bf9b834cd11ad054e06e00f7668875459f276cdde6afd0cf", "ref_doc_id": "340e1743-5330-48ed-9ca4-67ed362a809d"}, "3977971e-5c0b-464e-808c-e1c45a702dbd": {"doc_hash": "82bf7edcef88cb2d32e7f8cb4c801ff2ecb1de0f10875633e4892ac696fe38f3", "ref_doc_id": "e0d37b14-f0e5-4a09-9cd1-c8286ba4bf8a"}, "4b3d1fa0-260b-438e-83ac-b1e68b7d090f": {"doc_hash": "1de0979e2eabe59f85cce4548b182e09604baaccafb18b3fbae89deef6f6f046", "ref_doc_id": "c1f7f73d-7421-4468-a185-b505f9f09370"}, "8749c05f-8e10-4b92-9e76-eeb583ba6139": {"doc_hash": "88734a3e5cf65e9784a915971d21693d6238ac1a39646b18bdcf6af8841bb857", "ref_doc_id": "f52cd5e9-bffd-43d9-a257-add613e6cc0d"}, "9f892b0b-30d4-4fe7-908b-34653ca3a238": {"doc_hash": "165888f32f89d511950c4cf48bf2bc0ca6afaf69dc896bc67a0670dc239b617e", "ref_doc_id": "a2d06dbb-f9c0-449d-9fa4-829c91a1f550"}, "7061cb5e-0334-43da-ae0c-8498f205a074": {"doc_hash": "d03173b6f6cc29c73c10d5c426d620c94223c854241da5d99c85d785d3c81661", "ref_doc_id": "e0cd9b2f-c39f-428f-b917-7d35df381cdc"}, "32f93012-0c92-4df4-b074-30139b4b38f2": {"doc_hash": "1547bfe1807f47a8566d843cdc220cbc3c7f26f76e05359665d09c4e9199b9ab", "ref_doc_id": "ab62cc09-c3b8-48eb-8808-47e4b425a70b"}, "42fdfacb-558e-405e-8d91-4056051bde4a": {"doc_hash": "c6fb7772bc7d32adeeaf1e1acb8dea1c9be3610d5fa0c0897a948f5f5421828c", "ref_doc_id": "268b0864-6c3e-4fed-89ce-02ea5c0477f3"}}, "docstore/ref_doc_info": {"e601a121-939c-42de-ab20-bd854caf9563": {"node_ids": ["514e8902-9112-4d6e-815a-9c89ec369adc", "93e1f66e-dd05-46d0-b978-3a3fee314407", "4b5424f0-a83f-4033-8353-04207a8e8b2a", "2c60956e-1965-45c9-a1a8-ea49e2ddcf7f", "c166faa4-3864-41b1-b200-0ac816dcc8b3", "514e8902-9112-4d6e-815a-9c89ec369adc", "93e1f66e-dd05-46d0-b978-3a3fee314407", "4b5424f0-a83f-4033-8353-04207a8e8b2a", "2c60956e-1965-45c9-a1a8-ea49e2ddcf7f", "c166faa4-3864-41b1-b200-0ac816dcc8b3", "514e8902-9112-4d6e-815a-9c89ec369adc", "93e1f66e-dd05-46d0-b978-3a3fee314407", "4b5424f0-a83f-4033-8353-04207a8e8b2a", "2c60956e-1965-45c9-a1a8-ea49e2ddcf7f", "c166faa4-3864-41b1-b200-0ac816dcc8b3", "514e8902-9112-4d6e-815a-9c89ec369adc", "93e1f66e-dd05-46d0-b978-3a3fee314407", "4b5424f0-a83f-4033-8353-04207a8e8b2a", "2c60956e-1965-45c9-a1a8-ea49e2ddcf7f", "c166faa4-3864-41b1-b200-0ac816dcc8b3", "514e8902-9112-4d6e-815a-9c89ec369adc", "93e1f66e-dd05-46d0-b978-3a3fee314407", "4b5424f0-a83f-4033-8353-04207a8e8b2a", "2c60956e-1965-45c9-a1a8-ea49e2ddcf7f", "c166faa4-3864-41b1-b200-0ac816dcc8b3"], "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}}, "1001c839-d0d1-4937-bc6e-6abbbc1b966a": {"node_ids": ["e1664d56-4bdd-4636-b9c1-f4e36c5e525b", "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "924b1243-7403-4c09-95ee-813cd598c40c", "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "f7777771-603f-4b46-b825-f9241a116be9", "61e73cb9-f9b0-4942-b43e-c23414078342", "964455f0-6c97-46bd-8f85-968255a4761f", "e1664d56-4bdd-4636-b9c1-f4e36c5e525b", "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "924b1243-7403-4c09-95ee-813cd598c40c", "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "f7777771-603f-4b46-b825-f9241a116be9", "61e73cb9-f9b0-4942-b43e-c23414078342", "964455f0-6c97-46bd-8f85-968255a4761f", "e1664d56-4bdd-4636-b9c1-f4e36c5e525b", "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "924b1243-7403-4c09-95ee-813cd598c40c", "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "f7777771-603f-4b46-b825-f9241a116be9", "61e73cb9-f9b0-4942-b43e-c23414078342", "964455f0-6c97-46bd-8f85-968255a4761f", "e1664d56-4bdd-4636-b9c1-f4e36c5e525b", "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "924b1243-7403-4c09-95ee-813cd598c40c", "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "f7777771-603f-4b46-b825-f9241a116be9", "61e73cb9-f9b0-4942-b43e-c23414078342", "964455f0-6c97-46bd-8f85-968255a4761f", "e1664d56-4bdd-4636-b9c1-f4e36c5e525b", "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "924b1243-7403-4c09-95ee-813cd598c40c", "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "f7777771-603f-4b46-b825-f9241a116be9", "61e73cb9-f9b0-4942-b43e-c23414078342", "964455f0-6c97-46bd-8f85-968255a4761f", "e1664d56-4bdd-4636-b9c1-f4e36c5e525b", "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "924b1243-7403-4c09-95ee-813cd598c40c", "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "f7777771-603f-4b46-b825-f9241a116be9", "61e73cb9-f9b0-4942-b43e-c23414078342", "964455f0-6c97-46bd-8f85-968255a4761f", "e1664d56-4bdd-4636-b9c1-f4e36c5e525b", "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "924b1243-7403-4c09-95ee-813cd598c40c", "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "f7777771-603f-4b46-b825-f9241a116be9", "61e73cb9-f9b0-4942-b43e-c23414078342", "964455f0-6c97-46bd-8f85-968255a4761f", "e1664d56-4bdd-4636-b9c1-f4e36c5e525b", "f50563c6-f1cc-4cbe-a330-d3d2b0a37d2a", "924b1243-7403-4c09-95ee-813cd598c40c", "acf5d30f-b0b5-4086-91c7-6e0110bc22a4", "0cc59fcd-290a-4aa9-b3c6-71300a6a5585", "f7777771-603f-4b46-b825-f9241a116be9", "61e73cb9-f9b0-4942-b43e-c23414078342", "964455f0-6c97-46bd-8f85-968255a4761f"], "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}}, "9607249a-1341-472a-81f8-e8c861654e2c": {"node_ids": ["4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618", "4ec08d13-d259-4320-ac14-c082c54fd81d", "46e23f78-fbb3-42c8-805e-ea02f869ba8d", "bbd79f65-4ed7-4076-8fd4-de9be1d5b2ad", "f84033da-2946-4cb7-ab76-d68300f1d759", "2e9f21ac-777d-459b-b2a1-dfcc3bb081a8", "0f6d5450-7650-44c1-becc-83c00dc01ce6", "2aa5172c-c83e-4ec3-abd1-96a87b0bf1f4", "31f3e6a0-fff6-4f4a-9e4b-743f18920cc5", "82f37326-55ac-4255-926e-9642e2a69cc4", "32e6969d-b76c-41b3-a3a7-f7a74f51c11f", "22f9084a-e999-4632-adb4-f197179525bf", "9eb40b8d-ede7-4fc9-aee7-df6d33b09b53", "db442cc7-3ef3-4348-8fa3-4169b0e7e1f4", "1813cc8c-da3e-4ed5-ac9e-aa4ddba064b9", "62484605-dde2-4655-90f8-4ed35db10f1b", "048a2342-c722-4cdd-a21d-2dc72b6ff821", "30b8ceed-c561-4ca1-b92c-bb404d6a63fe", "9f61e087-53ee-4370-9d00-16c6584e5618"], "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}}, "4267f7ab-2ce7-48a5-9d2f-aeefc6afe469": {"node_ids": ["3f13e06e-f447-4e0e-8452-71d61930bb4e", "865ff251-d290-411e-8d16-5b981da7115c", "f96fcd1e-edb5-439d-9826-2eb7c0c9ddd4", "805a1371-c5b9-4499-a35a-bce0084e3581", "3f13e06e-f447-4e0e-8452-71d61930bb4e", "865ff251-d290-411e-8d16-5b981da7115c", "f96fcd1e-edb5-439d-9826-2eb7c0c9ddd4", "805a1371-c5b9-4499-a35a-bce0084e3581", "3f13e06e-f447-4e0e-8452-71d61930bb4e", "865ff251-d290-411e-8d16-5b981da7115c", "f96fcd1e-edb5-439d-9826-2eb7c0c9ddd4", "805a1371-c5b9-4499-a35a-bce0084e3581", "3f13e06e-f447-4e0e-8452-71d61930bb4e", "865ff251-d290-411e-8d16-5b981da7115c", "f96fcd1e-edb5-439d-9826-2eb7c0c9ddd4", "805a1371-c5b9-4499-a35a-bce0084e3581"], "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}}, "2eeb55d0-ce83-422e-9914-413abb9c9d2d": {"node_ids": ["ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118", "ee8e54f9-7071-442a-8c77-854a7ae27118", "0800fd72-b783-4cda-882b-6e66085ad6b9", "8d684716-d0c5-414c-9280-b94a0eb99c1c", "56dbfe8e-5c48-42ab-95d9-192c8c5c31ab", "308ad536-8781-4dd3-b5dc-f739aaf6202a", "31d95e9c-1baa-4000-a9f6-123a33a9cfd7", "38da4efd-d4bb-4902-82d2-20a078368d8b", "72b6de12-23a7-461b-b9bc-ad45c10e81da", "038cfb51-44d6-4872-953f-c57b4b388f11", "06559292-7485-444b-b545-bd2404895948", "84fbca06-688c-4ca1-9194-17209850a235", "9a913046-7b31-424a-9254-89d6ebf914e0", "e9233990-d9c9-41e0-8cc7-d9d89e29cad2", "74e7593d-86c3-4e71-8614-8b8970cd57ef", "77b729dc-066f-457d-b665-9b982766edeb", "574edc77-b679-4f4b-811a-cf5ec3565d88", "c2f69e74-8aad-438f-b1b3-6ec9c307741f", "391e7a34-4aa7-49ad-9cfb-00ba1b9e9b53", "2907aaa9-a729-444e-b31d-012d61232289", "e5881d9b-b288-4022-8f13-c4984cba5118"], "metadata": {"source": "web", "name": "web search results", "description": "Web search results for the paper-reviews"}}, "e62f7b37-ded4-4d30-a2e7-5574679b2e2c": {"node_ids": ["b0c70401-bfb8-460f-8f12-23f21c5e8a7e"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "fb7aec31-7ff3-4868-ba84-4cb19de2258e": {"node_ids": ["acae008e-b7f6-41f8-937a-f4314768f9ee", "e8083740-bf46-4e25-847f-f48a846b583e", "acae008e-b7f6-41f8-937a-f4314768f9ee", "e8083740-bf46-4e25-847f-f48a846b583e"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "2c8587f2-e3ce-47e6-be70-740471f08c36": {"node_ids": ["4954475e-ca1f-477c-be65-12867b8b31d1"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "5feee89b-d75f-4a27-b176-f436afeafc46": {"node_ids": ["11a8c62a-4d0a-4df1-a783-ac0db5a52475", "c1304b34-ed89-4aee-ad07-2596c341ea9c", "11a8c62a-4d0a-4df1-a783-ac0db5a52475", "c1304b34-ed89-4aee-ad07-2596c341ea9c"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "922c8bcd-01c9-4d72-b78a-c5e4b19dd874": {"node_ids": ["53585b40-5885-495c-b4d4-48f394faecf3"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "7dc0c5ce-56b6-4f45-ac77-040d89765751": {"node_ids": ["4936d8bd-2b2d-47e2-aeca-f9b2b24f20cc", "c81d1514-1080-4955-9760-44da8e3e7033", "254dd1b3-fd70-441e-a921-87375324c580", "4936d8bd-2b2d-47e2-aeca-f9b2b24f20cc", "c81d1514-1080-4955-9760-44da8e3e7033", "254dd1b3-fd70-441e-a921-87375324c580", "4936d8bd-2b2d-47e2-aeca-f9b2b24f20cc", "c81d1514-1080-4955-9760-44da8e3e7033", "254dd1b3-fd70-441e-a921-87375324c580"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "36b68b3c-dbfc-41b8-8155-fb1cbf250620": {"node_ids": ["c4d470f5-ad3d-4fba-9aa6-ac7ab97438d0", "ff65cb59-cdd1-4749-87a3-d9d4a2921119", "c4d470f5-ad3d-4fba-9aa6-ac7ab97438d0", "ff65cb59-cdd1-4749-87a3-d9d4a2921119"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "b7e37a30-e3ac-4a18-8790-e1dc7ee33043": {"node_ids": ["0036613c-beae-4296-9338-35ae15a49fd3"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "caa2a543-17b6-4a48-93b2-eb1d82a1bb15": {"node_ids": ["1cc903c2-a6b6-4266-a143-d3f8c3a751ea", "9aa89415-1382-4713-8b0e-af4e38d08b7f", "1cc903c2-a6b6-4266-a143-d3f8c3a751ea", "9aa89415-1382-4713-8b0e-af4e38d08b7f"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "f10934d8-1bea-4c8f-80a5-3e7e6f7a6f2b": {"node_ids": ["35ee119c-8def-4e3d-aaea-ff360494777d", "26c45932-c2b4-455e-99d7-5019546a6a89", "35ee119c-8def-4e3d-aaea-ff360494777d", "26c45932-c2b4-455e-99d7-5019546a6a89"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "b01a0f79-ddda-4810-baeb-5d5498b98ef1": {"node_ids": ["6a3dc7ba-a926-468f-9ce8-0b6f010cc015", "96855d91-a66d-4e78-82d8-a3dc61909875", "6a3dc7ba-a926-468f-9ce8-0b6f010cc015", "96855d91-a66d-4e78-82d8-a3dc61909875"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "63daab92-9f2e-424b-bb0e-be5c3074d359": {"node_ids": ["3a5d19e0-c3b6-42d3-83c2-24cba268750a", "fc8e156f-7bb6-415c-b400-33472b7d1cd1", "c88fdb2c-df10-42d3-b887-a7addb382bdd", "07ea05e1-3399-4103-af08-f3766b6e774e", "5a609265-cf78-4480-a2b4-724b41e99d68", "e009135f-ba6c-46b3-9799-57b6d4dc80a3", "3a5d19e0-c3b6-42d3-83c2-24cba268750a", "fc8e156f-7bb6-415c-b400-33472b7d1cd1", "c88fdb2c-df10-42d3-b887-a7addb382bdd", "07ea05e1-3399-4103-af08-f3766b6e774e", "5a609265-cf78-4480-a2b4-724b41e99d68", "e009135f-ba6c-46b3-9799-57b6d4dc80a3", "3a5d19e0-c3b6-42d3-83c2-24cba268750a", "fc8e156f-7bb6-415c-b400-33472b7d1cd1", "c88fdb2c-df10-42d3-b887-a7addb382bdd", "07ea05e1-3399-4103-af08-f3766b6e774e", "5a609265-cf78-4480-a2b4-724b41e99d68", "e009135f-ba6c-46b3-9799-57b6d4dc80a3", "3a5d19e0-c3b6-42d3-83c2-24cba268750a", "fc8e156f-7bb6-415c-b400-33472b7d1cd1", "c88fdb2c-df10-42d3-b887-a7addb382bdd", "07ea05e1-3399-4103-af08-f3766b6e774e", "5a609265-cf78-4480-a2b4-724b41e99d68", "e009135f-ba6c-46b3-9799-57b6d4dc80a3", "3a5d19e0-c3b6-42d3-83c2-24cba268750a", "fc8e156f-7bb6-415c-b400-33472b7d1cd1", "c88fdb2c-df10-42d3-b887-a7addb382bdd", "07ea05e1-3399-4103-af08-f3766b6e774e", "5a609265-cf78-4480-a2b4-724b41e99d68", "e009135f-ba6c-46b3-9799-57b6d4dc80a3", "3a5d19e0-c3b6-42d3-83c2-24cba268750a", "fc8e156f-7bb6-415c-b400-33472b7d1cd1", "c88fdb2c-df10-42d3-b887-a7addb382bdd", "07ea05e1-3399-4103-af08-f3766b6e774e", "5a609265-cf78-4480-a2b4-724b41e99d68", "e009135f-ba6c-46b3-9799-57b6d4dc80a3"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "61d76b70-8bd2-44b0-8c12-5b7be023555b": {"node_ids": ["8b7319bb-c3a5-47fe-914f-70fe113b4128", "bc3d187c-a058-4e3b-8230-b6bf738e164a", "8b7319bb-c3a5-47fe-914f-70fe113b4128", "bc3d187c-a058-4e3b-8230-b6bf738e164a"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "4f2c7a6a-0a8e-488a-8e2f-d3907e9706d9": {"node_ids": ["e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad", "8f9774e7-a059-48a5-a2c7-e94681a8adfd", "08f70598-5610-425e-abd1-a97ebd201ac6", "1f83ff51-d146-49cc-a71f-3dc88db543af", "dba88b25-1fbf-4bb0-9645-991de23588d8", "dea454af-e062-43f9-8cdc-a456c19ec801", "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c", "e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad", "8f9774e7-a059-48a5-a2c7-e94681a8adfd", "08f70598-5610-425e-abd1-a97ebd201ac6", "1f83ff51-d146-49cc-a71f-3dc88db543af", "dba88b25-1fbf-4bb0-9645-991de23588d8", "dea454af-e062-43f9-8cdc-a456c19ec801", "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c", "e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad", "8f9774e7-a059-48a5-a2c7-e94681a8adfd", "08f70598-5610-425e-abd1-a97ebd201ac6", "1f83ff51-d146-49cc-a71f-3dc88db543af", "dba88b25-1fbf-4bb0-9645-991de23588d8", "dea454af-e062-43f9-8cdc-a456c19ec801", "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c", "e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad", "8f9774e7-a059-48a5-a2c7-e94681a8adfd", "08f70598-5610-425e-abd1-a97ebd201ac6", "1f83ff51-d146-49cc-a71f-3dc88db543af", "dba88b25-1fbf-4bb0-9645-991de23588d8", "dea454af-e062-43f9-8cdc-a456c19ec801", "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c", "e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad", "8f9774e7-a059-48a5-a2c7-e94681a8adfd", "08f70598-5610-425e-abd1-a97ebd201ac6", "1f83ff51-d146-49cc-a71f-3dc88db543af", "dba88b25-1fbf-4bb0-9645-991de23588d8", "dea454af-e062-43f9-8cdc-a456c19ec801", "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c", "e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad", "8f9774e7-a059-48a5-a2c7-e94681a8adfd", "08f70598-5610-425e-abd1-a97ebd201ac6", "1f83ff51-d146-49cc-a71f-3dc88db543af", "dba88b25-1fbf-4bb0-9645-991de23588d8", "dea454af-e062-43f9-8cdc-a456c19ec801", "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c", "e5431681-a9a9-41d9-9b1c-ab3fbd6ef0ad", "8f9774e7-a059-48a5-a2c7-e94681a8adfd", "08f70598-5610-425e-abd1-a97ebd201ac6", "1f83ff51-d146-49cc-a71f-3dc88db543af", "dba88b25-1fbf-4bb0-9645-991de23588d8", "dea454af-e062-43f9-8cdc-a456c19ec801", "c8d4dd3e-d31d-4d6a-ad95-3ae09daaab1c"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "365a0e8d-416c-4077-931f-4527a21aa813": {"node_ids": ["96b7d701-bab3-496e-9983-f91c7c10d99f", "87c9e0a3-9ea4-4dcc-9b3c-548848424704", "96b7d701-bab3-496e-9983-f91c7c10d99f", "87c9e0a3-9ea4-4dcc-9b3c-548848424704"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "9ffca80b-def6-4e14-aa7e-c7d6b9fa1147": {"node_ids": ["60f01090-cea4-4e5a-9afd-d0db06795648", "fbe28570-9918-4321-a3d3-d95aeee6725a", "17cc3242-19f1-482e-8005-f2eda9ff14bc", "f0621218-24f2-47e6-a2a6-a78f0e52da5a", "60f01090-cea4-4e5a-9afd-d0db06795648", "fbe28570-9918-4321-a3d3-d95aeee6725a", "17cc3242-19f1-482e-8005-f2eda9ff14bc", "f0621218-24f2-47e6-a2a6-a78f0e52da5a", "60f01090-cea4-4e5a-9afd-d0db06795648", "fbe28570-9918-4321-a3d3-d95aeee6725a", "17cc3242-19f1-482e-8005-f2eda9ff14bc", "f0621218-24f2-47e6-a2a6-a78f0e52da5a", "60f01090-cea4-4e5a-9afd-d0db06795648", "fbe28570-9918-4321-a3d3-d95aeee6725a", "17cc3242-19f1-482e-8005-f2eda9ff14bc", "f0621218-24f2-47e6-a2a6-a78f0e52da5a"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "95533d23-8d21-48de-9019-b7d57264539a": {"node_ids": ["ebb21d80-1110-4bd8-bdf3-c3a6af5ac616", "0b2e8f94-c841-4cf2-9857-e4b149c83199", "ebb21d80-1110-4bd8-bdf3-c3a6af5ac616", "0b2e8f94-c841-4cf2-9857-e4b149c83199"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "b0d91f6e-f6d8-4314-b8d5-696e91b90d3a": {"node_ids": ["17eb10ae-b5fc-4846-9c4f-fffb6a928323", "3e775158-6f90-4613-a73d-0d6a27ea26ec", "17eb10ae-b5fc-4846-9c4f-fffb6a928323", "3e775158-6f90-4613-a73d-0d6a27ea26ec"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "8144dd54-a80a-4505-a0d8-5bc4cb857c7b": {"node_ids": ["17b4198b-c2c7-4221-a269-37f0ab636d5d", "022487f1-5249-4fda-9dfc-9dbd89191093", "17b4198b-c2c7-4221-a269-37f0ab636d5d", "022487f1-5249-4fda-9dfc-9dbd89191093"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "c3077941-d825-4144-9a16-4d06e432d7fd": {"node_ids": ["c355c207-99d4-490f-8c8e-5812433f524a", "15d2effa-4713-4ebb-841a-4698fb93e294", "c355c207-99d4-490f-8c8e-5812433f524a", "15d2effa-4713-4ebb-841a-4698fb93e294"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "58a7d5dc-a634-407b-9d1d-fec2ee973a1e": {"node_ids": ["72956801-6df6-4b1e-9be0-a12be8f38a1e", "1c7e8abe-f3bd-44ec-9aa5-cd6c1aed22d1", "72956801-6df6-4b1e-9be0-a12be8f38a1e", "1c7e8abe-f3bd-44ec-9aa5-cd6c1aed22d1"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "d8d930cb-81aa-40a0-97d8-b10fb05382dd": {"node_ids": ["0f480892-3733-4265-ae7a-752ecc5f7335", "a93f74f9-f1b2-419d-bb76-e17f653e5fb0", "0f480892-3733-4265-ae7a-752ecc5f7335", "a93f74f9-f1b2-419d-bb76-e17f653e5fb0"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "2af4c573-5453-4b45-bb77-cdc97e693bf0": {"node_ids": ["0479d186-4c78-4e64-8b51-f370ade21803"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "9bca7bcf-f0f9-4180-9d3d-826191e75964": {"node_ids": ["2698f872-4a01-4db2-ac79-f5b203af1cda"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "c414de5c-48e5-4267-8e68-0b73cd20e3aa": {"node_ids": ["f8220d4a-b1c5-4c6c-92ed-de1d866d51e4", "5b387f16-fb08-4562-97ab-3a7c78b9e595", "f8220d4a-b1c5-4c6c-92ed-de1d866d51e4", "5b387f16-fb08-4562-97ab-3a7c78b9e595"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "711a7c3a-394f-45c6-9ba5-2c65df8d710d": {"node_ids": ["887caca0-64e5-470d-9886-dc1752cf86db", "74422d86-3795-4868-99e3-f409c0ba8c1f", "887caca0-64e5-470d-9886-dc1752cf86db", "74422d86-3795-4868-99e3-f409c0ba8c1f"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "610c667e-43cc-4654-9295-89b872a08cfb": {"node_ids": ["108f131a-7f2b-4092-82bf-7e561870a1ce"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "065300df-b11f-4b7f-a419-2e206d0a664d": {"node_ids": ["20ef0d0d-4192-48b8-9ea4-5aea529af68b", "665848e6-9bdd-4d67-bc2d-ca637060e2b5", "20ef0d0d-4192-48b8-9ea4-5aea529af68b", "665848e6-9bdd-4d67-bc2d-ca637060e2b5"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "2cbd515e-f836-40e8-94c5-76d3bb93e560": {"node_ids": ["b9557003-8285-425a-8928-175c7616ebbb", "0337ef6d-3da4-489d-af27-dc7f9d3e190d", "b9557003-8285-425a-8928-175c7616ebbb", "0337ef6d-3da4-489d-af27-dc7f9d3e190d"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "f75b81fb-3f95-460d-a36a-f13a82c35003": {"node_ids": ["3dd2273a-8fe4-4038-a8ad-de6074197a1d"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "7647cdb5-7eae-48e8-99c7-54ff2b5a6a83": {"node_ids": ["0bea9ab5-a422-4970-869e-cfa62a8c55d5"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "48570be3-fae9-48da-89ed-78e50dbb36ff": {"node_ids": ["f2fa0547-acda-458d-8062-abd84f97c018", "913bf01b-9a9c-4f64-9283-24fb7ba5d020", "aaef88a2-0435-4201-a4bc-dcbee4c52c08", "b50f0c47-443b-483f-bf88-ca52be52e591", "4212a501-bb7b-47e1-a202-a3ad3e5a1585", "f2fa0547-acda-458d-8062-abd84f97c018", "913bf01b-9a9c-4f64-9283-24fb7ba5d020", "aaef88a2-0435-4201-a4bc-dcbee4c52c08", "b50f0c47-443b-483f-bf88-ca52be52e591", "4212a501-bb7b-47e1-a202-a3ad3e5a1585", "f2fa0547-acda-458d-8062-abd84f97c018", "913bf01b-9a9c-4f64-9283-24fb7ba5d020", "aaef88a2-0435-4201-a4bc-dcbee4c52c08", "b50f0c47-443b-483f-bf88-ca52be52e591", "4212a501-bb7b-47e1-a202-a3ad3e5a1585", "f2fa0547-acda-458d-8062-abd84f97c018", "913bf01b-9a9c-4f64-9283-24fb7ba5d020", "aaef88a2-0435-4201-a4bc-dcbee4c52c08", "b50f0c47-443b-483f-bf88-ca52be52e591", "4212a501-bb7b-47e1-a202-a3ad3e5a1585", "f2fa0547-acda-458d-8062-abd84f97c018", "913bf01b-9a9c-4f64-9283-24fb7ba5d020", "aaef88a2-0435-4201-a4bc-dcbee4c52c08", "b50f0c47-443b-483f-bf88-ca52be52e591", "4212a501-bb7b-47e1-a202-a3ad3e5a1585"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "d4a959c3-b750-470e-81c6-56462b0a507e": {"node_ids": ["76089f80-822c-486d-b33f-d24b000c6d95"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "df06da45-c846-425a-9064-c7ee89125f04": {"node_ids": ["c4c07ec7-8fd5-4b20-8076-424428b7fc9d", "427dc512-e4ba-4a5f-9b6b-15376ec8c99a", "c4c07ec7-8fd5-4b20-8076-424428b7fc9d", "427dc512-e4ba-4a5f-9b6b-15376ec8c99a"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "ca4e743b-0712-40ee-9311-138ca40e566e": {"node_ids": ["618a4b15-25e3-4b14-a822-0f45295fc14f", "194b2c64-b2fc-4631-a2d6-684de9d2fd25", "6bde38b3-b5a3-4c28-9506-0291a6c83c25", "10e711de-a2e8-4ef5-9e05-42e9e928f939", "e80025e3-8ff5-44e2-90cb-969ab25d6499", "3855f25f-15dc-4623-b2ad-c023249d84d7", "618a4b15-25e3-4b14-a822-0f45295fc14f", "194b2c64-b2fc-4631-a2d6-684de9d2fd25", "6bde38b3-b5a3-4c28-9506-0291a6c83c25", "10e711de-a2e8-4ef5-9e05-42e9e928f939", "e80025e3-8ff5-44e2-90cb-969ab25d6499", "3855f25f-15dc-4623-b2ad-c023249d84d7", "618a4b15-25e3-4b14-a822-0f45295fc14f", "194b2c64-b2fc-4631-a2d6-684de9d2fd25", "6bde38b3-b5a3-4c28-9506-0291a6c83c25", "10e711de-a2e8-4ef5-9e05-42e9e928f939", "e80025e3-8ff5-44e2-90cb-969ab25d6499", "3855f25f-15dc-4623-b2ad-c023249d84d7", "618a4b15-25e3-4b14-a822-0f45295fc14f", "194b2c64-b2fc-4631-a2d6-684de9d2fd25", "6bde38b3-b5a3-4c28-9506-0291a6c83c25", "10e711de-a2e8-4ef5-9e05-42e9e928f939", "e80025e3-8ff5-44e2-90cb-969ab25d6499", "3855f25f-15dc-4623-b2ad-c023249d84d7", "618a4b15-25e3-4b14-a822-0f45295fc14f", "194b2c64-b2fc-4631-a2d6-684de9d2fd25", "6bde38b3-b5a3-4c28-9506-0291a6c83c25", "10e711de-a2e8-4ef5-9e05-42e9e928f939", "e80025e3-8ff5-44e2-90cb-969ab25d6499", "3855f25f-15dc-4623-b2ad-c023249d84d7", "618a4b15-25e3-4b14-a822-0f45295fc14f", "194b2c64-b2fc-4631-a2d6-684de9d2fd25", "6bde38b3-b5a3-4c28-9506-0291a6c83c25", "10e711de-a2e8-4ef5-9e05-42e9e928f939", "e80025e3-8ff5-44e2-90cb-969ab25d6499", "3855f25f-15dc-4623-b2ad-c023249d84d7"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "1c837738-8aa0-4c63-b7f6-c9cfa40a5a63": {"node_ids": ["2bd34ae7-e2c5-4635-b3d1-c1cfe61510be"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "9b044ba5-6d31-492d-850d-08107355bfd3": {"node_ids": ["18e9c44f-6e39-435e-bb89-5e044a356c61"], "metadata": {"source": "paper", "name": "paper", "description": "papers of the paper-reviews"}}, "29e222db-c5b0-44d1-8709-416fb7974ac9": {"node_ids": ["fcf9f991-59bd-445a-b3c4-8937606ef41a"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "2867d599-e4d9-46fd-82ca-49b70e15bf54": {"node_ids": ["f2406856-b1b7-4334-a778-7e491082dc15"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "f722f3fc-7882-4527-b93c-28046e7c8db1": {"node_ids": ["7ceeb091-da72-4c0a-b770-ef128067b2ef"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "521916c5-52e9-43e2-bef7-0bab63da516b": {"node_ids": ["fc4433fd-b660-41cb-8b83-ea695cab760b"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "ebd47b6e-9ff5-455e-a4cd-a899c1f06fed": {"node_ids": ["4edf8599-f82a-4b26-8bbe-edda45a7a815"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "9b871f9a-317f-445e-9e14-1dfdf6c40c5d": {"node_ids": ["270c6075-67cd-4045-b11e-7a85af1bb155"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "24cfa21a-0a01-4a18-9439-46d856e33dcd": {"node_ids": ["2c21dca9-aed1-4f3d-a1ac-3d47dff5e313"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "340e1743-5330-48ed-9ca4-67ed362a809d": {"node_ids": ["9c9d765c-3259-4066-a872-2f7513207958"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "e0d37b14-f0e5-4a09-9cd1-c8286ba4bf8a": {"node_ids": ["3977971e-5c0b-464e-808c-e1c45a702dbd"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "c1f7f73d-7421-4468-a185-b505f9f09370": {"node_ids": ["4b3d1fa0-260b-438e-83ac-b1e68b7d090f"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "f52cd5e9-bffd-43d9-a257-add613e6cc0d": {"node_ids": ["8749c05f-8e10-4b92-9e76-eeb583ba6139"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "a2d06dbb-f9c0-449d-9fa4-829c91a1f550": {"node_ids": ["9f892b0b-30d4-4fe7-908b-34653ca3a238"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "e0cd9b2f-c39f-428f-b917-7d35df381cdc": {"node_ids": ["7061cb5e-0334-43da-ae0c-8498f205a074"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "ab62cc09-c3b8-48eb-8808-47e4b425a70b": {"node_ids": ["32f93012-0c92-4df4-b074-30139b4b38f2"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}, "268b0864-6c3e-4fed-89ce-02ea5c0477f3": {"node_ids": ["42fdfacb-558e-405e-8d91-4056051bde4a"], "metadata": {"source": "user", "name": "user report", "description": "user reports of the paper-reviews"}}}}