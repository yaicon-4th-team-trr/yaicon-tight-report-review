메뉴

#  [brunch](/ "brunch")

실행

신고

라이킷 19 댓글 공유

닫기

You can make anything  
by writing

C.S.Lewis

브런치스토리 시작하기

  * [브런치스토리 홈](/)
  * [브런치스토리 나우](/now)
  * [브런치스토리 책방](/publish)

[ ![프로젝트 리스트
바로가기](//i1.daumcdn.net/thumb/R336x0/?fname=http://t1.daumcdn.net/brunch/static/img/help/pc/top/side_banner_20221221.png)
](/special/list)

[계정을 잊어버리셨나요?](/signin/find_account)

by [delight](https://brunch.co.kr/@delight412) Feb 25. 2024

# 왜 맘바를 트랜스포머 언어 모델링의 대안으로 주목하는가

> 학습 차원에서 틈틈이 해외 전문가들이 블로그나 미디어 그리고 책에서 쓴 글을 번역 또는 요약 정리하고 있습니다. 이번 포스팅도 그중
> 하나고요. 거칠고 오역된 부분이 있을 수 있습니다. 제대로 번역되지 않은 부분은 확인 주시면 반영토록 하겠습니다. 의미 전달이 애매한
> 일부 문장은 삭제했습니다. 이번에는 미디엄에 올라온 [Ignacio de Gregorio의 글을 정리한
> 것](https://medium.com/towards-artificial-intelligence/is-mamba-the-end-of-
> chatgpt-as-we-know-it-a2ce57de0b02)입니다.

  

두 명의 연구자가 21세기 최대 알고리즘 혁신을 창밖으로 던지는 대담한 주장을 펼쳤다. 맘바(Mamba) 알고리즘은 한때 불가능하다고
여겨졌던 것을 달성했다. 더 빠르고 훨씬 저렴하면서도 트랜스포머(Transformer) 언어 모델링 기능과 비슷하거나 능가하는 성능을
구현했다. 모두가 맘바에 대해 이야기하고 있는 것 같다. 맘바에 대해 알아보자.

  

2017년 출시 이후 트랜스포머 아키텍처는 자연어 모델링(텍스트를 생성하는 모델)을 위한 '사실상의' 선택 표준이 됐다. ChatGPT,
Gemini, Claude 등이 모두 트랜스포머에 기반한다. 트랜스포머 아키텍처에서 핵심인 hatGPT의 'T'가 '트랜스포머'를 의미한다.

  

'시퀀스 대 시퀀스 모델(텍스트 구절 또는 이미지 픽셀 시퀀스 등 시퀀스를 입력으로 받아 다른 시퀀스, 반적으로 새로운 텍스트를를 제공하는
트랜스포머의 비밀은 바로 어텐션 메커니즘(attention mechanism)이다.

간단히 말하자면, 시퀀스 모든 토큰을 쌍으로 곱해 단어 간 관계를 파악하기 위해 '대화'하게 만드는 것이다.

간단히 말해, 각 단어가 시퀀스에서 어떤 단어에 '주의'를 기울여야 하는지를 나타낸다. 예를 들어, 대명사에 대한 어텐션 메커니즘은 명사에
주의를 기울일 수 있도록 한다.

  

이것은 훌륭하게 작동하지만 엄청난 비용이 든다. 간단히 말해, 트랜스포머는 매우 비효율적인 대가로 인상적인 성능을 제공한다.

  

챗GPT와 같은 모델이 언어를 처리하는 방식을 이해하려면 해리 포터 책을 반쯤 읽었다고 상상해 보라. 하지만 다음 페이지를 성공적으로
읽으려면 이전에 읽은 모든 페이지, 즉 전체 문맥을 머릿속에 저장해야 한다. 간단히 말해, 다음 단어를 읽으려면 그 시점까지 모든 내용을
다시 읽어야 한다. 트랜스포머는 실제로 매번 모든 것을 다시 읽지 않고 이전 계산 캐시를 저장한다. 하지만 모든 계산을 메모리에 저장해야
하므로 다음 단어를 성공적으로 읽으려면 이전에 읽은 모든 단어를 기억해야 하는 것과 마찬가지다.

  

이것이 바로 챗GPT와 같은 트랜스포머가 언어를 처리하는 방식이다. 매우 비효율적으로 보이지 않나?

  

맞다, 그렇다.

  

하지만 인간은 해리 포터 책을 어떻게 읽을까? 그때까지 읽은 많은 내용을 기억하지만 헤르미오네의 여름 활동과 같은 관련 없는 데이터는
잊어버린다. 스토리 라인과 관련이 없기 때문에 우리 머릿속은 그것을 잊어버린다. 즉, 지금까지 읽은 모든 정보를 기억하는 대신 스토리에 대해
압축된 표현을 만들어 관련성 있는 데이터는 유지하고 관련 없는 부분은 지워버리는 것이다.

  

트랜스포머는 문맥을 압축하지 않는다. 컨텍스트가 압축되지 않기 때문에 텍스트 시퀀스가 커질수록 트랜스포머 계산 요구 사항이 상당히 커진다.
특히, 트랜스포머는 훈련과 추론에서 각각 이차적 복잡성과 선형적 복잡성을 갖는다. 즉, 훈련에서는 시퀀스를 두 배로 늘리면 비용이
2차(quadratic) 함수 형태로 증가하고 추론(실행)에서는 두 배로 증가한다.

  

그렇다면 연구실에서는 어떻게 할까? 비용이 통제 불능 상태가 되는 것을 방지하기 위해 모델 '작업 공간'을 컨텍스트 창으로 제한하기 때문에
ChatGPT 등은 처리할 수 있는 텍스트 크기가 제한돼 있다. 지난 몇 년 동안 보다 '효율적인' 어텐션 메커니즘을 갖춘 다양한
아키텍처들이 제안됐지만 성능 저하로 인해 트랜스포머를 대체할 수 없었다.

  

그래서 맘바 연구원들은 어떻게 했을까? 간단히 말해, 지난 몇 년간 가장 큰 알고리즘 혁신이었던 어텐션 메커니즘과 결별했다. 맘바 아키텍처는
중요한 질문에서 탄생했다.: 트랜스포머만큼 효과적으로 언어를 모델링하면서도 훨씬 더 효율적일 수 있을까?

  

답연 그렇다였다. '상태'(‘state)라고 정의하는 것 덕분이었다. 해리포터 사례로 돌아가 우리는 책을 읽을 때 책의 최신 상태를
유지하면서 핵심 요소만 저장하고 나머지는 삭제하면서 무슨 일이 일어나고 있는지 압축적이고 대략적인 이해를 천천히 구축한다. 이것이 기본적으로
바로 맘바가 하는 일이다.

  

트랜스포머의 핵심에서 어텐션 모델이 있는 것처럼 선택적 SSM(Selective State Space Model)이 맘바의 핵심에 있다.
SSM은 1960년대 상태 공간 모델에서 영감을 받은 다소 새로운 언어 모델링 아키텍처다.

  

간단히 말해, 이 모델은 컨텍스트 역할을 하는 '상태' 또는 메모리를 유지한다. 즉, 다음 출력은 현재 입력과 그 시점까지 내 현재 상태의
함수가 된다. 현재 입력이 '해리'라면 SSM은 그 상태를 사용해 다음 단어가 '포터'일 것이라고 추론한다. 이전에 두 단어를 함께 본 적이
있기 때문에 상태는 이를 기억할 것이다.

  

하지만 ChatGPT는 '포터'도 예측할 수 있을까? 그렇다. 하지만 여기서 중요한 것은 선택적 SSM은 중요한 문맥만 메모리에 유지하므로
훈련과 추론을 위한 복잡성이 선형적이고 일정하다. 즉, 시퀀스가 두 배가 되면 훈련 비용은 두 배(트랜스포머는 네 배)가 되는 반면 추론
비용은 길이에 상관없이 일정하게 유지된다!

  

일정한 추론 복잡도는 상태 크기가 고정되어 있고, 시퀀스 길이에 상관없이 비용이 증가하지 않으며, 모델이 핵심 정보만 저장하고 나머지는
잊어버리기 때문에 가능하다. 반대로 이러한 모델이 제공하는 예측은 압축된 컨텍스트만큼 정확도가 높다.

  

그렇다면 어떻게 선택적 SSM은 어떤 컨텍스트를 유지할지 선택하고 트랜스포머는 안그런걸까? 답은 바로 선택성( selectivity)이다.

  

맘바의 독특한 점은 선택적 SSM 모듈이 어떤 컨텍스트를 유지하고 어떤 컨텍스트를 버릴지 선택한다는 것이다. 다시 말해, 우리가 선택에 의한
압축이라고 정의하는 작업을 수행한다.이를 가능하게 하는 핵심은 이전 모든 아키텍처와 달리 맘바는 입력과 시간에 따라 달라진다는 점이다. 진짜
궁금한 점은 맘바가 트랜스포머와 비교했을 때 어떤 차이가 있을까?

  

 **흥미롭지만 여전히 남아있는 질문**

다양한 작은 크기(최대 70억 개 매개변수)에서 테스트한 결과, Mamba는 난해성(모델이 다음 토큰을 얼마나 잘 예측하는지 측정하는
척도)과 정확도 모두에서 비슷한 크기 GPT를 포함한 다른 모든 모델을 능가했다.

심지어 크기가 두 배 이상 큰 트랜스포머 결과와도 맞먹는다. 길이가 길어져도 정확도가 떨어지지 않는데, 이는 지금까지 전례가 없던 일이다.

  

하지만 맘바는 아직 큰 사이즈에서는 성능이 입증되지 않았지만 맘바가 보여주는 결과는 품질에 대한 설명 뿐만 아니라 확장 가능한 솔루션에
상태를 성공적으로 통합했다는데 아키텍처 자체의 아름다움이 있다. 보다 중요한 것은 그것이 합리적이라는 사실이다.

  

이러한 결과를  최첨단 크기 LLM애서 추론할 수 있다면, 우리가 알고 있는 ChatGPT의 종말이라고 자신 있게 말할 수 있으며 곧 맘바의
'M'을 딴 ChatGPM의 탄생을 볼 수 있을 것이다.

**keyword**

  * [ AI ](/keyword/AI)
  * [ IT ](/keyword/IT)
  * [ 비즈니스 ](/keyword/비즈니스)

**[ delight ](https://brunch.co.kr/@delight412) ** [ IT 분야 크리에이터
](https://storyhome.kakao.com/topcreator/?category=it) [
![](//img1.daumcdn.net/thumb/C200x200.fjpg/?fname=http://t1.daumcdn.net/brunch/service/guest/image/0zsUDNIVIwXoF6PIqlICT74kA5k)
](https://brunch.co.kr/@delight412)

[

delight의 브런치입니다. 책과 디지털 공간에서 곱씹어볼만한 오피니언을 정리하고 있습니다. delight412@gmail.com

](https://brunch.co.kr/@delight412)

[ 구독자 1,241 ](https://brunch.co.kr/@delight412)
![](https://t1.daumcdn.net/brunch/static/img/icon/ico-plus.png)구독

[ 작가의 이전글  **일 론 머스크가 트위터 이름을 X로 바꾼 까닭은** ](/@delight412/662) [ **오 픈AI 소라,
동영상 생성 AI 너머 파괴력을 보라** 작가의 다음글  ](/@delight412/664)

취소 완료

### 작품 선택

키워드 선택 0 / 3 0 검색

### 댓글여부

댓글 쓰기 허용 afliean

_브런치는 최신 브라우저에 최적화 되어있습니다._ [IE](http://windows.microsoft.com/ko-kr/internet-
explorer/download-ie) [chrome](http://www.google.co.kr/chrome/)
[safari](http://www.apple.com/kr/safari/)

