본문 바로가기

# [Ostin X](https://ostin.tistory.com/)

메뉴

  * [ 분류 전체보기 (487) ](/category)
    * [ 논문 리뷰 (0) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)
      * [ Language Model (126) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Language%20Model)
      * [ Diffusion Model (136) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Diffusion%20Model)
      * [ Vision Transformer (62) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Vision%20Transformer)
      * [ Mamba (7) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Mamba)
      * [ GAN (20) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/GAN)
      * [ etc. (57) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/etc.)
      * [ Concept (5) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Concept)
      * [ 논문 분류 (7) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/%EB%85%BC%EB%AC%B8%20%EB%B6%84%EB%A5%98)
    * [ 코드 리뷰 (8) ](/category/%EC%BD%94%EB%93%9C%20%EB%A6%AC%EB%B7%B0)
      * [ Diffusion (8) ](/category/%EC%BD%94%EB%93%9C%20%EB%A6%AC%EB%B7%B0/Diffusion)
    * [ Deep Learning (34) ](/category/Deep%20Learning)
      * [ GAN (14) ](/category/Deep%20Learning/GAN)
      * [ Fine Tuning (10) ](/category/Deep%20Learning/Fine%20Tuning)
      * [ Diffusion (4) ](/category/Deep%20Learning/Diffusion)
      * [ Memo or etc. (6) ](/category/Deep%20Learning/Memo%20or%20etc.)
    * [ Code, Error, Tip, Etc. (4) ](/category/Code%2C%20Error%2C%20Tip%2C%20Etc.)
    * [ Output (8) ](/category/Output)
      * [ Model (4) ](/category/Output/Model)
      * [ Small Things (4) ](/category/Output/Small%20Things)
    * [ 사설 (13) ](/category/%EC%82%AC%EC%84%A4)
      * [ X (0) ](/category/%EC%82%AC%EC%84%A4/X)
      * [ 독후감 (3) ](/category/%EC%82%AC%EC%84%A4/%EB%8F%85%ED%9B%84%EA%B0%90)

블로그 내 검색 검색

* * *

논문 리뷰/Mamba

# Mamba: Linear-Time Sequence Modeling with Selective State Spaces

Ostin 2024\. 1. 15. 15:07

\+ [Mamba에 대해 매우 잘 설명되어 있는 글](https://tulip-
phalange-a1e.notion.site/05f977226a0e44c6b35ed9bfe0076839) <- 그냥 이거 보세요



SSM에 선택성을 부여하고 하드웨어 최적화



[[Github](https://github.com/state-spaces/mamba)]

[[arXiv](https://arxiv.org/abs/2312.00752)](2023/12/01 version v1)







## __**Abstract**__

Transformer 기반 모델들이 긴 시퀀스 처리에서 보여주는 계산 비효율성을 해결하기 위해 Mamba라는 새로운 신경망 구조를 제안







## __**State Space Models**__

[필독!!! S4 model](https://ostin.tistory.com/379)

[  

Efficiently Modeling Long Sequences with Structured State Spaces (S4)

[arXiv](2022/08/05 version v3) 영어 잘하시면 이거 보세요.
https://srush.github.io/annotated-s4/ 근데 솔직히 원어민도 이거 보고 이해 못 할 듯; The
Annotated S4 srush.github.io 시작하기 전에 말하자면 이 논문에 관

ostin.tistory.com

](https://ostin.tistory.com/379)



표기가 살짝 다르고(state h(t), input x) 피라미터가 4개입니다. (∆, A, B, C)

![](https://blog.kakaocdn.net/dn/TWWHe/btsDmWz0nbf/bisXpSd3Zm3eO1HCVMxMo0/img.png)



__Discretization__

이산 시간 피라미터는 연속 시간 피라미터 A, B로 나타낼 수 있다.

![](https://blog.kakaocdn.net/dn/lwCH3/btsDqaRh5xL/UpdPoSmPqTKlWKyNBpBEN1/img.png)



__Computation__

전체 입력 시퀀스를 미리 볼 수 있는 경우 convolution mode(3a, 3b)를 사용하고 한 번에 한 timestep만 볼 수 있는
경우 recurrent mode(2a, 2b)로 전환한다.



__Linear Time Invariance (LTI)__

방정식 (1~3)의 중요한 특성은 모델의 dynamics가 시간에 따라 일정하다는 것이다.

이를 [Linear Time-Invariant](https://keyboard-lover.tistory.com/18)(LTI,
시불변성)이라고 한다.

![](https://blog.kakaocdn.net/dn/bW4u2f/btsDqaEOWf6/cbjazLVuA2CkqvRbA7qtoK/img.png)



지금까지 모든 structured SSM은 LTI였다.

그러나 본 논문의 핵심 통찰은 LTI 제약을 제거하는 것과 관련이 있다.



__Structure and Dimensions__

Structured SSM이라는 이름이 붙여진 이유는 SSM을 효율적으로 계산하기 위해 A에 구조를 부여해야 했기 때문이다. 가장 널리
사용되는 구조는 대각선이며, 본문에서도 이를 사용한다.



그럴 경우 A ∈ ℝN×N, B ∈ ℝN×1, C ∈ ℝ1×N이고 batch size B, length L, channels D인 시퀀스
x에 대해 작동하려면 SSM이 각 채널에 독립적으로 적용되어야 하며 hidden state가 DN이 되고, 이는 병목 현상의 근원이다.
요컨대 차원이 부족해서.







## __**Selective State Space Models**__

  1. Motivation: Selection as a Means of Compression
  2. Improving SSMs with Selection
  3. Efficient Implementation of Selective SSMs
  4. A Simplified SSM Architecture
  5. Properties of Selection Mechanisms



#### **Motivation: Selection as a Means of Compression**

시퀀스 모델은 efficiency vs effectiveness의 trade-off가 중요하다.



Attention은 context를 전혀 압축하지 않기 때문에 효과적이지만 비효율적이다.



LTI 모델은 효율적이지만 내용 인식이 부족하여 아래 그림의 오른쪽과 같이 입력과 출력 사이의 간격이 다양하고 정보를 선택적으로 취합해야
하는 경우를 모델링할 수 없다.

![](https://blog.kakaocdn.net/dn/zEbrH/btsDrHO0JsM/CyUVFgWPQPcCm0QAmxmeT0/img.png)



#### **Improving SSMs with Selection**

추가 차원을 받아들이고 시가변성, 선택성을 부여하기 위해 linear projection을 도입하였다. 이전까지의 피라미터는 단순 행렬.

![](https://blog.kakaocdn.net/dn/bbNP1w/btsDqOVvjlA/jgRK5DR2hHDMyvFKc7FLt1/img.png)
![](https://blog.kakaocdn.net/dn/TVVkA/btsDpbcPVrC/AgrMuIZ8NLC4nPsoF5ZTaK/img.png)



#### **Efficient Implementation of Selective SSMs**

__Motivation of Prior Models__

Hidden state dimension이 큰 모델은 효과적이지만 비효율적이다. 효율성을 저하시키지 않고 hidden state
dimension을 최대화하고자 한다.



Recurrent mode는 convolution mode보다 유연하지만 hidden state를 계산해야 하므로 이를 우회할 수 있는
convolution mode는 일반적으로 더 효율적이다.



__Overview of Selective Scan: Hardware-Aware State Expansion__

Recurrent는 O(BLDN) FLOPs를 사용하고 convolution은 O(BLDlog(L)) FLOPs를 사용하기 때문에 L이
충분히 크고 N이 크지 않은 경우, recurrent가 실제로 더 적은 FLOPs를 사용할 수도 있다.



한 가지 문제는, recurrent의 hidden state 계산으로 인해 메모리 사용이 많다는 점이다.



  * Kernel Fusion: 스캔 입력 (Ā, B̄)를 느린 GPU HBM에서 준비하는 대신 SRAM으로 피라미터 (∆, A, B, C)를 직접 로드하고 이산화, 스캔, C와의 곱셈을 하나의 커널로 융합하여 모두 SRAM에서 수행한 뒤 출력을 HBM에 기록하는 방식으로 memory I/O를 크게 줄인다.
  * Recomputation: 순전파 시 역전파에 필요한 intermediate state를 저장하지 않고 역전파 시 재계산함으로써 메모리 사용량을 줄인다.

![](https://blog.kakaocdn.net/dn/ck1HQW/btsDp7nTUBl/mIjZGVNCerkH0hFxZecytK/img.png)



#### **A Simplified SSM Architecture**

![](https://blog.kakaocdn.net/dn/wf5kO/btsDs4wN2JE/dtYK0EqEvxVm3sGhwsKRz0/img.png)

입력 projection에서 모델 차원을 확장한 두 개의 mamba block stack이 MHA과 MLP가 있는 transformer
block 하나의 피라미터 수와 맞먹는다. transformer처럼 원하는 만큼 쌓으면 되는 것으로 보인다.

대부분의 피라미터는 projection에 존재하고 SSM의 피라미터의 비중은 훨씬 적으며,
[SiLU](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html) 활성화 사용.



#### **실험 및 잡다한 이야기들 생략~**



공유하기

게시글 관리

_구독하기_ **Ostin X**

[ 저작자표시 ](https://creativecommons.org/licenses/by/4.0/deed.ko)

#### '[논문 리뷰](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0) >
[Mamba](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Mamba)' 카테고리의 다른 글

[Jamba: A Hybrid Transformer-Mamba Language Model](/498)  (0) | 2024.04.01  
---|---  
[Zoology: Measuring and Improving Recall in Efficient Language Models](/459)
(0) | 2024.02.28  
[VMamba: Visual State Space Model](/396)  (0) | 2024.01.24  
[Vision Mamba: Efficient Visual Representation Learning with Bidirectional
State Space Model](/393)  (0) | 2024.01.22  
[MoE-Mamba: Efficient Selective State Space Models with Mixture of
Experts](/382)  (0) | 2024.01.15  
[Efficiently Modeling Long Sequences with Structured State Spaces (S4)](/379)
(0) | 2024.01.12  
  
## **'논문 리뷰/Mamba'** Related Articles

  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdj2TCO%2FbtsDSsYnFEQ%2Fe14raSZMw3FrxqobzqSX20%2Fimg.png) VMamba: Visual State Space Model ](/396?category=1117576)
  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FblAUR7%2FbtsDHnjEdOj%2Fm64E5WJGpwKWCzMuSOjX8k%2Fimg.png) Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model ](/393?category=1117576)
  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FDWEAK%2FbtsDxTBbg6g%2FwxyCHXE1ZcCelz4nSARp91%2Fimg.png) MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts ](/382?category=1117576)
  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fri9GO%2FbtsDpa4Wv4w%2FA0Nr1rBKlYMbKwx5pqWXU1%2Fimg.png) Efficiently Modeling Long Sequences with Structured State Spaces (S4) ](/379?category=1117576)

Secret

댓글

댓글달기

* * *

DESIGN BY TISTORY [관리자](https://ostin.tistory.com/manage)

## 티스토리툴바

