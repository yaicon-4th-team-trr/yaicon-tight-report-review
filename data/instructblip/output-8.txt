본문 바로가기

# [Ostin X](https://ostin.tistory.com/)

메뉴

  * [ 분류 전체보기 (477) ](/category)
    * [ 논문 리뷰 (410) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0)
      * [ Language Model (117) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Language%20Model)
      * [ Diffusion Model (136) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Diffusion%20Model)
      * [ Vision Transformer (62) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Vision%20Transformer)
      * [ Mamba (7) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Mamba)
      * [ GAN (20) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/GAN)
      * [ etc. (56) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/etc.)
      * [ Concept (5) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Concept)
      * [ 논문 분류 (7) ](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/%EB%85%BC%EB%AC%B8%20%EB%B6%84%EB%A5%98)
    * [ 코드 리뷰 (8) ](/category/%EC%BD%94%EB%93%9C%20%EB%A6%AC%EB%B7%B0)
      * [ Diffusion (8) ](/category/%EC%BD%94%EB%93%9C%20%EB%A6%AC%EB%B7%B0/Diffusion)
    * [ Deep Learning (34) ](/category/Deep%20Learning)
      * [ GAN (14) ](/category/Deep%20Learning/GAN)
      * [ Fine Tuning (10) ](/category/Deep%20Learning/Fine%20Tuning)
      * [ Diffusion (4) ](/category/Deep%20Learning/Diffusion)
      * [ Memo or etc. (6) ](/category/Deep%20Learning/Memo%20or%20etc.)
    * [ Code, Error, Tip, Etc. (4) ](/category/Code%2C%20Error%2C%20Tip%2C%20Etc.)
    * [ Output (8) ](/category/Output)
      * [ Model (4) ](/category/Output/Model)
      * [ Small Things (4) ](/category/Output/Small%20Things)
    * [ 사설 (13) ](/category/%EC%82%AC%EC%84%A4)
      * [ X (0) ](/category/%EC%82%AC%EC%84%A4/X)
      * [ 독후감 (3) ](/category/%EC%82%AC%EC%84%A4/%EB%8F%85%ED%9B%84%EA%B0%90)

블로그 내 검색 검색

* * *

논문 리뷰/Language Model

# Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video
Understanding

Ostin 2023\. 6. 11. 12:56

비디오를 이해하는 언어 모델



[Github](https://github.com/damo-nlp-sg/video-llama)

[arXiv](https://arxiv.org/abs/2306.02858)



![](https://blog.kakaocdn.net/dn/cJB0N5/btsjnHt7h7Z/u8vd4xbpPBm1ZIepbwFwKK/img.gif)



## **Abstract**

Video Q-former, Audio Q-former를 통해 비디오의 시청각 콘텐츠를 이해하는 multi-modal framework인
Video-LLaMA 제안.







## **Related Works**

[BLIP & BLIP-2](https://junia3.github.io/blog/BLIP)

[MiniGPT-4](https://moon-
walker.medium.com/%EB%A6%AC%EB%B7%B0-llama%EA%B8%B0%EB%B0%98-vicuna%EC%99%80-vicuna%EA%B8%B0%EB%B0%98-multi-
modal-%EB%AA%A8%EB%8D%B8-minigpt-4-31838c4193c5)

[ImageBind](https://ostin.tistory.com/201)





## **Introduction**

BLIP-2의 아이디어를 채택해 Video Q-former, Audio Q-former를 도입하고 multi-branch cross-
model 고안.

![](https://blog.kakaocdn.net/dn/bN3V0g/btsjkGcfAUU/3HlPMzsQ1w4m60BeEU9bz0/img.png)



Audio-text 데이터가 존재하지 않기 때문에 대응을 위해 ImageBind를 인코더로 활용.







## ****Method****

#### **Architecture**

#### **Vision-Language Branch**

![](https://blog.kakaocdn.net/dn/bzF3iJ/btsjseyg090/Caj4Nrzple1Dc0U5nTxM3K/img.png)

이미지 인코더, 위치 임베딩 레이어, Q-Former, 최종 선형 레이어로 구성



각 프레임을 이미지 인코더로 인코딩하고 시간 정보를 위치 임베딩으로 주입한 뒤, BLIP-2와 동일한 아키텍처의 Q-Former에 입력하고
언어 모델과 같은 차원으로 맞추기 위해 최종 선형 레이어를 통과함.

* * *

#### **Audio-Language Branch**

![](https://blog.kakaocdn.net/dn/dxHxqS/btsjtAusVkO/KT2xkzMzYe6KMSd34RqCdk/img.png)

오디오 인코더로 ImageBind 사용.

구체적으로 비디오를 M 구간으로 나눈 뒤에 spectrogram으로 변환하고 인코더를 통해 벡터로 임베딩하는 과정을 거침.



나머지는 video branch와 같음.







#### **Multi-branch Cross-Modal Training**

#### **Training of Vision-Language Branch**

노이즈가 많은 대규모 video-text 데이터에서 사전 훈련하고 고품질의 소규모 데이터셋에서 fine-tuning하는 방법으로 학습.

최근 언어 모델 훈련에서 매우 흔하게 쓰이는 방법.



#### **Training of Audio-Language Branch**

일단 audio-text 데이터 자체가 많이 존재하지 않음.

따라서 인코더로 ImageBind를 채택.



훈련은 visual-text 데이터로 진행되지만 ImageBind의 창발적인 특성 덕분에 오디오를 이해할 수 있게 된다고 한다.

(Visual data가 정확히 정의되어 있지 않은데, 스펙트로그램을 말하는 듯.

예시를 보면 음성 정보를 잘 이해하긴 하는 듯?)

![](https://blog.kakaocdn.net/dn/cB5vVW/btsjlYDfoF8/XXS5tBK3s4HyMdk6BTOO81/img.png)







## **Examples**

사실 예시로 나와있는 정도로는 성능을 알 수 없어서 [hugging face
space](https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA)에서 한 번 써볼려고 했는데
로딩이 너~무 오래 걸려서 포기함.

![](https://blog.kakaocdn.net/dn/bCAGAe/btsjjCA9g0x/BOioOCWM2hzTwa2UYZyz70/img.png)
![](https://blog.kakaocdn.net/dn/mpq0Z/btsjpD59tjp/PefZuUWAsG6mzeTNDgwHr1/img.png)

공유하기

게시글 관리

_구독하기_ **Ostin X**

[ 저작자표시 ](https://creativecommons.org/licenses/by/4.0/deed.ko)

#### '[논문 리뷰](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0) > [Language
Model](/category/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/Language%20Model)'
카테고리의 다른 글

[Efficient Streaming Language Models with Attention Sinks
(StreamingLLM)](/235)  (1) | 2023.10.06  
---|---  
[LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](/228)
(0) | 2023.09.26  
[Augmenting Language Models with Long-Term Memory (LongMem)](/218)  (0) |
2023.07.08  
[LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model](/209)  (0) |
2023.06.15  
[LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init
Attention](/208)  (0) | 2023.06.15  
[LoRA: Low-Rank Adaptation of Large Language Models](/171)  (0) | 2023.01.30  
  
## **'논문 리뷰/Language Model'** Related Articles

  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQ5n2E%2FbtsmRMrNGKT%2FnES8k03sshDs3TlTP1sVTK%2Fimg.png) Augmenting Language Models with Long-Term Memory (LongMem) ](/218?category=1070155)
  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbfmyoM%2FbtsjZySsjGW%2FlJAZKDCjIQXWSA1ZGC67F0%2Fimg.png) LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model ](/209?category=1070155)
  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FAkNHl%2Fbtsj5QQ6zrN%2FfIgfgBjFDjr8tEcu5uFzKK%2Fimg.jpg) LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention ](/208?category=1070155)
  * [ ![](//i1.daumcdn.net/thumb/C264x200/?fname=https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbjfCWe%2FbtrXBzRJnyU%2FhZxRZYW1SEjgn9NqlnIo7K%2Fimg.png) LoRA: Low-Rank Adaptation of Large Language Models ](/171?category=1070155)

Secret

댓글

댓글달기

* * *

DESIGN BY TISTORY [관리자](https://ostin.tistory.com/manage)

## 티스토리툴바

